\documentclass[a4paper]{scrartcl}

\usepackage[
    fancytheorems, 
    fancyproofs, 
    noindent, 
]{adam}
\usepackage{floatrow}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[2]{%
    \def\svgwidth{#1mm}
    \import{./figures/}{#2.pdf_tex}
}

\title{IB Linear Algebra (from lecture 18)}
\author{Martin von Hodenberg (\texttt{mjv43@cam.ac.uk})}
\date{\today}


\allowdisplaybreaks

\begin{document}

\maketitle

This course focuses on linear algebra, a subject which is fundamental in mathematics and physics. This course will formalise material which you have already encountered in IA Vectors and Matrices, and introduces many new objects and systems such as Jordan Normal Form and bilinear maps.

This article constitutes my notes for the `IB Linear Algebra' course, held in Michaelmas 2021 at Cambridge. The course was lectured by Prof. Pierre Raphael.


\tableofcontents
\newpage 

\section{Bilinear forms revisited}
\begin{lemma}
    We have a bilinear form $\varphi: V \times V \rightarrow \mathbb{F}$, where $V$ is a finite vector space and $B, B'$ are bases of $V$. Let 
    \[P=[\operatorname{Id}]_{B,B'}.\]
     Then 
    \[[\varphi]_{B'}=P^T [\varphi]_B P.\]
\end{lemma}
\begin{proof}
     This is just a special case of the general change of basis formula; see that proof.
\end{proof}

\begin{definition}[Congruent matrices]
     Two square matrices $A,B$ are said to be \vocab{congruent} if there exists an invertible square matrix $P$ such that 
     \[A=P^T B P.\]
\end{definition}
\begin{remark}
     This defines an equivalence relation.
\end{remark}
\begin{definition}[Symmetric bilinear form]
     A bilinear form on $V$ is said to be \vocab{symmetric} if 
     \[\varphi(u,v)=\varphi(v,u) \ \forall u,v \in V.\]
\end{definition}
\begin{remark}
     We have encountered this definition before in IA Vectors and Matrices.
    \begin{enumerate}
        \item If $A$ is a square matrix, we say that $A$ is symmetric if $A^T=A$. Equivalently, $A_{ij}=A_{ji}$.
        \item $\varphi$ is symmetric iff $[\varphi]_B$ is symmetric in \emph{any} basis $B$.
        \item To be able to represent $\varphi$ by a diagonal matrix in some basis $B$, it is necessary that $\varphi$ is symmetric: 
        \[P^T AP=D=D^T=P A^T P^T \implies A=A^T \implies \varphi \text{ is symmetric} .\]    
    \end{enumerate}
     
\end{remark}
\begin{definition}[Quadratic form]
     A map $Q: V \rightarrow F$ is said to be a \vocab{quadratic form} if there exists a bilinear form $\varphi: V \times V \rightarrow F$ such that 
     \[\forall u \in V, \ Q(u)=\varphi (u,u).\]
\end{definition}
\begin{remark}[Computation in a basis]
     Let $B=(e_i)_{1 \leq i \leq n}$ be a basis of $V$, and let $A=[\varphi]_B$. Let $u=\sum_{i=1}^{n}u_i e_i$, then 
     \[Q (u)=\varphi (u,u)=\varphi \left(\sum_{i=1}^{n}u_i e_i, \sum_{j=1}^{n}u_j e_j\right)=\sum_{i,j=1}^{n}u_i u_j \varphi(e_i,e_j)=\sum_{i,j=1}^{n}a_{ij} u_i u_j.\]
     (by bilinearity of $\varphi$)
     Therefore we essentially have 
     \[Q (u)=U^T A U \text{, where } U=\begin{pmatrix}
     u_1\\\vdots \\ u_n
     \end{pmatrix}
     .\]
\end{remark}
\begin{remark}
     We can note that 
     \[Q (u)=U^T A U =\sum_{i,j=1}^{n}a_{ij}u_i u_j=\sum_{i,j=1}^{n}(\frac{a_{ij}+a_{ji}}{2})u_i u_j=U^T (\frac{A+A^T}{2}) U.\]
     So this representation of $A$ is not necessarily unique.
\end{remark}

\begin{proposition}
     If $Q: V \rightarrow F$ is a quadratic form, then there exists a unique symmetric bilinear form $\varphi: V \times V \rightarrow F $ such that 
     \[Q(u)=\varphi (u,u) \ \forall u \in V.\]
\end{proposition}
\begin{proof}(Polarisation identity)\newline 
     \textbf{Proof of existence}\newline 
     Let $\psi$ be a bilinear form on $V$ such that 
     \[\forall u \in V, \ Q(u)=\psi (u,u).\]
     Let $\varphi (u,v)=\frac{1}{2} (\psi (u,v)+ \psi (v,u))$. Thus we have that:
     \begin{itemize}
         \item $\varphi$ is a bilinear form
         \item $\varphi$ is symmetric
         \item $\varphi (u,u)=\psi (u,u)=Q (u)$.
     \end{itemize}
     This concludes the proof of existence.\newline 
     \textbf{Proof of uniqueness}\newline 
     Let $\varphi$ be a symmetric bilinear form such that 
     \[\forall u \in V, \ \varphi (u,u)=Q(u).\]
     Then 
     \begin{equation*}
          \begin{split}
            Q (u+v)&=\varphi (u+v,u+v)\\
            &=\varphi (u,u)+ \varphi (u,v)+\varphi (v,u)+ \varphi (v,v) \text{ by bilinearity} \\
            &=Q(u)+ 2\varphi (u,v)+Q(v) \text{ by symmetry}\\
          \end{split}
     \end{equation*}
     From this we get that 
     \[\varphi(u,v)=\frac{1}{2} (Q (u+v)-Q (u)- Q (v)).\]
\end{proof}

\begin{theorem}[Diagonalisation of symmetric bilinear forms]
     Let $\varphi: V \times V \rightarrow F$ be a symmetric bilinear form. (dim $V$=n). Then there exists a basis $B$ of $V$ such that $[\varphi]_B$ is diagonal.
\end{theorem}
\begin{proof}
     We proceed by induction on the dimension of $V$. For $n=1$ it is trivially true. Suppose the theorem holds for all dimensions $<n$: then
     \begin{itemize}
         \item If $\varphi (u,u)=0 \forall u \in V$, then $\varphi=0$ by the polarisation identity ($\varphi $ is symmetric).
         \item If $\varphi \neq 0$, then there exists a $u \in V \backslash \{0\}$ such that $\varphi (u,u) \neq 0$. Let us call $u=e_1$.
         \item Let $U$ be the 'orthogonal' of $\langle e_1 \rangle $: 
         \begin{equation*}
               \begin{split}
                    U&=\langle e_1 \rangle^\perp \\
                    &=\{v \in V: \ \varphi (e_1,v)=0\}\\
                    &=\operatorname{ker} \theta \text{ where } \theta:V \rightarrow F \text{ is given by } \theta (v)= \varphi (e_1,v).
               \end{split}
         \end{equation*}
         Since it is a kernel of a linear map $V \rightarrow F$, $U$ is a vector subspace of $V$. By the Rank-Nullity theorem, we have 
         \[\operatorname{dim} V=n= r(\theta)+ \operatorname{null}  \theta=\operatorname{dim}U+1.\]
         We now claim that $U+\langle e_1 \rangle =U \oplus \langle e_1 \rangle $. Indeed, 
         \[v= \langle e_1 \rangle  \cap U \implies v= \lambda e_1 \text{ and }  \varphi(e_1,v)=0 .\]
         \[\implies 0=\varphi (e_1,v)=\varphi (e_1, \lambda e_1)=\lambda \varphi (e_1,e_1) \implies \lambda=0 \implies v=0.\]
         \[\implies U+\langle e_1 \rangle =U \oplus \langle e_1 \rangle .\]
         Therefore $V=U \oplus \langle e_1 \rangle $, and pick a basis $B'=(e_2, \ldots ,e_n)$ such that $(e_1, e_2, \ldots , e_n)$ is a basis of $V$ (since the sum is direct). So 
         \[[\varphi]_B=(\varphi(e_i,e_j))_{1 \leq i,j \leq n}=\left(
          \begin{array}{c|c}
          \varphi (e_1,e_1) &0\\
            \hline
            0&A'
          \end{array}
          \right)
         .\]
         Therefore $(A')^T=A'$, and 
         $A'=[\varphi|_U]_{B'}$ where $\varphi|_U$ is the restriction of $\varphi$ onto $U$. Now we apply the induction hypothesis to find a basis $(e_1', \ldots ,e_n;)$ of $V$ such that $[\varphi|U]_{B}$ is diagonal. So 
         \[\hat{B}=(e_1,e_2',\ldots ,e_n')\] is a basis of $V$, and finally we have that $[\varphi]_{\hat{B}}$ is diagonal.      
     \end{itemize}
\end{proof}

\begin{example}
     Let $V=\mathbb{R}^{3}$, and 
     \begin{equation*}
           \begin{split}
               Q(x_1,x_2,x_3)&=x_1^2+x_2^2+2x_3^2+2x_1x_2+2x_1x_3-2x_2x_3\\
               &=x^T A x \text{ where }  A = 
               \begin{pmatrix}
                    1&1&1\\ 1&1&-1 \\ 1&-1&2
               \end{pmatrix}
           \end{split}    
     \end{equation*}
     There are two ways to diagonalise our quadratic form:
     \begin{enumerate}
         \item Diagonalise using the algorithm we developed in the previous proof.
         \item Complete the square.
     \end{enumerate}
     Let's complete the square: 
     \begin{equation*}
           \begin{split}
               Q(x_1,x_2,x_3)&=(x_1+x_2+x_3)^2+x_3^2-4x_2x_3 \\
               &= (x_1+x_2+x_3)^2+(x_3-2x_1)^2-(2x_2)^2\\
               &=(x_1')^2+(x_2')^2+(x_3')^2.
           \end{split}
     \end{equation*}
     Therefore 
     \[P^T AP =\begin{pmatrix}
     1&0&0\\0&1&0\\0&0&-1
     \end{pmatrix}
     .\]
     To find $P$, note that 
     \[\begin{pmatrix}
     x_1'\\x_2'\\x_3'
     \end{pmatrix}
     =\begin{pmatrix}
     1&1&1\\0&-2&1\\0&-2&0
     \end{pmatrix}
     \begin{pmatrix}
     x_1\\x_2\\x_3
     \end{pmatrix}
     ={P}^{-1}\begin{pmatrix}
     x_1\\x_2\\x_3
     \end{pmatrix}
     .\]
     
\end{example}

Recall our theorem that a symmetric bilinear form has a diagonal basis.

\begin{corollary}
    Let $F=\mathbb{C}$, and $\varphi$ be a symmetric bilinear form of rank $r$ on $V \times V$ where $\operatorname{dim}V=n$. Then there exists a basis $B$ of $V$ such that the matrix $[\varphi]_B$ can be expressed in the form
    \[\left(
    \begin{array}{c|c}
     I_r &0\\
     \hline
     0&0
     \end{array}
     \right).\]
\end{corollary}
\begin{proof}
     Pick $E=(e_1, \ldots , e_n)$ such that 
     \[[\varphi]_E=
     \begin{pmatrix}
     a_1&&0\\ &\ddots&\\ 0&&a_n
     \end{pmatrix}
     .\]
     Order the $a_i$ such that 
     \begin{equation*}
          \begin{cases}
              a_i \neq 0 & 1 \leq i \leq r\\
              a_i=0 & i>r
          \end{cases}
     \end{equation*}
     For $i \leq r$, let $\sqrt{a_i}$ be a choice of complex root for $a_i$. Let 
     \begin{equation*}
        \begin{cases}
            v_i=\frac{e_i}{\sqrt{a_i}} & 1 \leq i \leq r\\
            v_i=e_i& i>r
        \end{cases}
   \end{equation*}
   Then $B= (v_1, \ldots ,v_r, v_{r+1}, \ldots , v_n)$ is a basis of $V$ and we can check that we have diagonalised and normalised $\varphi$:
   \[[\varphi]_B=\left(
     \begin{array}{c|c}
      I_r &0\\
      \hline
      0&0
      \end{array}
      \right).\]
\end{proof}

\begin{corollary}
    Every symmetric matrix of $M_n (\mathbb{C})$ is congruent to a \emph{unique} matrix of the form 
    \[\left(
     \begin{array}{c|c}
      I_r &0\\
      \hline
      0&0
      \end{array}
      \right).\]
\end{corollary}
\begin{proof}
     Immediate.
\end{proof}
\begin{corollary}
    Let $F=\mathbb{R}$. Let $\varphi$ be a symmetric bilinear form on $V \times V$ where $\operatorname{dim}V=n$. Then there exists a basis $B=(v_1, \ldots ,v_n)$ basis of $V$ such that for some $p-q \geq 0$ with $p+q=r (\varphi)$,
    \[[\varphi]_B=\left(
     \begin{array}{c|c|c}
      I_p &0&0\\
      \hline
      0 &-I_q&0\\
      \hline
      0 &0&0\\
      \end{array}
      \right).\]
\end{corollary}
\begin{proof}
    Pick $E=(e_1, \ldots , e_n)$ such that 
    \[[\varphi]_E=
    \begin{pmatrix}
     a_1&&0\\ &\ddots&\\ 0&&a_n
     \end{pmatrix}.\]
    Order the $a_i$ such that 
    \begin{equation*}
         \begin{cases}
             a_i > 0 & 1 \leq i \leq p\\
             a_i<0 & p+1 \leq i \leq p+q\\
             a_i=0 & i > p+q
         \end{cases}
    \end{equation*}
    Similarly to before we define
    \begin{equation*}
        \begin{cases}
            v_i=\frac{e_i}{\sqrt{a_i}} & 1 \leq i \leq p\\
            v_i=\frac{e_i}{\sqrt{-a_i}} & p+1 \leq i \leq p+q\\
            v_i=e_i& i > p+q
        \end{cases}
    \end{equation*}
    Then this basis will do the job.
\end{proof}
\begin{definition}[Signature of a quadratic form]
     For $F=\mathbb{R}$, we define the \vocab{signature} of $\varphi$
     \[s(\varphi)=p-q.\]
     This is also the signature of the associated quadratic form $Q$. We will soon see that this is well-defined.
\end{definition}

\begin{definition}[Positive definite quadratic/bilinear form]
     Let $\varphi$ be a symmetric bilinear form on a real vector space $V$. We say that 
     \begin{itemize}
         \item[(i)] $\varphi$ is positive definite if 
         \[\varphi (u,u) >0 \ \forall u \in V \backslash \{0\}.\]
         \item[(ii)] $\varphi$ is positive semidefinite if 
         \[\varphi (u,u) \geq 0 \ \forall u \in V \backslash \{0\}.\]
         \item[(iii)] $\varphi$ is negative definite if 
         \[\varphi (u,u) <0 \ \forall u \in V \backslash \{0\}.\]
         \item[(iv)] $\varphi$ is negative semidefinite if 
         \[\varphi (u,u) \leq 0 \ \forall u \in V \backslash \{0\}.\]
     \end{itemize}
\end{definition}
\begin{example}
     The matrix 
     \[\left(
     \begin{array}{c|c}
          I_p &0\\
          \hline
          0&0
     \end{array}
     \right)
     \]
     is
     \begin{itemize}
         \item positive definite for $p=n$
         \item positive semidefinite for $1 \leq p \leq n$.
     \end{itemize}
\end{example}
\subsection{Sylvester's law}

\begin{theorem}[Sylvester's law of inertia]
    The signature of a quadratic form is well defined. \newline 
     Formally, if a real symmetric bilinear form is represented in two different ways by two identity blocks of size $p,q$ and $p',q'$ then we have 
     \[p=p',q=q'.\]
\end{theorem}
\begin{proof}
     In order to prove uniqueness of $p$, it is enough to show that $p$ is the largest dimension of a subspace of $V$ on which $\varphi $ is positive definite. Say $B=(v_1, \ldots , v_n)$ and 
     \[[\varphi]_B=\left(
          \begin{array}{c|c|c}
           I_p &0&0\\
           \hline
           0 &-I_q&0\\
           \hline
           0 &0&0\\
           \end{array}
           \right).\]
     Let $X=<v_1,\ldots ,v_p$. Then $\varphi $ is positive definite on $X$. If we take a $u \in X$ with $u=\sum_{i=1}^{p}\lambda_i v_i$, then 
     \[Q(u)= \varphi (u,u)=\varphi(\sum_{i=1}^{p}\lambda_i v_i,\sum_{j=1}^{p}\lambda_j v_j)=\sum_{i,j=1}^{p}\lambda_i \lambda_j \varphi (v_i,v_j)= \sum_{i=1}^{n} \lambda_i^2 .\]
     This is $>0$ as long as $u \neq 0$. Suppose that $\varphi $ is positive definite on another subspace $X$. Let 
     \[X=\langle v_1,\ldots ,v_p \rangle  , Y=\langle v_{p+1},\ldots ,v_n \rangle .\]
     Then arguing as above, looking at $[\varphi]_B$ we know that $\varphi$ is negative semidefinite on $Y$. This implies that 
     $Y \cap X'=\{0\}$. Indeed, if $y \in Y \cap X$, then $Q(y)\leq 0$ since $y \in Y$, but this implies that $y=0$ since $y \in X$. Therefore 
     \begin{align*}
          &Y+X= Y \oplus X\\
          &\implies n=\operatorname{dim} V \geq \operatorname{dim} (Y+X) =\operatorname{dim}Y +\operatorname{dim}X\\
          &\implies n \geq n-p+\operatorname{dim}X\\
          &\implies \operatorname{dim}X \leq p.
     \end{align*}
     
     Similarly, we show that $q$ is the largest dimension of a subspace on which $\varphi$ is negative definite. So we have a \emph{geometric characterisation} of $p$ and $q$, which concludes the proof.
\end{proof}

\begin{definition}[Kernel of a bilinear form]
     Define the kernel of the bilinear form
     \[K=\{v \in V : \ \forall u \in V, \varphi (u,v)=0 \}.\]
\end{definition}
\begin{remark}
     $\operatorname{dim} K + r (\varphi)=n$.
\end{remark}
One can show using the above notation that there is a subspace $T$ of dimension $m-(p+q)+\min p,q$ such that $\varphi_T=0$. The subspace we want to take consists of all the basis vectors that are killed by the lower corner of $[\varphi]_B$, but it also includes the 'cancellations' between $I_p$ and $-I_q$.

Moreover, one can show that the dimension of $T$ is the largest possible dimension of such a subspace.
\subsection{Sesquilinear forms}
Recall that the standard inner product on $\mathbb{C}^n$ is 
\[\langle x,y \rangle =\langle \begin{pmatrix}
x_1\\\ldots \\x_n
\end{pmatrix},\begin{pmatrix}
y_1\\\ldots \\y_n
\end{pmatrix} \rangle 
=\sum_{i=1}^{n}x_i \overline{y_i}
.\]
However note that this map is \textbf{not} a bilinear form, since the conjugation of the second coordinate prevents this. However, it is still bilinear 'in a way'. We now explore this further.
\begin{definition}[Sesquilinear form]
     Let $V,W$ be vector spaces over $\mathbb{C}$. A sesquilinear form on $V \times W$ is a function 
     \[\varphi: V \times W \to \mathbb{C} .\]
     such that 
     \begin{enumerate}
         \item $\varphi (\lambda_1 v_1+ \lambda_2 v_2, w)=\lambda_1 \varphi (v_1,w) + \lambda_2 \varphi (v_2,w)$
         \item $\varphi (v,\lambda_1 w_1+ \lambda_2 w_2)=\overline{\lambda_1} \varphi (v,w_1) + \overline{\lambda_2} \varphi (v,w_2)$
     \end{enumerate}
     For $B$ a basis of $V$, C a basis of $W$, we define the matrix 
     \[[\varphi]_{B,C}=(\varphi (v_i,w_j))_{1 \leq i \leq m, 1 \leq j \leq n}.\]
\end{definition}
\begin{lemma}
     If $B$ basis for $V$, $C$ a basis for $W$,
     \[\varphi (v,w)=[v]_B^T [\varphi]_{B,C}\overline{[w]_C}.\]
     Further, if $B',C'$ are other bases for $V,W$ respectively, and $P=[Id]_{B',B}, Q=[Id]_{C',C}$ then 
     \[[\varphi]_{B',C'}=P^T [\varphi]_{B,C} \overline{Q}.\]
     
\end{lemma}
\begin{proof}
     Same proofs as for bilinear forms, just slightly modified.
\end{proof}

\subsection{Hermitian forms}
We will now generalise symmetric bilinear forms to sesquilinear forms.
\begin{definition}[Hermitian form]
     Let $V$ be a finite dimensional vector space and $\varphi: V \times V \rightarrow C$ be a sesquilinear form. We call $\varphi$ \vocab{hermitian} if for all $(u,v) \in V \times V$ we have 
     \[\varphi (u,v)=\overline{\varphi (v,u)}.\]
\end{definition}
\begin{remark}
     If $\varphi$ is Hermitian, then $\varphi (u,u)=\overline{\varphi (u,u)} \implies \varphi (u,u) \in \mathbb{R}$. Moreover, 
     \[\varphi (\lambda u , \lambda u)= |\lambda|^2 \varphi (u,u).\]
\end{remark}
This allows us to talk about negative/ positive definite Hermitian form.

\begin{lemma}
     A sesquilinear form $\varphi: V \times V \rightarrow \mathbb{C}$ is hermitian if and only if for every basis $B$ of $V$, 
     \[[\varphi]_B=\overline{[\varphi]_B^T}.\]
     
\end{lemma}
\begin{proof}
     Let $A=[\varphi]_B$ have entries $a_{ij}=\varphi(e_i,e_j)$. 
     If $\varphi$ is Hermitian, then 
     \[a_{ji}=\varphi(e_j,e_i)=\overline{\varphi(e_i,e_j)}=\overline{a_{ij}} .\]
     Thus $[\varphi]_B=\overline{[\varphi]_B^T}.$\newline 
     For the converse direction, if we have $[\varphi]_B=\overline{[\varphi]_B^T}$, and we write $u=\sum_{i=1}^{n}u_i e_i, v=\sum_{i=1}^{n}v_i e_i$, then 
     \[\varphi \left(u,v\right)=\varphi \left(\sum_{i=1}^{n}u_i e_i\right), \sum_{i=1}^{n}v_ie_i=\sum_{i,j=1}^{n}u_i \overline{v_j} \varphi \left(e_i,e_j\right)=\sum_{i,j=1}^{n}u_i \overline{v_j} a_ij.\]
     We can similarly expand $\varphi(e_j,e_i)$ to get the same result and we are done.
\end{proof}

\begin{proposition}[Polarisation identity for sesquilinear forms]
      A Hermitian form $\varphi$ on a complex vector space $V$ is entirely determined by $Q:V \rightarrow R$ where $Q(v)=\varphi \left(v,v\right)$ via the formula 
      \[\varphi \left(u,v\right)=\frac{1}{4}\left(Q \left(u+v\right)-Q \left(u-v\right)+iQ \left(u+iv\right)-iQ \left(u-iv\right)\right).\]
\end{proposition}
\begin{proof}
      Similar to the case of symmetric bilinear forms.
\end{proof}

\begin{theorem}[Hermitian formulation of Sylvester's law]
      Let $V$ be an $n$-dimensional vector space over $\mathbb{C}$. Let $\varphi:V \times V \rightarrow C$ be a Hermitian form on $V$. Then there exists a basis $B=\left(v_1, \ldots , v_n\right)$ of $V$ so that 
      \[[\varphi]_B=\left(
          \begin{array}{c|c|c}
           I_p &0&0\\
           \hline
           0 &-I_q&0\\
           \hline
           0 &0&0\\
           \end{array}
           \right).\]
\end{theorem}
\begin{proof}
      We just provide a sketch proof as the proof is nearly identical to the case of real symmetric bilinear forms.\newline 
      \textbf{Existence:}\newline 
       $\varphi=0$, done. Otherwise, using the polarisation identity, there exists $e_1 \neq 0$ such that $\varphi \left(e_1,e_1\right) \neq 0$. We then normalise $e_1$ to $v_1$ and consider the orthogonal of $<v_1>$. Then we check (arguing like for real bilinear symmetric forms): 
       \[V=<v_1> \oplus W ,\]
       where $\operatorname{dim}W=n-1$. Finally, we argue by induction to diagonalise $\varphi|_W$.\newline 
       \textbf{Uniqueness:}\newline 
       We argue that $p$ is the maximal dimension of a subspace on which $\varphi$ is positive definite, and that $q$ is the maximal dimension of a subspace on which $\varphi$ is negative definite. Then by our geometric characterisation we are done.
\end{proof}

\subsection{Skew symmetric forms}
We now return to $F=\mathbb{R}$.
\begin{definition}[Skew symmetric bilinear forms]
      Let $V$ be a vector space over $\mathbb{R}$, and $\operatorname{dim}V=n$. A bilinear form $\varphi: V \times V \rightarrow \mathbb{R}$ is \vocab{skew symmetric} if: 
      
      \[\forall \left(u,v\right) \in V \times V, \ \varphi \left(u,v\right)=-\varphi \left(v,u\right).\]
\end{definition}
\begin{remark} This leads to a few easy consequences:
      \begin{enumerate}
           \item $\varphi \left(u,u\right)=\varphi \left(u,u\right) \implies \varphi \left(u,u\right)=0$.
           \item For all bases $B$ of $V$, $[\varphi]_B=-[\varphi]_B^T$
           \item For all $A \in M_n \left(\mathbb{R}\right)$,
           \[A=\frac{1}{2}\left(A+A^T\right)+ \frac{1}{2}\left(A-A^T\right).\]
           (we can decompose any matrix into symmetric and skew symmetric parts)
      \end{enumerate}
\end{remark}
\begin{theorem}[Sylvester form of skew symmetric matrices]
      Let $V$ be a finite $n$-dimensional vector space over $\mathbb{R}$. Let $\varphi$ be a skew symmetric bilinear form over $V$. Then there exists a basis $B$ of $V$ such that 
      \[B=\left(v_1,w_1,v_2,w_2, \ldots v_m,w_m,v_{2m+1},v_{2m+2},\ldots ,v_n\right).\]
\end{theorem}
\begin{proof}
      We give a sketch proof as this proof is again similar to past proofs. We proceed by induction on the dimension of $V$.
      \begin{enumerate}
           \item If $\varphi=0$, then we are done.
           \item Else there exists $\left(v_1,w_1\right)$ such that $\varphi \left(v_1,w_1\right)\neq 0$.
           \item After scaling $v_1$, we can assume $\varphi \left(v_1,w_1\right)=1$ and thus by skew symmetricity $\varphi \left(w_1,v_1\right)=-1$. Observe $v_1,w_1$ are linearly independent since 
           \[\varphi \left(v_1, \lambda v_1\right)=\lambda \varphi \left(v_1,v_1\right)=0.\]
           We now let W be the orthogonal of $U=<v_1,w_1>$, 
           \[ W=\{v \in V: \ \varphi \left(v_1,v\right)=\varphi \left(v,w_1\right)=0 \}. \]
           \item Finally we show $V=U \oplus W$ and apply our inductive hypothesis.
      \end{enumerate}
\end{proof}
\begin{corollary}
     Skew symmetric matrices have an even rank.
\end{corollary}
\section{Inner product spaces}
We will find that positive definite bilinear forms will be useful for a form of inner product, which leads us to norms (notions of distance). Looking past this course, this extends to an infinite dimensional counterpart of Hilbert spaces in Part II Linear Analysis.
\begin{definition}[Inner product]
      Let $V$ be a vector space over $\mathbb{R}$ (resp. $\mathbb{C}$). An \vocab{inner product} on $V$ is a positive definite symmetric (resp. Hermitian) bilinear form $\varphi$ on $V$. We use the notation 
      \[\langle u ,v \rangle =\varphi \left(u,v\right).\]
      We call $V$ a real (resp. complex) inner product space.
\end{definition}
\begin{example}
     Here are several examples of inner product spaces.
      \begin{enumerate}
           \item $V=\mathbb{R}^{n} $, where we define 
           \[\langle x ,y \rangle =\langle \begin{pmatrix}
           x_1\\\vdots\\x_n
           \end{pmatrix}, \begin{pmatrix}
           y_1\\\vdots\\y_n
           \end{pmatrix}\rangle
           =\sum_{i=1}^{n}x_i y_i
           .\]
           \item $V=\mathbb{C}^n$, $\langle x ,y \rangle =\sum_{i=1}^{n}x_i \overline{y_i} $.
           \item $V=C^{0} \left([0,1],\mathbb{C}\right)$. We define 
           \[\langle f,g \rangle =\int_{0}^{1}f (t)\overline{g (t)} \ dt.\]
           \item We can fix a weight $\omega: [0,1] \rightarrow \mathbb{R}^*_+$ and define on $V=C^{0} \left([0,1],\mathbb{C}\right)$ 
           \[\langle f,g \rangle =\int_{0}^{1}\omega(t)f (t)\overline{g (t)}  \ dt.\]
      \end{enumerate}
      One can check that all these examples are inner products. The only non-trivial thing that needs to be checked in this case is the positive definite property: 
      \[\langle u,u \rangle =0 \implies u=0.\]
\end{example}
\begin{remark}
      The study of $L^2$ spaces is the heart of the definition of a new integral, the Lebesgue integral.
\end{remark}

\begin{definition}[Norm/length]
      We define the \vocab{norm} of $v$ by 
      \[||v||=\sqrt{\langle v,v \rangle }.\]
\end{definition}
\begin{remark}
      We say that the norm derives from a scalar product.
\end{remark}

\begin{theorem}[Cauchy-Schwarz inequality]
      In an inner product space, 
      \[|\langle u,v \rangle |\leq ||u||||v||.\]
      
\end{theorem}
\begin{proof}
      We have $F=\mathbb{R}$ or $\mathbb{C}$. Let $t \in F$, then 
      \begin{equation*}
            \begin{split}
               0 \leq ||tu-v||^2 &=\langle tu-v,tu-v \rangle \\
               &=t \overline{t} \langle u,u \rangle -t\langle u,v \rangle -\overline{t} \langle v,u \rangle +||v||^2\\
               &=|t|^2 ||u||^2-2 \operatorname{Re}(t\langle u,v \rangle )+||v||^2.
            \end{split}
      \end{equation*}
      We now choose explicitly 
      \[t=\frac{\overline{\langle u,v \rangle } }{||u||^2}.\]
      This gives 
      \[0 \leq \frac{|\langle v,u \rangle |^2}{||u||^4}||u||^2-2 \operatorname{Re}\left(\frac{|\langle u,v \rangle |^2}{||u||^2}\right)+||v||^2\]
      \[\implies 0 \leq ||v||^2-\frac{|\langle v,u \rangle |^2}{||u||^2}\]
      \[\implies |\langle v,u \rangle |^2 \leq ||v||^2 ||u||^2.\]
\end{proof}
\begin{remark}
      It is left as an exercise to show that equality holds iff the vectors $u,v$ are collinear.
\end{remark}
\begin{corollary}[Triangle inequality]
     $||u+v||\leq ||u||+||v||$.
\end{corollary}

\begin{proof}
     \begin{equation*}
           \begin{split}
                ||u+v||&=\langle u+v,u+v \rangle  \\
                &=||u||^2+ 2 \operatorname{Re}\langle v,u \rangle  +||v||^2\\
                & \leq ||u||^2+2 |\langle u,v \rangle  |+||v||^2\\
                & \leq ||u||^2+2 ||u||||v||+||v||^2 \text{ by Cauchy-Scwarz} \\
                &=\left(||u||+||v||\right)^2.
           \end{split}
     \end{equation*}     
\end{proof}
\begin{remark}
      $||.||$ is a norm.
\end{remark}
\begin{definition}[Orthogonal/orthonormal family]
      A set $\left(e_1, \ldots ,e_k\right)$ of vectors of $V$ is 
      \begin{itemize}
           \item[(i)] \vocab{orthogonal} if $\langle e_i,e_j \rangle  =0 \ \forall i \neq j$
           \item[(ii)] \vocab{orthonormal} if $\langle e_i,e_j \rangle  =\delta_{ij} \ \forall i,j$
      \end{itemize}
\end{definition}

\begin{lemma}
      If $\left(e_1, \ldots , e_k\right)$ are orthogonal non-zero vectors, then 
      \begin{itemize}
           \item[(i)] they are linearly independent.
           \item[(ii)] Moreover, writing $v=\sum_{j=1}^{k}\lambda_j e_j$, then 
           \[\lambda_j=\frac{\langle v,e_j \rangle  }{||e_j||^2}.\]
      \end{itemize} 
\end{lemma}
\begin{proof}
     From the definitions:
     \begin{itemize}
          \item[(i)]Let us take $\sum_{i=1}^{k}\lambda_i e_i=0$. Then
          \begin{align*}
               0&= \langle \sum_{i=1}^{k}\lambda_i e_i,e_j \rangle \\&=\sum_{i=1}^{k}\lambda_i \langle e_i,e_j \rangle \\&= \lambda_j ||e_j||^2 \\&\implies \lambda_j=0 \ \ \forall 1 \leq j \leq k.
          \end{align*}
          \item[(ii)] If $v=\sum_{i=1}^{k}\lambda_i e_i$, then 
          \[\langle v,e_j \rangle  =\langle \sum_{i=1}^{k}\lambda_i e_i,e_j \rangle=\sum_{i=1}^{k}\lambda_i \langle e_i,e_j \rangle  =\lambda_j ||e_j||^2 .\]
          \[\implies \lambda_j=\frac{\langle v,e_j \rangle  }{||e_j||^2}.\]
     \end{itemize}
      
\end{proof}

\begin{lemma}[Parseval's identity]
      If $V$ is a finite dimensional inner product space, and $\left(e_1, \ldots ,e_n\right)$ is an \emph{orthonormal} basis, then 
      \[\langle u,v \rangle  =\sum_{i=1}^{n}\langle u,e_i \rangle  \overline{\langle v,e_i \rangle  } .\]
      In particular, 
      \[||u||^2 =\sum_{i=1}^{n}|\langle u,e_i \rangle |^2.\]
\end{lemma}

\begin{proof}
     By the previous lemma with $||e_i||=1$, 
     \[u= \sum_{i=1}^{n}\langle u,e_i \rangle   e_i, \ v= \sum_{i=1}^{n}\langle v,e_i \rangle  e_i.\]
     Therefore 
     \[\langle u,v \rangle  =\langle \sum_{i=1}^{n}\langle u,e_i \rangle   e_i,\sum_{i=1}^{n}\langle v,e_i \rangle  e_i \rangle=\sum_{i=1}^{n}\langle u,e_i \rangle  \overline{\langle v,e_i \rangle  }.\]
     (by orthonormality, and since our form is sesquilinear). Setting $u=v$ recovers our other result.
\end{proof}
Having proved these facts, we now turn our attention to an important result which will lead to us understanding bases of inner product spaces.
\subsection{Gram-Schmidt orthogonalisation}
\begin{theorem}[Gram-Schmidt orthogonalisation process]
      Let $V$ be an inner product space. Let $\left(v_i\right)_{i \in I}$ such that $I$ is countable, and the $v_i$ are non-zero and linearly independent. Then there exists a family $\left(e_i\right)_{i \in I}$ of \emph{orthonormal} vectors such that 
      \[\forall k \geq 1, \operatorname{span} \{v_1,\ldots ,v_k\}=\operatorname{span} \{e_1,\ldots ,e_k\}.\]
\end{theorem}

\begin{proof}
      The proof is an explicit construction of the family $\left(e_i\right)_{i \in I}$. We proceed by induction on $k$.\newline 
      If $k=1$, then $e_1=\frac{v_1}{||v_1||} \ \left(v_1 \neq 0\right)$. For the general case, say we have found $\left(e_1, \ldots ,e_k\right)$ orthonormal with 
      \[\operatorname{span} \{v_1,\ldots ,v_k\}=\operatorname{span} \{e_1,\ldots ,e_k\}.\]
      Let us compute $e_{k+1}$. We define 
      \[e_{k+1}=v_{k+1}-\sum_{i=1}^{k}\langle v_{k+1},e_i \rangle e_i .\]
      Geometrically, we are projecting $v_{k+1}$ onto the plane spanned by $\left(e_1, \ldots , e_k\right)$.[drawing] Note that $e_{k+1} \neq 0$, as otherwise 
      \[v_{k+1} \in \operatorname{span} \{e_1,\ldots ,e_k\}=\operatorname{span} \{v_1,\ldots ,v_k\},\]
      a contradiction. Now take $j \in \{1,\ldots ,k\}$, then 
      \begin{equation*}
          \begin{split}
               \langle e_{k+1},e_j \rangle&=\langle \left(v_{k+1}-\sum_{i=1}^{k}\langle v_{k+1},e_i \rangle e_i\right) ,e_j \rangle \\
               &=\langle v_{k+1},e_j \rangle - \sum_{i=1}^{k}\langle v_{k+1},e_j \rangle \underbrace{\langle e_i,e_j \rangle}_{=\delta_{ij}}\\
               &=\langle v_{k+1},e_j \rangle-\langle v_{k+1},e_j \rangle=0.
          \end{split}
      \end{equation*}
      So $e_{k+1}$ is orthogonal to $e_j$ for all $1 \leq j \leq k$. Thus 
      \[\operatorname{span} \{v_1,\ldots ,v_{k+1}\}=\operatorname{span} \{e_1,\ldots ,e_{k+1}\}.\]
      Now define (since we know $e_{k+1} \neq 0$), 
      \[e'_{k+1}=\frac{e_{k+1}}{||e_{k+1}||}.\]
      Then $\left(e_1, \ldots ,e_k, e'_{k+1}\right)$ is orthonormal
\end{proof}

\begin{corollary}
     If $V$ is a finite dimensional inner product space, then any orthonormal set of vectors can be extended to an orthonormal basis of $V$.
\end{corollary}
\begin{proof}
      Pick $\left(e_1, \ldots ,e_k\right)$ orthonormal. Then they are linearly independed, hence can extend to a basis $\left(e_1,\ldots ,e_k,v_{k+1},\ldots ,v_n\right)$ of $V$. We apply the Gram-Schmidt algorithm to this basis to get an orthonormal set $S=\left(e_1,\ldots ,e_k,e_{k+1},\ldots ,e_n\right)$ with 
      \[\operatorname{span}S=\operatorname{span}\left(e_1,\ldots ,e_k,v_{k+1},\ldots ,v_n\right)=V.\]
      Hence this is an orthonormal basis of $V$.
\end{proof}

\begin{remark}
      If we have a matrix $A \in M_n \left(\mathbb{R}\right)$ (resp. $A \in M_n \left(\mathbb{C}\right)$), then the column vectors of $A$ are orthonormal iff 
      \[A^T A =\operatorname{Id} (\mathbb{R}), A^T\overline{A}\operatorname{Id} (\mathbb{C}).\]
\end{remark}

\begin{definition}[Orthogonal/unitary matrix]
      We say $A \in M_n(\mathbb{R})$ is \vocab{orthogonal} if $A^T A= \operatorname{Id}$. \newline 
      We say $A \in M_n(\mathbb{C})$ is \vocab{unitary} if $A^T \overline{A} = \operatorname{Id}$. 
\end{definition}

\begin{proposition}
      Let $A \in M_n(\mathbb{R})$ (resp. $M_n \left(\mathbb{C}\right)$) be non-singular. Then $A$ can be written as $A=RT$ where $T$ is upper triangular, and $R$ is orthogonal (resp. unitary).
\end{proposition}
\begin{proof}
      Apply Gram-Schmidt to the column vectors of $A$ (the fact $T$ is upper triangular is because the Gram-Schmidt algorithm only uses the previous vectors in the set).
\end{proof}

\subsection{Orthogonal complement and projection}
\begin{definition}[Orthogonal direct sum]
      Let $V$ be an inner product space, and $V_1, V_2 \leq V$. We say that $V$ is the \vocab{orthogonal direct sum} of $V_1$ and $V_2$ if 
      \begin{enumerate}
           \item $V=v_1 \oplus V_2$.
           \item $\forall (v_1,v_2) \in V_1 \times V_2: \ \langle v_1 ,v_2 \rangle =0 $.
      \end{enumerate}
\end{definition}
\begin{remark}
      The directness of the sum in 1. is redundant (it follows from 2.)
\end{remark}
\begin{definition}[Orthogonal complement]
      For $V$ an inner product space and $W \leq V$, the \vocab{orthogonal complement} of $W$ in $V$ is 
      \[W^{\perp}=\{v \in V: \ \forall w \in W, \ \langle v,w \rangle =0 \}.\]
\end{definition}

\begin{lemma}
      Let $V$ be a finite dimensional inner product space, and $W \leq V$. Then 
      \[V=W \oplus W^{\perp}.\]
\end{lemma}
\begin{proof}
     If $w \in W, w \in W^\perp$ then 
     \[||w||^2=\langle w,w \rangle  =0 \implies w=0.\]
     We need to show $V=W+W^\perp$. Let $\left(e_1, \ldots ,e_k\right)$ be an orthonormal basis of $W$. Extend it to an orthonormal basis of $V$: $(e_1,\ldots ,e_k, e_{k+1},\ldots ,e_n)$. Observe that $(e_{k+1},\ldots ,e_n)\in W^\perp$. Therefore $V=W+W^\perp$.
\end{proof}
\begin{remark}
      To make sense of this in infinite dimensions we would have to introduce a notion of \emph{topology}.
\end{remark}
\begin{definition}[Projection map]
     Suppose we can decompose a vector space $V$ as $V=U \oplus W$. ($U$ is a \vocab{complement} of $W$ in $V$). Now define 
     \[\pi: V \rightarrow W, \pi(v)=\pi(u+w)=w.\]
     We have that $\pi$ is well-defined, linear, and $\pi^2=\pi$. We say that $\pi$ is the \vocab{projection operator} onto $W$.
\end{definition}
\begin{remark}
     $\operatorname{Id}-\pi$ is the projection onto $U$. We can make the project map very explicit when $U=W^\perp$, and $V$ is an inner product space.
\end{remark}

\begin{lemma}
      Let $V$ be an inner product space. Let $W \leq V$ be finite dimensional. Let $(e_1, \ldots ,e_k)$ be an orthonormal basis of $W$. Then 
      \begin{itemize}
           \item[(i)] $\pi (v)=\sum_{i=1}^{k}\langle v,e_i\rangle  e_i  \ \forall v \in V $
           \item[(ii)] For all $v \in V$, for all $w \in W$, we have 
           \[||v-\pi (v)||\leq ||v-w||,\]
           with equality iff $w=\pi (v)$. ($\pi (v)$ is the point in $W$ which is closest to $v$.)
      \end{itemize}
\end{lemma}
\begin{proof}
     We prove both statements.
      \begin{itemize}
           \item[(i)]We define $\pi (v)= \sum_{i=1}^{k}\langle v,e_i \rangle e_i$. Since $W=\operatorname{span}\{e_1,\ldots ,e_k\}$, $\pi (v) \in W$. We write 
           \[v=\underbrace{v-\pi (v)}_{\in W^\perp}+\underbrace{\pi (v)}_{\in W}.\]
           Since we have claimed that $v-\pi (v) \in W^\perp$, we need to show for all $w \in W$, $\langle v-\pi (v), w \rangle =0$. This happens iff for all $1 \leq j \leq k$, 
           \[\langle v-\pi (v),w \rangle=0 .\]
           But 
           \begin{align*}
                \langle v-\pi (v), e_j \rangle &=\langle v,e_j \rangle -\langle \sum_{i=1}^{k}\langle v,e_i \rangle e_i,e_j \rangle\\&=\langle v,e_j \rangle - \sum_{i=1}^{k}\langle v,e_i \rangle \underbrace{\langle e_i,e_j \rangle }_{=\delta_{ij}}\\&=\langle v,e_j \rangle -\langle v,e_j \rangle \\&=0
           \end{align*}
           So our claim is true and hence $V=W +W^\perp$. Since $W \cap W^\perp=\{0\},$ we have $V=W \oplus W^\perp$.
           \item[(ii)] Let $v \in V$, $w \in W$, and let us compute 
           \begin{align*}
                ||v-w||^2&=||\underbrace{v-\pi (v)}_{\in W^\perp}+\underbrace{\pi (v)-w}_{\in W}||^2\\
                &=||v-\pi (v)||^2+ ||\pi (v)-w||^2\\
                \geq ||v-\pi (w)||^2
           \end{align*}
          with equality iff $w=\pi (v)$.
      \end{itemize}
\end{proof}
\subsection{The adjoint map}
\begin{definition}[Adjoint map]
      Let $V,W$ be finite dimensional inner product spaces and let $\alpha \in L (V,W)$. Then there is a unique linear map $\alpha^* : W \rightarrow V$ such that for all $(v,w) \in V \times W$, 
      \[\langle \alpha (v),w \rangle =\langle v,\alpha^* (w) \rangle .\]
      Moreover, if $B$ is an orthonormal basis of $V$ and $C$ is an orthonormal basis of $W$, then 
      \[[\alpha^*]_{C,B}=(\overline{[\alpha]_{B,C}} )^T.\]
\end{definition}
\begin{proof}
      We just do a brute force computation. Let $B =\{v_1, \ldots , v_n \} $ and $C= \{ \omega_1, \ldots , \omega_m\}$. Let $A=[\alpha]_{B,C}=(a_{ij})$. Let $C$ be the matrix be defined by $c_{ij}=\overline{a_{ij}} $. We compute 
      \begin{align*}
          \langle \alpha (\sum \lambda_i v_i), \sum \mu_i \omega_i \rangle &=\langle \sum_{i,k} a_{ki} \omega_k, \sum \mu_j \omega_j  \rangle \\
          &=\sum_{i,j} \lambda_i a_{ji} \overline{\mu_j} \text{ by orthonormality of } \omega_i.
      \end{align*}
      Then we also have 
      \begin{align*}
           \langle \sum_{i} \lambda_i v_i, \alpha^* \left( \sum_j \mu_J \omega_j \right) \rangle &=\langle \sum_{i} \lambda_i v_i, \sum_{j,k} \mu_j c_{kj}v_k \rangle \\&= \sum_{i,j}\lambda_i \overline{c_{ij}} \overline{\mu_j} 
      \end{align*}
      Therefore $\overline{c_{ij}} = a_{ij}$ so we have existence, and uniqueness also follows as our two expressions are equal iff $\omega_j=a_{ji}$.  
\end{proof}
\begin{remark}
      We denote $\overline{A} ^T=A^\dagger$. We are using the same notation $\alpha^*$ for both the adjoint and the dual map. We can check that if $V,W$ are real product inner spaces and $\alpha \in L (V,W)$, then we can define 
      \[\psi_{R,V}: V \rightarrow V^{*}, v \rightarrow \langle .,v \rangle\]
      and 
      \[\psi_{R,W}: W \rightarrow W^{*}, w \rightarrow \langle .,w \rangle.\]
      we can then find that the adjoint of $\alpha$ is given by: 
      \[W \underset{\psi_{R,W}}{\rightarrow }W^* \underset{\text{dual of } \alpha}{\rightarrow }V^* \underset{{\psi_{R,V}}^{-1}}{\rightarrow }V.\]
\end{remark}

\subsection{Self-adjoint maps and isometries}

\begin{proposition}
      Let $V$ be an inner product space, and $\alpha \in L (V)$. Let $\alpha^* \in L (V)$ be the adjoint map. Then the following are equivalent:
      \begin{enumerate}
           \item For all $(v,w) \in V \times V$, $\langle \alpha v,w \rangle =\langle (v),\alpha (w) \rangle $.
           \item $\alpha=\alpha^*$
      \end{enumerate}
      We call the map $\alpha$ \vocab{self-adjoint}. In $\mathbb{R}$ we call it symmetric, and in $\mathbb{C}$ we call it Hermitian.   
\end{proposition}

\begin{proposition}
     Let $V$ be an inner product space, and $\alpha \in L (V)$. Let $\alpha^* \in L (V)$ be the adjoint map. Then the following are equivalent:
     \begin{enumerate}
          \item For all $(v,w) \in V \times W$, $\langle \alpha (v), \alpha (w)\rangle =\langle v,w \rangle $.
          \item $\alpha={\alpha^*}^{-1}$.
     \end{enumerate}
     We call the map an \vocab{isometry}. In $\mathbb{R}$ we call it orthogonal, and in $\mathbb{C}$ we call it unitary.
\end{proposition}
\begin{proof}
      Prove both directions.\newline 
      \textbf{1 $\implies$ 2:}\newline 
      $\langle \alpha (v), \alpha (w) \rangle =\langle v,w \rangle$ 
      Now take $v=w$. $\implies ||\alpha (v)||^2=||v||^2$. Therefore $\operatorname{ker} \alpha=\{0\}$, and hence $\alpha$ is bijective (since we are in finite dimension). So ${\alpha}^{-1}$ is well defined. Then for all $(v,w)\in V \times V$, 
      \[\langle v,\alpha^* (w) \rangle =\langle \alpha (v), w \rangle =\langle \alpha (v), \alpha ({\alpha}^{-1}(w)) \rangle =\langle v,{\alpha}^{-1}(w)w \rangle .\]
      We have show that for all $v$ and $w$, 
      \[\langle v, (\alpha^* - {\alpha}^{-1})w \rangle =0.\]
      Thus if we choose $v=(\alpha^* - {\alpha}^{-1})w$, $||(\alpha^* - {\alpha}^{-1})w||^2=0$. So for all $w$, $(\alpha^* - {\alpha}^{-1})w=0$. So we can conclude that $\alpha^*={\alpha}^{-1}$.\newline 
      \textbf{2 $\implies$ 1:}\newline 
     If we have that $\alpha={\alpha^*}^{-1}$, then 
     \[\langle \alpha (v), \alpha (w) \rangle =\langle v, \alpha (\alpha^* (w)) \rangle=\langle v, \alpha ({\alpha}^{-1} (w)) \rangle= \langle v,w \rangle.\]
\end{proof}
\begin{remark}
      Using the polarisation identity, we can show that if $\alpha$ is an isometry if and only if for all $v \in V$, $||\alpha (v)||=||v||$ (it preserves the norm).
\end{remark}

\begin{lemma}
      Let $V$ be a finite dimensional real (complex) inner product space. Then given an $\alpha \in L (V)$, $\alpha$:
      \begin{itemize}
           \item[(i)] is self-adjoint iff for \emph{any} orthonormal basis $B$ of $V$, $[\alpha]_B$ is symmetric (Hermitian).
           \item[(ii)] is an isomety iff for \emph{any} orthonormal basis of $V$, $[\alpha]_B$ is orthogonal (unitary).
      \end{itemize}
\end{lemma}
\begin{proof}
      If $B$ is an orthonormal basis of $V$, we have that $[\alpha^*]_B=(\overline{[\alpha]_B})^T $.\newline 
     In (i) we have that $(\overline{[\alpha]_B} )^T=[\alpha]_B$, and in (ii) we have that $(\overline{[\alpha]_B} )^T={[\alpha]_B}^{-1}$.
\end{proof}
\begin{definition}[Orthogonal/unitary group]
      Let $V$ be a finite dimensional inner product space.\newline 
     In $\mathbb{R}$ we define the \vocab{orthogonal group} of $V$ by 
      \[O (V)=\{\alpha \in L (V): \ \alpha \text{ is an isometry} \}.\]
      In $\mathbb{C}$ we define the \vocab{unitary group} of $V$ by 
      \[U (V)=\{\alpha \in L (V): \ \alpha \text{ is an isometry} \}.\]
\end{definition}
\subsection{Spectral theory for self-adjoint maps and isometries}
\begin{lemma}
      Let $V$ be a finite dimensional inner product space. Let $\alpha \in L (V)$ be self-adjoint. Then
      \begin{enumerate}
           \item $\alpha $ has real eigenvalues.
           \item The eigenvalues of $\alpha$ wrt. different eigenvalues are orthogonal.
      \end{enumerate}
\end{lemma}
\begin{proof}
      \begin{enumerate}
           \item Let $\lambda \in \mathbb{C}$, $v \in V\setminus \{0\}$. Suppose $\alpha (v)= \lambda v$. Then 
           \begin{align*}
                \lambda ||v||^2&=\langle \lambda v,v \rangle\\
                &=\langle \alpha (v), v \rangle \\
                &=\langle v, \alpha (v) \rangle =\langle v,\lambda v \rangle \\&= \overline{\lambda} ||v^2||\\
                &\implies (\lambda-\overline{\lambda} )||v||^2=0\\
                &\implies \lambda=\overline{\lambda}  \implies \lambda \in \mathbb{R}.
           \end{align*}
           \item Let us consider two eigenvectors $v,w$ for different eigenvalues $\lambda, \mu $ respectively. Then
           \begin{align*}
                \lambda \langle v,w \rangle &=\langle \lambda v,w \rangle
                =\langle \alpha (v), w \rangle \\
                &=\langle v, \alpha (w) \rangle = \langle v, \mu w \rangle \\
                &=\overline{\mu} \langle v,w \rangle =\mu \langle v,w \rangle.
           \end{align*}
           Since $\lambda \neq \mu$, we have $\langle v,w \rangle =0$.
      \end{enumerate}
\end{proof}

\begin{theorem}[Spectral theorem for self-adjoint operators]
      Let $V$ be a finite dimensional inner product space. Let $\alpha \in L (V)$ be self-adjoint. Then $V$ has an orthonormal basis of eigenvectors of $\alpha$. This implies that $\alpha$ is diagonalisable.
\end{theorem}
\begin{proof}
      Let $F=\mathbb{R}$ or $\mathbb{C}$. We argue on induction on the dimension of $V$. The $n=1$ case is trivial. Suppose it holds for $n-1$ dimensions. Say $A=[\alpha]_B$ with respect to the fundamental basis $B$. By the Fundamental Theorem of Algebra, we know that the characteristic polynomial $\chi_A (\lambda)$ has a root. This root is an eigenvalue of $\alpha$, and so it is real. Let us call this real eigenvalue $\lambda \in \mathbb{R}$. Pick an eigenvector $v_1 \in V \setminus \{0\}$ such that 
      \[\alpha (v_1)= \lambda v_1, ||v_1||=1.\]
      Let $U=\langle v_1 \rangle^\perp \leq V$/
      The key observation is that $\alpha (U) \leq U$. Indeed, let $u \in U$, then 
      \[\langle \alpha (u), v_1 \rangle =\langle u,\alpha (v_1) \rangle =\langle u, \lambda v_1 \rangle =\lambda \langle u,v_1 \rangle =0.\]
      Therefore $\alpha (u)\perp v_1 \implies \alpha (u) \in U$. Hence we may consider $\alpha|_U \in L (U)$ which is also self-adjoint, and 
      \[n=\operatorname{dim}V=\operatorname{dim}U+1.\]
      Therefore $\operatorname{dim}U \leq n-1$, so we apply our induction hypothesis: there exist an orthonormal basis $(v_2,\ldots v_n)$ of eigenvectors of $\alpha|_U$. Since $V=\langle v_1 \rangle \oplus U$, we have that $(v_1,v_2, \ldots , v_n)$ is an orthonormal basis of $V$.
\end{proof}
\begin{corollary}
     If $V$ is a finite dimensional inner product space and $\alpha \in L (V)$ is self-adjoint, then $V$ is the orthogonal direct sum of all the eigenspaces of $\alpha$.
\end{corollary}
\begin{proof}
      Immediate.
\end{proof}

\begin{lemma}
      Let $V$ be a \emph{complex} inner product space. Let $\alpha \in L (V)$ be unitary ($\alpha={\alpha^*}^{-1}$). Then
      \begin{itemize}
           \item[(i)]All eigenvalues of $\alpha$ lie on the unit circle.
           \item[(ii)]Eigenvectors corresponding to different eigenvectors are orthogonal. 
      \end{itemize}
\end{lemma}

\begin{proof}
      \begin{itemize}
           \item[(i)] Let $\lambda \in \mathbb{C}$ and $v \in V \setminus \{0\}$ such that $\alpha (v)=\lambda v$. Note that $\lambda \neq 0$, since we showed that unitary maps are invertible ($\operatorname{ker}\alpha =0$). We compute 
           \[\lambda \langle v,v \rangle =\langle \lambda v, v \rangle =\langle \alpha (v), v \rangle =\langle v,\alpha^* (v) \rangle =\langle v,{\alpha}^{-1} (v) \rangle .\]
           Applying ${\alpha}^{-1}$, we have $v=\lambda{\alpha}^{-1}(v)$. Thus 
           \[\lambda \langle v,v \rangle =\langle v, {\alpha }^{-1}(v) \rangle=\langle v,\frac{1}{\lambda}v \rangle =\frac{1}{\overline{\lambda} }\langle v,v \rangle .\]
           So 
           \begin{equation*}
                 (\lambda \overline{\lambda}-1 ) ||v||^2=0 \implies |\lambda|=1.
           \end{equation*}
          \item[(ii)]Let us take two evecs $v,w$ with distinct eigenvalues $\lambda, \mu $ respectively. Then 
          \[\lambda \langle v,w \rangle =\langle \lambda v, w \rangle =\langle \alpha (v), w \rangle =\langle v, \alpha^* (w) \rangle =\langle v, {\alpha}^{-1} (w) \rangle =\langle v,\frac{1}{\mu}w \rangle=\frac{1}{\overline{\mu} } \langle v,w \rangle .\]
          Thus $(\lambda-\mu) \langle v,w \rangle =0$. Since $\lambda \neq \mu$, $\langle v,w \rangle =0$.
           
      \end{itemize}
\end{proof}

\begin{theorem}[Spectral theorem for unitary maps]
      Let $V$ be a finite $n$-dimensional \emph{complex} inner product space. Let $\alpha \in L (V)$ be unitary. Then $V$ has an orthonormal basis consisting of eigenvectors of $\alpha $.
\end{theorem}
\begin{proof}
      This is very similar to the self-adjoint case. Take $A=[\alpha]_B$, where $B$ is an orthonormal basis. Let $\chi_A (\lambda)$ be our characteristic polynomial. It has a complex root, say $\lambda \in \mathbb{C}$. Fix $v_1 \in V \setminus \{0\}$ with $\alpha (v_1)= \lambda v_1$ and $||v_1||=1$. Letting $U=\langle v_1 \rangle ^\perp $ as in the previous proof, we can show that $\alpha (U) \leq U$. Hence $\alpha|_U \in L (U)$, which is unitary and $\operatorname{dim} U=n-1$. By the inductive hypothesis, we can find an orthonormal basis of $U$ composed of evecs of $\alpha|_U $ and then combine it with $v_1$ to get an orthonormal basis of $V$.
\end{proof}

\begin{remark}
      We used the complex structure to assert that there is a complex eigenvalue. In general, a real valued orthonormal matrix $A$ with $A A^T=\operatorname{Id}$ CANNOT be diagonalised over $\mathbb{R}$.
\end{remark}

\begin{example}[Rotation in $\mathbb{R}^{2} $]
      Consider the matrix 
      \[A=\begin{pmatrix}
      \cos (\theta)&- \sin (\theta)\\
      \sin (\theta)&\cos (\theta)
      \end{pmatrix}
      .\]
      This is clearly an orthogonal matrix. However 
      \[\chi_A (\lambda)=(\cos (\theta)-\lambda)^2+\sin^2 (\theta).\]
      The roots of this are $\lambda=e^{\pm i \theta}$, so the roots are not generally real.
\end{example}

\subsection{Applications to bilinear forms}
\begin{corollary}
     Let $A \in M_n (\mathbb{R})$ (resp. $M_n (\mathbb{C})$) be a symmetric (resp. Hermitian) matrix. Then there is an orthogonal (resp. unitary) matrix $B$ such that $P^TAP$ (resp. $P^\dagger AP$) is diagonal with real-valued entries.
\end{corollary}
\begin{proof}
      Let $F=\mathbb{R}$ WLOG. Let $\langle \ , \ \rangle $ be the standard inner product over $\mathbb{R}^n$. Then $A \in L (F^n)$ is self-adjoint, hence we can find an orthonormal (for the standard inner product) basis $\mathcal{B} $ of $F^n$ such that $A$ is diagonal in that basis $\mathcal{B} =(v_1, \ldots ,v_n)$. Let 
      \[P= (v_1 | \ldots |v_n) \iff P \text{ is unitary} \iff P^TP=\operatorname{Id}.\]
      We know that ${P}^{-1}AP=D$, where $D$ is diagonal. Then 
      \[P^TAP={P}^{-1}AP=D.\]
      Its entries are real since the elements of $P$ on the diagonal, $(\lambda_i)_{1 \leq i \leq n}$ are real (eigenvalues of a symmetric operator).
\end{proof}

\begin{corollary}
     Let $V$ be a finite $n$-dimensional real (resp. complex) inner product space. Let $\varphi: V \times V \rightarrow F$ be a symmetric (resp. Hermitian) form. Then there exists an orthonormal basis of $V$ such that $\varphi$ in this basis is represented by a diagonal matrix.
\end{corollary}

\begin{proof}
      Let $B=(v_1,\ldots ,v_n)$ be an orthonormal basis of $V$. Let $A=[\varphi]_B$. We know $A=A^T$ (resp. $A^\dagger$), and hence there is an orthogonal (unitary) matrix $P$ such that $P^TAP$ ($P^\dagger AP$) is diagonal, say $D$. Let $(v_i)$ be the $i$th row of $P^T$ ($P^\dagger$), then $(v_1,\ldots , v_n)$ is an orthonormal basis $B$ of $V$ and 
      \[[\varphi]_B=D.\]
      This is using the change of basis formula for bilinear forms.
\end{proof}

\begin{remark}
      In this process, the diagonal entries of $P^TAP$ ($P^\dagger AP$) are the eigenvalues of $A$. Moreover, the signature $s (\varphi)=$ (no. positive evals of $A$)-(no. negative evals of $A$).
\end{remark}

\begin{corollary}[Simultaneous diagonalisation]
     Let $V$ be a finite dimensional real (resp. complex) vector space. Let $\varphi, \psi : V \times V \rightarrow F$ be symmetric (resp. Hermitian) bilinear forms. Assume that $\varphi$ is positive definite. Then there exists a basis $(v_1, \ldots ,v_n)$ of $V$ with respect to which \emph{both} forms are represented by a diagonal matrix. 
\end{corollary}
\begin{proof}
     The key point is that $\varphi$ is positive definite, which means that $V$ equipped with $\varphi$ is a finite dimensional inner product space. We define 
     \[\langle u,v \rangle =\varphi (u,v).\]
     Hence there exists an orthonormal (for the $\varphi$-induced scalar product) basis of $V$ in which $\psi$ is represented by a diagonal matrix (since it's symmetric). Observe that $\varphi$ in this basis is just the identity matrix (since $\varphi (v_i,v_j)=\langle v_i,v_j \rangle= \delta_{ij}$). So we are done.
\end{proof}
\begin{corollary}[Simultaneous diagonalisation for matrices]
     Let $A,B \in M_n (\mathbb{R})$ (resp. $M_n (\mathbb{C})$) that are symmetric (resp. Hermitian). Assume for all $x \neq 0$, 
     \begin{equation}
          \overline{x} ^TAx>0 \tag{$\ast$}
     \end{equation}
     
      Then there exists $Q \in M_n (\mathbb{R})$ (resp. $M_n (\mathbb{C})$) such that both matrices 
     \[Q^TAQ, Q^TBQ\]
     are diagonal.
\end{corollary}
\begin{proof}
      $(\ast)$ just expresses the fact that $A$ induces a positive definite symmetric bilinear form by $\overline{x} ^T Ax=\varphi (x,x)$. Then the proof follows from the previous result.
\end{proof}

\begin{remark}
      End of notes!
\end{remark}
\end{document}