\documentclass[egregdoesnotlikesansseriftitles,a4paper]{scrartcl}

\usepackage{martin}

\title{IB Statistics}
\author{Martin von Hodenberg (\texttt{mjv43@cam.ac.uk})}
\date{Last updated: \today}
\setcounter{section}{-1}

\allowdisplaybreaks

\begin{document}

\maketitle
These are my notes\footnote{Notes are posted online on \href{https://mjv43.user.srcf.net/}{my website}.} for the IB course Statistics, which was lectured in Lent 2022 at Cambridge by Dr S.Bacallado.

\newpage
\tableofcontents
\newpage

\section{Introduction}
Statistics can be defined as the science of \emph{making informed decisions}. It can include:
\begin{enumerate}
    \item Formal statistical inference
    \item Design of experiments and studies
    \item Visualisation of data
    \item Communication of uncertainty and risk
    \item Formal decision theory
\end{enumerate}
In this course we will only focus on formal statistical inference.
\begin{definition*}[Parametric inference]
     Let $X_1 , \ldots , X_n$ be iid. random variables. We will assume the distribution of $X_1 $ belongs to some family with parameter $\theta \in \Theta$.
\end{definition*}
\begin{example*}
    We will give some examples of such families:
     \begin{enumerate}
         \item $X_1 \sim \operatorname{Po}(\mu), \theta=\mu \in \Theta=(0,\infty )$ .
         \item $X_1 \sim N (\mu, \sigma^2) \quad N (\mu, \sigma^2)\in \Theta=\mathbb{R} \times (0, \infty)$.
     \end{enumerate}
\end{example*}
We will use the observed $X= (X_1 , \ldots X_n)$ to make inferences about $\theta$ such as:
\begin{enumerate}
    \item Point estimate $\theta (X)$ of $\theta$.
    \item Interval estimate of $\theta$: $(\theta_1 (x),\theta_2 (x))$ 
    \item Testing hypotheses about $\theta$: for example checking if there is evidence in $X$ against the hypothesis $H_0 : \ \theta=1$.
\end{enumerate}
\begin{remark}
     In general, we'll assume the distribution of the family $X_1 , \ldots , X_n$ is known but the parameter is unknown. Some results (on mean square error, bias, Gauss-Markov theorem) will make weaker assumptions.
\end{remark}
\newpage
\subsection{Probability}
First we will briefly recap IA Probability.

Let $\Omega$ be the \vocab{sample space} of outcomes in an experiment. A measurable subset of $\Omega$ is called an \vocab{event}. The set of events is denoted $\mathcal{F}$. 
\subsubsection*{Random variables}
\begin{definition*}[Probability measure]
     A probability measure $\mathbb{P}: \mathcal{F} \rightarrow [0,1]$ satisfies:
     \begin{enumerate}
         \item $\mathbb{P} (\emptyset)=0$ 
         \item $\mathbb{P}(\Omega)=1$
         \item $\mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i= \sum_{i}^{}\mathbb{P} (A_i)\right)$ if $(A_i)$ is a sequence of disjoint events.   
     \end{enumerate} 
\end{definition*}
\begin{definition*}[Random variable]
     A random variable is a (measurable) function $X: \Omega \rightarrow \mathbb{R}$.
\end{definition*}
\begin{example*}
     Tossing two coins has $\Omega= \left\{HH,HT,TH,TT\right\}$. Since $\Omega$ is countable, $\mathcal{F}$ is the power set of $\Omega$. We can define $X$ to be the random variable that counts the number of heads. Then \[
     X (HH)=2, X (HT)=X (TH)=1, X (TT)=0
     .\] 
\end{example*}
\begin{definition*}[Distribution function]
     The distribution function of $X$ is $F_X (x)=\mathbb{P} (X \leq x)$.
\end{definition*}
\begin{definition*}[Discrete/continuous random variable]
     A discrete random variable takes values in a countable set $S \subset \mathbb{R}$. Its probability mass function is \[
          p_X (x)=\mathbb{P}(X=x)
          .\] 
     
     A random variable $X$ has a continuous distribution if it has a probability density function $f_X (x)$ which satisfies \[
          \mathbb{P} (X \in A)=\int_A f_X (x) \mathrm{d}x
     ,\]
     for measurable sets $A$. 
\end{definition*}
\begin{definition*}[Expectation/variance]
     The expectation of $X$ is 
     \begin{equation*}
          \mathbb{E} (X)=
          \begin{cases}
              \sum_{x \in X}^{}x p_X (x) & X \text{ is discrete} \\
              \int_{-\infty }^{\infty} x f_X (x)\mathrm{d}x & X \text{ is continuous}
          \end{cases}
     \end{equation*}
     
     If $g: \mathbb{R} \to \mathbb{R} $, then for a continuous r.v \[
     \mathbb{E} (g (X))=\int_{-\infty }^{\infty} g (x) f_X (x)\mathrm{d}x
     .\] 
     The variance of $X$ is \[
          \operatorname{Var} (X)= \mathbb{E} [(X-\mathbb{E}(X))^2]
     .\] 
\end{definition*}
\begin{definition*}[Independence]
     We say $X_1 , \ldots ,X_n$ are independent if for all $x_1 , \ldots , x_n$ we have \[
     \mathbb{P} (X_1 \leq x_1 , \ldots ,X_n \leq x_n )=\mathbb{P}(X_1 \leq x_1) \ldots \mathbb{P}(X_n \leq x_n)
     .\] 
     If $X_1 , \ldots ,X_n $ have pdfs or pmfs $f_{X_1 }, \ldots,f_{X_n } $ then their joint pdf or pmf is \[
     f_X (x)=\prod_{i}f_{X_i}(x_i)
     .\] 
     If $Y=\max (X_1 , \ldots ,X_n)$ independent, then \[
     F_Y (y)=\mathbb{P} (Y \leq y)=\mathbb{P} (X_1 \leq y , \ldots ,X_n \leq y )=\prod_{i}F_{X_i}(y)
     .\] The pdf of $Y$ (if it exists) is obtained by differentiating $F_Y$.
\end{definition*}
\subsubsection*{Linear transformations}
Let $(a_1 , \ldots a_n)^T=a \in \mathbb{R}^{n}$ be a constant. \[
\mathbb{E} (a_1 X_1 +\ldots +a_n X_n)=\mathbb{E}(a^{T}X)=a^{T}\mathbb{E}(X)
.\]
This gives linearity of expectation (does not require independence). 
\[
\operatorname{Var}(a^{T}X)=\sum_{i,j}^{}a_{i}a_{j}\underbrace{\operatorname{Cov}(X_{i}, X_{j})}_{=\mathbb{E}((X_{i}-\mathbb{E}(X_{i})(X_{j}-\mathbb{E}(X_{j}))))} =a^{T}\operatorname{Var}(X)a
.\] 
where the matrix $[\operatorname{Var}(X)]_{ij}=\operatorname{Cov}(X_{i},X_{j})$. This gives the "bilinearity of variance".
\subsubsection*{Standardised statistics}
Let $X_1 , \ldots , X_n$ be iid. with $\mathbb{E}(X_1 )=\mu$ , $\operatorname{Var}(X_1)=\sigma^2$. We define $S_n=\sum_{i}^{}X_{i}$ and $\overline{X_n} \frac{S_n}{n} $ (the sample mean). By linearity \[
\mathbb{E} (\overline{X_n} )=\mu, \quad \operatorname{Var }(\overline{X_n} )= \frac{\sigma^2}{n}
.\]   
Define $Z_{n}= \frac{S_{n}-n \mu}{n}$. Then $\mathbb{E}(Z_{n})=0$ and $\operatorname{Var}(Z_{n})=1$. 
\subsubsection*{Moment generating functions}
\begin{definition*}[Moment generating function]
     The \vocab{moment generating function} (mgf) of a random variable $X$ is the function \[
          M_{x}(t)=\mathbb{E}(e^{tx})
     ,\] 
     provided that it exists for $t$ in some neighbourhood of 0.
\end{definition*} This is the Laplace transform of the pdf. It relates to moments of the pdf, for example $M_{x}^{(n)}(0)=\mathbb{E} (X^{n})$. 

Under broad conditions $M_{x}=M_{y} \iff F_{X}=F_{Y}$. (The Laplace transform is invertible.) The mgf is also useful for finding distributions of sums of independent random variables: 
\begin{example*}
     Let $X_1 , \ldots ,X_n \sim \operatorname{Po}(\mu)$. Then \[
     M_{X_{i}}(t)=\mathbb{E}(e^{tX_{i}})=\sum_{x=0}^{ \infty}e^{tx} \frac{e^{-\mu}\mu^{x}}{x!}=e^{-\mu}\sum_{x=0}^{ \infty}\frac{(e^{t}\mu^{x})}{x!}=e^{-\mu (1-e^{t})}
     .\] 
     What is $M_{S_{n}}$? We have \[
     M_{S_{n}}(t)=\mathbb{E}(e^{t (X_1 +\ldots +X_n)})=\prod_{i=1}^n e^{tX_{i}}=e^{-n \mu (1-e^{t})}
     .\] So we conclude $S_{n} \sim \operatorname{Po}(n \mu)$.
\end{example*}
\begin{example*}[mgf of the gamma distribution]
     If $X_{i} \sim \Gamma (\alpha_{i}, \lambda)$ for $i=1,\ldots ,n$ with $X_1 , \ldots X_n$ independent, then what is the distribution of $S_{n}=\sum_{i=1}^{n}X_{i}$? \[
          M_{S_{n}}(t)=\prod_{i}M_{X_{i}}(t)=
          \begin{cases}
              \left(\frac{\lambda}{\lambda t}\right)^{\sum_{i}^{}\alpha_{i}} & t<\lambda\\
              \infty & t>\lambda
          \end{cases}
          .\]  
          So $S_{n}$ is $\Gamma (\sum_{i}^{}a_{i}, \lambda)$. We call the first parameter the "shape parameter", and the second one the "rate parameter". A consequence of what we have just done is that if $X \sim \Gamma (\alpha, \lambda)$, then for all $b>0$ we have $bX \sim \Gamma (\alpha, \frac{\lambda}{b})$. 
          
          Special cases: 
          \begin{itemize}
              \item $\Gamma (1, \lambda)=\operatorname{Exp}(\lambda)$ 
              \item $\Gamma \left(\frac{k}{2},\frac{1}{2}\right)=\chi_k^2$ (the chi-squared distribution with $k$ degrees of freedom, i.e the distribution of a sum of $k$ independent squared $N (0,1)$ r.v's.)
          \end{itemize}
\end{example*}
\subsubsection*{Limits of random variables}
The weak law of large numbers states that $\forall \varepsilon >0$, as $n \rightarrow \infty$, \[
\mathbb{P} \left(|\overline{X_n} -\mu > \epsilon|\right) \rightarrow 0
.\] 
The strong law of large numbers states that as $n \rightarrow \infty$, \[
\mathbb{P}(\overline{X_{n}} \rightarrow \mu)=1
.\] The central limit theorem states that if we have the variable $Z_{n}= \frac{S_{n}-n \mu}{\sigma \sqrt{n}}$, then as $n \rightarrow \infty$ we have \[
\mathbb{P}(Z_{n} \leq z) \rightarrow \Phi (z) \quad \forall z \in \mathbb{R}
.\] where $\Phi$ is the distribution function of a $N (0,1)$ random variable.  
\subsubsection*{Conditional probability}
\begin{definition*}[Conditional probability]
     If $X,Y$ are discrete r.v's then \[
     P_{X|Y}(x|y)= \frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)}
     .\]   
     If $X,Y$ are continuous then the joint pdf of $X,Y$ satisfies: \[
     \mathbb{P}(X \leq x, P Y \leq y)=\int_{- \infty}^{x}\int_{- \infty}^{y}f_{X,Y} (x',y')\mathrm{d}y'  \mathrm{d}x' 
     .\] 
     The conditional pdf of $X$ given $Y$ is \[
     f_{x|y}= \frac{f_{X,Y}(x,y)}{\int_{- \infty}^{ \infty}f_{X,Y}(x,y) \mathrm{d}x }
     .\] 
     The conditional expectation of $X$ given $Y$ is 
     \begin{align*}
          \mathbb{E}(X|Y)=
          \begin{cases}
               \sum_{x}^{}xp_{X|Y}(x|Y) & \text{discrete}\\
               \int_{}^{}x f_{X|Y}(x|Y) \mathrm{d}x & \text{continuous}
          \end{cases}
     \end{align*}
     Note this is itself a random variable, as it is a function of $Y$. We define $\operatorname{Var}(X|Y)$ similarly. 
\end{definition*}
There are several notable properties of conditional random variables:
\begin{itemize}
     \item Tower property: $\mathbb{E}(\mathbb{E}(X|Y))=\mathbb{E}(X)$.
     \item Law of total variance: $\operatorname{Var}(X)=\mathbb{E}(\operatorname{Var}(X|Y))+\operatorname{Var}(\mathbb{E}(X|Y))$. 
     \item Change of variables (in 2D):

     Let $(x,y) \mapsto (u,v)$ be a differentiable bijection $\mathbb{R}^{2} \rightarrow \mathbb{R}^{2} $. Then \[
     f_{U,V}(u,v)=f_{X,Y}(x (u,v),y (u,v))|\det (J)|
     ,\] where $J=\frac{\partial (x,y)}{\partial (u,v)}$ is the Jacobian matrix we have seen before. 
\end{itemize}
\newpage
\section{Estimation}
Suppose $X_1 , \ldots X_n$ are iid observations with pdf or pdf (or pmf) $f_{X}(x| \theta)$ where $\theta$ is an unknown parameter in $\Theta$. Let $X=(X_1 , \ldots , X_{n})$. 
\begin{definition*}[Estimator]
     An estimator is a statistic or function of the data $T (X)=\hat{\theta}$ which does not depend on $\theta$, and is used to approximate the true parameter $\theta$. The distribution of $T (X)$ is called its "sampling distribution". 
\end{definition*}
\begin{example*}
     Let $X_1 , \ldots ,X_{n} \sim N (\mu,1)$ iid. Here $\hat{\mu}=\frac{1}{n}\sum_{i}^{}X_{i}=\overline{X_{n}}$. The sampling distribution of $\hat{\mu} $ is $T (X)=N (\mu, \frac{1}{n})$.   
\end{example*}
\begin{definition*}[Bias]
     The bias of $\hat{\theta}=T (X)$ is \[
     \operatorname{bias}(\hat{\theta})=\mathbb{E}_{\theta} (\hat{\theta})-\theta
     .\] Here $\mathbb{E}_{\theta}$ is the expectation in the model where $X_1 , X_2 , \ldots ,X_n \sim f_{X}(x|\theta)$.
\end{definition*}
\begin{remark}
     In general the bias is a function of true parameter $\theta$, even though it is not explicit in notation.  
\end{remark}
\begin{definition*}[Unbiased estimator]
     We say $\hat{\theta}$ is unbiased if $\operatorname{bias}(\hat{\theta})=0$ for all values of the true parameter $\theta$. 
\end{definition*}
In our example, $\hat{\mu}$ is unbiased because \[
\mathbb{E}_{\mu}(\hat{\mu})=\mathbb{E}_{\mu}(\overline{X_{n}})=\mu \quad \forall \mu \in \mathbb{R}
.\]  
\begin{definition*}[Mean squared error]
     The mean squared error (mse) of $\theta$ is \[
     \operatorname{mse}(\hat{\theta})=\mathbb{E}_{\theta} \left[ (\hat{\theta}-\theta)^2\right]
     .\] 
     It tells us "how far" $\hat{\theta}$ is from $\theta$ "on average".
\end{definition*}
\subsection{Bias-variance decomposition}
We expand the square in the definition of mse to get
\begin{align*}
     \operatorname{mse}(\hat{\theta})&=\mathbb{E}_{\theta} \left[ (\hat{\theta}-\theta)^2\right]\\
     &=\mathbb{E}_{\theta} \left((\hat{\theta}-\mathbb{E}_{\theta}\hat{\theta}-\theta)^{2}\right)
     &=\operatorname{Var}_{\theta}(\hat{\theta})+\operatorname{bias}^2 (\hat{\theta})\\
     & \geq 0
\end{align*}
There is a tradeoff between bias and variance. For example, let $X \sim \operatorname{Bin}(n,\theta)$. Suppose $n$ is known, and $\theta \in [0,1]$ is our unknown parameter. We define $T_{u}=\frac{X}{n}$, i.e the proportion of successes observed. Clearly $T_{u}$ is unbiased since \[
\mathbb{E}_{\theta} (T_{u})= \frac{E_{\theta}(X)}{n}=n \theta /n =\theta
.\] We can caculate \[
\operatorname{mse}(T_{u})=\operatorname{Var}_{\theta}(\frac{X}{n})= \frac{\operatorname{Var}_{\theta}}{n^2}= \frac{\theta (1-\theta)}{n}
.\] Consider another estimator $T_{B}= \frac{X+1}{n+2}=w \frac{X}{n}+ (1-w )\frac{1}{2}$ for $w=\frac{n}{n+2}$. This is called a "fixed estimator". In this case we have \[
\operatorname{bias}(T_{B})=\mathbb{E}_{\theta}(T_{B})-\theta=\mathbb{E}_{\theta}( \frac{X+1}{n+2})-\theta=\frac{n}{n+2}\theta +\frac{1}{n+2}-\theta
.\] This is $\neq 0$ for all but one value of $\theta$. Note that 
\begin{align*}
     \operatorname{Var}_{\theta}(T_{B})= \frac{\operatorname{Var}_{\theta}(X+1)}{(n+2)^2}\\
     \implies \operatorname{mse}(T_{B})=(1-w^2)\left(\frac{1}{2}-\theta\right)^2.
\end{align*}
\begin{remark}
     In this example, there are regions where either estimator is better. Prior judgement on the true value of $\theta$ determines which estimator is better. 
\end{remark}
Unbiasedness is not necessarily desirable. Let's look at a pathological example:
\begin{example*}
     Suppose $X \sim \operatorname{Po}(\lambda)$. We want to estimate $\theta= \mathbb{P} (X=0)^2=e^{-2\lambda}$. For some estimator $T (X)$ to be unbiased, we need \[
          \mathbb{E}_{\lambda}(T (x))=\sum_{x=0}^{ \infty}T (x) \frac{\lambda^{x}e^{-\lambda}}{x!}=e^{-2\lambda}=\theta \iff \sum_{x=0}^{ \infty}T (x) \frac{\lambda^{x}}{x!}=e^{-\lambda}=\sum_{x=0}^{ \infty}(-1)^{x} \frac{\lambda^{x}}{x!}  
          .\] The only function $T: N \rightarrow \mathbb{R}$ satisfying this equality is $T (x)=(-1)^{x}$. This is clearly an absurd estimator.
\end{example*}
\subsection{Sufficiency}
\begin{notation}
     From now on in the course we drop the $\theta$ subscript on expectations etc. in order to simplify notation.
\end{notation}
\begin{definition*}[Sufficiency]
     A statistic $T (X)$ is sufficient for $\theta$ if the conditional distribution of $X$ given $T (X)$ does not depend on $\theta$.
\end{definition*}
\begin{remark}
      $\theta$ can be a vector and $T (X)$ can also be vector-valued.
\end{remark}
\begin{example*}
      Let $X_1 , \ldots ,X_n$ be iid. Bernoulli$(\theta)$  variables for some $\theta$. Then \[
      f_{X}(X|\theta)=\prod_{i=1}^n \theta^{x_{i}}(1-\theta)^{1-x_{i}}=\theta^{\sum_{}^{}x_{i}} (1-\theta)^{n-\sum_{}^{}x_{i}}
      .\] This only depends on $x$ through $T (X)=\sum_{}^{}x_{i}$. To check it's sufficient: 
      \begin{align*}
           f_{X|T=t}(x|T=t)&= \frac{\mathbb{P}(X=x, T (x)=t)}{\mathbb{P} (T (x)=t)}\\
           \text{ If } \sum_{}^{}x_i =t, \quad &= \frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\mathbb{P}(T (x)=t)}\\
           &= \frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{{n \choose t}\theta^{t}(1-\theta)^{n-t}} \text{ since } \sum X_i \sim Bin (n,\theta)\\
           &= {n \choose t}^{-1}
      \end{align*}
      Therefore $T$ is sufficient.
\end{example*}
\begin{theorem}[Factorisation criterion]
     $T$ is sufficient for $\theta$ iff $f_{x}(x| \theta)=g (T (x), \theta)h (x)$ for suitable functions $g,h$.
\end{theorem}
\begin{proof}
      We will only prove this for the discrete case; the continuous case is similar. 
      
      \underline{Reverse implication:} Suppose $f_{x}(x| \theta)=g (T (x), \theta)h (x)$. Then if $T (x)=t$, we have 
      \begin{align*}
           f_{X|T=t} (x|T=t)&= \frac{\mathbb{P}(X=x,T (x)=t)}{\mathbb{P}(T (x)=t)}\\
           &= \frac{g t, \theta)h (x)}{\sum_{x': \ T (x')=t}^{}g (t, \theta)h (x')}\\
           &=\frac{h (x)}{\sum_{x': \ T (x')=t}^{}h (x')}.
      \end{align*}
      This doesn't depend on $\theta$ so $T$ is sufficient.

      \underline{Forward implication:} Suppose $T (X)$ is sufficient. Then we have 
      \begin{align*}
           f_X (x| \theta)&=\mathbb{P} (X=x, T (X)=T (x))\\
           &=\underbrace{\mathbb{P}(X=x| T (X)=T (x))}_{h (x)} \underbrace{\mathbb{P}(T(X)=T (x))}_{g (T (X),\theta)}.
      \end{align*}
      By noting that $\mathbb{P}(X=x| T (X)=x)$ only depends on $x$ by assumption and $\mathbb{P}(T(X)=T (x))$ only depends on $x$ through $T (x)$, we are done.
\end{proof}
\begin{remark}
      This criterion makes our previous example much easier.
\end{remark}
Let's look at another example.
\begin{example*}
      Let $X_1 , \ldots ,X_n \sim U ([0, \theta])$ be iid with $\theta >0$. Then \[
      f_{X}(x| \theta)=\prod_{i=1}^{n}\frac{1}{\theta} 1_{x_{i} \in [0, \theta]}=\frac{1}{\theta^{n}} 1_{\min_i x_{i} \geq 0} 1_{\max_i x_{i} \leq \theta} 
      .\] Define $T (x)=\max_i x_i$. Then we can write \[
      g (T (x), \theta)=\frac{1}{\theta^{n}}1_{\max_i x_{i} \leq \theta} , \quad h (x)=1_{\min_i x_{i} \geq 0}
      .\] So $T (x)$ is sufficient.
\end{example*}
\subsection{Minimal sufficiency}
Sufficient statistics are \textbf{not} unique. 
\begin{remark}
      Any bijection applied to a sufficient statistic yields another sufficient statistic.
\end{remark}
It's not hard to find sufficient statistics, for example $T (X)=X$ is a trivial sufficient statistic (that is useless!). Instead, we want statistics which give us `maximal' compression of the data in $X$. This motivates our next definition.
\begin{definition*}[Minimal sufficient statistic]
      $T (X)$ is \vocab{minimal sufficient} if for every other sufficient statistic $T'$, \[
      T' (x)=T' (y) \implies T \left(x\right)=T \left(y\right) \quad \forall x,y \in X^{n}
      .\] Note that it follows from this definition that minimal sufficient statistics are unique up to bijection.
\end{definition*}
\begin{theorem}
      Suppose that $f_{X}(x| \theta)/f_{X}(y| \theta)$ is constant in $\theta$ iff $T (x)= T (y)$. Then $T$ is minimal sufficient.
\end{theorem}
\begin{proof}
      Let $x \overset{1}{\sim}  y$ if $f_{X}(x| \theta)/f_{X}(y| \theta)$ is constant in $\theta$. It's easily checked that this defines an equivalence relation. Similarly, let $x \overset{2}{\sim} y$ if $T (x)=T (y)$; this is also an equivalence relation. The hypothesis in the theorem states that the equivalence classes of $\overset{1}{\sim} $ and $\overset{2}{\sim} $ are the same. 
      
      We will construct a statistic $T$ which is constant on the equivalence classes of $\overset{1}{\sim} $. For any value $t$ of $T$ let $z_{t}$ be a representative from $\left\{x; T (x)=t\right\}$. Then
      \begin{align*}
           f_{X}(x| \theta)&=f_{X}(z_{T (x)}|\theta)= \frac{f_{X}(x|\theta)}{f_{X}(z_{T (x)}| \theta)}\\
           &=g (T (x), \theta) h (x).
      \end{align*}
      Hence $T$ is sufficient by the factorisation criterion. To prove $T$ is minimal sufficient, let $S$ be any other sufficient statistic. By the factorisation criterion, there exist functions $g_{S}, h_{S}$ such that \[
      f_{X}(x| \theta)=g_{S}(S (x),\theta)h_{S}(x)
      .\] Now suppose $S (x)=S (y)$ for some $x,y$. Then \[
          \frac{f_{X}(x|\theta)}{f_{X}(y| \theta)}= \frac{g_{S}(S (x),\theta)h_{S}(x)}{g_{S}(S (y),\theta)h_{S}(y)}= \frac{h_{S}(x)}{h_{S}(y)}
      .\] which is constant in $\theta$, so $x \overset{1}{\sim} y$. By the hypothesis, $x \overset{2}{\sim} y$ and $T (x)=T (y)$. 
\end{proof}
\begin{example*}
      Suppose $X_1 , \ldots ,X_n \overset{\operatorname{iid}}{\sim} N (\mu, \sigma^2)$. Then 
      \begin{align*}
          \frac{f_{x}\left(\pi \mid \mu, \sigma^{2}\right)}{f_{x}\left(y \mid \mu, \sigma^{2}\right)}&=\frac{\left(2 \pi \sigma^{2}\right)^{-\pi / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i}\left(x_{i}-\mu\right)^{2}\right\}}{\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i}\left(y_{i}-\mu\right)^{2}\right\}}\\
          &=\exp \left\{-\frac{1}{2 \sigma^{2}}\left(\sum_{i} x_{i}^{2}-\sum_{i} y_{i}^{2}\right)+\frac{\mu}{\sigma^{2}}\left(\sum x_{i}-\sum y_{i}\right)\right\}.
      \end{align*}
      This is constant in $\left(\mu, \sigma^{2}\right)$ iff $\sum x_{i}^{2}=\sum y_{i}^{2}$ and $\Sigma x_{i}=\sum y_{i}$. Hence $\left(\sum x_{i}^{2}, \sum x_{i}\right)$ is a minimal sufficient statistic.A more common minimal sufficient statistic is obtained by taking a bijection of $\left(\Sigma x_{i}^{2}, \Sigma x_{i}\right)$ :
     $$
     \begin{aligned}
     &S(x)=\left(\bar{x}_{n}, S_{x x}\right) \\
     &\bar{x}_{n}=\frac{1}{n} \sum x_{i} \quad S_{x x}=\sum_{i}\left(x_{i}-\bar{x}_{n}\right)^{2}
     \end{aligned}
     $$
In this example $\theta=\left(\mu, \sigma^{2}\right)$ has same dimension as $S(x)$. In general, they can be different.
\end{example*}
\subsection{Rao-Blackwell theorem}
We will now look at the Rao-Blackwell theorem. This theorem allows us to start from any estimator $\widetilde{\theta}$, and then by conditioning on a sufficient statistic we get a better one.
\begin{theorem}[Rao-Blackwell theorem]
      Let $T$ be a sufficient statistic for $\theta$ and define an estimator $\widetilde{\theta}$ with $\mathbb{E}(\widetilde{\theta}^2)< \infty$ for all $\theta$. Define a new estimator \[
      \hat{\theta}=\mathbb{E} (\widetilde{\theta} \left|T (x)\right.)
      .\] Then for all $\theta \in \Theta$, \[
      \mathbb{E}((\hat{\theta}-\theta)^2) \leq \mathbb{E} ((\widetilde{\theta}-\theta)^2)
      .\] Furthermore, the inequality is strict unless $\widetilde{\theta}$ is a function of $T (x)$.
\end{theorem}
\begin{proof}
      By tower property of $\mathbb{E}$, we have \[
      \mathbb{E}(\hat{\theta})=\mathbb{E}(\mathbb{E}(\widetilde{\theta}|T))=\mathbb{E}(\widetilde{\theta})
      .\] By the conditional variance formula, 
      \begin{align*}
          \operatorname{Var}(\widetilde{\theta})&=\mathbb{E}(\operatorname{Var}(\widetilde{\theta}|T))+\operatorname{Var}(\mathbb{E}(\widetilde{\theta}|T))\\
          &=\mathbb{E}(\operatorname{Var}(\widetilde{\theta}|T))+\operatorname{Var}(\hat{\theta})\\
          &\geq \operatorname{Var}(\hat{\theta}).
      \end{align*}
      So by the bias-variance decomposition, $\operatorname{mse} \widetilde{\theta}\geq \operatorname{mse}\hat{\theta}$. The inequality is strict unless $\operatorname{Var}(\widetilde{\theta}|T)=0$ with probability 1, which requires $\widetilde{\theta}$ is a function of $T$.
\end{proof}
\begin{remark}
      $T$ must be sufficient, since otherwise $\hat{\theta}$ would be a function of $\theta$, so it wouldn't be an estimator.
\end{remark}
We will now look at a few examples to show how powerful this theorem can be.
\begin{example*}
      $X_1 , \ldots ,X_n \overset{\operatorname{iid}}{\sim} \operatorname{Poi}(\lambda)$. Let $\theta=\mathbb{P}(X_1 =0)=e^{-\lambda}$. Then 
      \begin{align*}
           f_{X}(x|\lambda)&= \frac{e^{-n \lambda}\lambda^{\Sigma x_{i}}}{\prod_{i}^{}x_{i}!}\\
           \implies f_{X}(x|\theta)&= \frac{\theta^{n}(-\log \theta)^{\sum_{}^{}x_i}}{\prod_{i=1}^{}x_{i}!}=g (\sum_{}^{}x_i, \theta)h (x)
      \end{align*}
      So $\sum_{}^{}x_i=T (x)$ is sufficient by factorisation.

      Recall $\sum_{}^{}X_{i} \sim \operatorname{Poi}(n \lambda)$. Let $\widetilde{\theta}=1_{X_1 =0}$ (which only depends on $X_1$, so it is a bad estimator). However, it is unbiased, which is desirable as the Rao-Blackwell process will then also yield an unbiased estimator. So let's calculate 
      \begin{align*}
           \hat{\theta}&=\mathbb{E}(\widetilde{\theta}|T=t)=\mathbb{P}(X_1 =0|\sum_{i=1}^{n}X_{i}=t)\\
           &= \frac{\mathbb{P}(X_1 =0, \sum_{i=1}^{n}X_{i}=t)}{\mathbb{P}(\sum_{i=1}^{n}X_{i}=t)}
      \end{align*}
\end{example*}
\begin{example*}
      Let $X_1 , \ldots , X_n $ be iid. $U ([0, \theta])$; want to estimate $\theta$.

      We previously saw that $T=\max_{i}X_i$ is sufficient. Let $\widetilde{\theta} =2 X_1 $, an unbiased estimator of $\theta$. Then 
      \begin{align*}
          \hat{\theta}&=\mathbb{E} (\widetilde{\theta} | T=t)=2 \mathbb{E} (X_1 |\max_{i}X_i=t )\\
          &=2 \mathbb{E} (X_1 |\max_{i}X_i=t, X_1 =\max_{i}X_i) \mathbb{P}(\max_{i}X_i=X_1| \max_{i}X_i=t )\\& \quad +2\mathbb{E} (X_1 |\max_{i}X_i=t, X_1 \neq \max_{i}X_i) \mathbb{P}(\max_{i}X_i \neq X_1 | \max_{i}X_i=t )\\
          &=\frac{2t}{n}+ \frac{2 (n-1)}{n}\underbrace{\mathbb{E}(X_1 | X_1 <t, \max_{2 \leq i \leq n}X_i =t)}_{=t/2 \text{ as } X_1 | X_1 <t \sim U ([0,t])} \\
          &= \frac{n+1}{n }\max_i X_i.
      \end{align*}
      By Rao-Blackwell $\operatorname{mse}(\hat{\theta}) \leq \operatorname{mse}(\widetilde{\theta} )$. Also, $\hat{\theta} $ is unbiased.
\end{example*}
\subsection{Maximum likelihood estimation}
\begin{definition*}[Likelihood function/Maximum likelihood estimator]
     Let $X_1 , \ldots ,X_n $ be iid. with pdf (or pmf ) $f_{X}(\cdot | \theta)$. The \vocab{likelihood function} $L: \theta \rightarrow \mathbb{R}$ is given by \[
     L (\theta)=f_{X}(x| \theta)=\prod_{i=1}^n f_{X_i}(x_{i}| \theta)
     .\] (We take $X$ to be fixed observations.) We further define the \vocab{log-likelihood} \[
     l (\theta)=\log L (\theta)=\sum_{i=1}^{n}\log f_{X_i}(x_{i}| \theta)
     .\]A \vocab{maximum likelihood estimator} (mle) is an estimator that maximises $L$ over $\Theta$.
\end{definition*}
\begin{example*}
      Let $X_1 , \ldots X_n \sim^{iid} $Ber$(p)$. Then we have
      \begin{align*}
          l (p)&=\sum_{i=1}^{n }X_{i} \log p+ (1-X_i) \log (1-p)\\
          &=\log p (\sum_{}^{}X_{i})+\log (1-p)(n-\sum_{}^{}X_i)\\
          \implies \frac{\mathrm{d}l}{\mathrm{d}p}= \frac{\sum_{}^{}X_i}{p}+ \frac{n-\sum_{}^{}X_i}{1-p}
      \end{align*}
      This is $>0$ iff $p=\frac{1}{n}\sum_{}^{}X_i= \overline{X_i}$. We have $\mathbb{E} (\overline{X_i})=\frac{n}{n}\mathbb{E} (X_1 )=p$. So the mle $\hat{p} =\overline{X_i}$ is unbiased.
\end{example*}
Now let's try a more involved example.
\begin{example*}
      Let $X_1 , \ldots X_n \sim^{iid} N(\mu, \sigma^2)$. Then we have
      \begin{align*}
           l (\mu, \sigma^2)&=-\frac{n}{2}\log ( 2\pi)-\frac{n}{2 }\log (\sigma^2)-\frac{1}{2\sigma^2 }\sum_{i}^{}(X_i-\mu)^2.
      \end{align*}
      This is maximised when $\frac{\partial l}{\partial \mu}=\frac{\partial l}{\partial \sigma^2}=0$. But $\frac{\partial l}{\partial \mu}=\frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i-\mu)$ so is equal to 0 iff \[
      \mu=\hat{X_n}=\frac{1}{n}\sum_{}^{}X_i
      .\] for all $\sigma^2 >0$.
      We also have that $\frac{\partial l}{\partial \sigma^2}=-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4} \sum_{i=1}^{n}(X_i-\mu)^2$.
      If we set $\mu=\overline{X_n}$, $\frac{\partial l}{\partial \sigma^2}=0$ iff \[
      \sigma^2= \frac{1}{n} \sum_{i=1}^{n}(X_i-\hat{X_n})^2= \frac{S_{xx}}{n}
      .\] Hence the mle is $(\hat{\mu},\hat{\sigma^2})=(\overline{X_n},\frac{S_{xx}}{n}) $. We can check $\hat{\mu}$ is unbiased. Later in the course we will see that \[
          \frac{S_{xx}}{\sigma^2}=\frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi_{n-1}^2
      .\] Therefore $\mathbb{E} (\sigma^2)= \frac{\sigma^2}{n}\mathbb{E}(\chi_{n-1}^2)= \frac{n-1}{n}\sigma^2 \neq \sigma^2.$ Hence $\hat{\sigma}^2$ is biased. But as $n \rightarrow \infty$ the bias converges to 0, so we say $\hat{\sigma}^2$ is \vocab{asymptotically unbiased}. 
\end{example*}
The next example will focus on an example where the mle is discontinuous, and doesn't behave as nicely.
\begin{example*}
     Let $X_1 , \ldots , X_n $ be iid. $U ([0, \theta])$. Recall the estimator we derived, $\hat{\theta}=\frac{n+1}{n }\max_i X_i$. The likelihood function is \[
     L (\theta)=\frac{1}{\theta^{n}} 1_{\max_i X_i \leq \theta}
     .\]
     \begin{figure}[H]
          \centering
          \incfig{70}{mle1}
          \caption{The plot of $L (\theta)$; note the discontinuity.}
     \end{figure}
     Hence the mle is $\hat{\theta}^{\operatorname{mle}}=\max_i X_{i}$. As $\hat{\theta}$ is unbiased, $\hat{\theta}^{\operatorname{mle}}$ is \textbf{not} unbiased.
\end{example*}
\subsubsection*{Properties of the mean likelihood estimator}
\begin{enumerate}
     \item If $T$ is sufficient for $\theta$, then the mle is a function of $T$. Recall \[
     L (\theta)=g (T, \theta)h (X)
     .\] So the maximiser of $L$ only depends on $X$ through $T$.
     \item If we parameterise $\theta$ in some way, say $\phi= H (\theta)$ where $H$ is a bijection, and $\hat{\theta}$ is the mle for $\theta$, then $H (\hat{\theta})$ is the mle for $\phi$.
     \item Asymptotic normality: Under regularity conditions, as $n \rightarrow \infty$ the statistic $\sqrt{n}(\hat{\theta}-\theta)$ is approx $N (0,\Sigma)$, i.e for some `nice' set $A$ we have \[
     \mathbb{P}(\sqrt{n}(\hat{\theta}-\theta) \in A) \xrightarrow{n \rightarrow \infty}\mathbb{P} (Z \in A), \quad \text{ where } Z \sim N (0, \Sigma)
     .\] The limiting covariance matrix $\Sigma$ is a known function of $L$. In some sense it is the `best' or `smallest' variance that any estimator can achieve asymptotically. See Part II Principles of Statistics for more on this.
     \item When the mle is not available analytically in closed form, in real-world applications it is often found numerically (see Part IB Numerical Analysis).
\end{enumerate}
\subsection{Confidence intervals}
The idea of confidence intervals is omnipresent; it is used in the real world to describe a measure of certainty, and you may well have used the term in conversation or seen it in media before. We will give a rigorous mathematical definition of confidence.
\begin{definition*}[Confidence interval]
      A $100 \cdot \gamma$\% \vocab{confidence interval} with $\gamma \in (0,1)$ and for a parameter $\theta$ is a random interval $(A (x),B (x))$ such that \[
      \mathbb{P}(A (x) \leq \theta \leq B (x)) = \gamma \quad \text{ for all } \theta \in \Theta
      .\] Note that we consider $\theta$ to be a fixed parameter, but the endpoints of the interval are randomly changing.
\end{definition*}
\begin{remark}
     When $\theta$ is a vector, we talk about confidence sets instead of confidence intervals.
\end{remark}
A \textbf{frequentist interpretation} is that if we repeat the experiment many times, on average $100 \cdot \gamma \%$ of the time the interval will contain $\theta$. 

A \textbf{misleading interpretation} is: ``having oberved $X=x$, there is now a probability $\gamma$ that $\theta \in [A (x),B (x)]$''. This is actually \textbf{incorrect}, and we will later see an example that shows this.
\begin{example*}
      Let $X_1 , \ldots, X_n \overset{\operatorname{iid}}{\sim} N (\theta,1)$. We want to find the 95\% confidence interval for $\theta$. We know $\overline{X} \sim N (\theta, \frac{1}{n})$ and $Z=\sqrt{n}(\overline{X}-\theta)\sim N (0,1)$ for all $\theta \in \mathbb{R}$. 

      Let $a,b$ be numbers s.t. $\Phi (b)-\Phi (a)=0.95$. Then \[
      \mathbb{P}(a \leq \sqrt{n}(\overline{X}-\theta)\leq b)=0.95
      .\] Rearrange: 
      \begin{align*}
           \mathbb{P}(\overline{X}-\frac{b}{\sqrt{n}}\leq \theta \leq \overline{X}-\frac{a}{\sqrt{n}})=0.95
      \end{align*}
      Hence $(\overline{X}-\frac{b}{\sqrt{n}},\overline{X}-\frac{a}{\sqrt{n}})$ is a 95\% C.I for $\theta$. 
      
      Note $a,b$ are not unique. Typically we centre the interval around some estimator $\hat{\theta}$ and aim to minimise its length. In this case, we would choose $a=-b$, which would give $b=Z_{0.025}\approx 1.96$ where $Z_{\alpha}$ is equal to ${\Phi}^{-1}(1-\alpha)$. We call this the ``upper $\alpha$-point'' of the $N (0,1)$ distribution.

      Therefore our final C.I is $(\overline{X}\pm \frac{1.96}{\sqrt{n}})$. A quick sanity check is to note that our interval decreases as $n$ gets larger (with more observations).
\end{example*}
We can generalise the method we used in this example.
\begin{remark}
      \underline{Recipe for finding a confidence interval}:
      \begin{enumerate}
           \item Find a quantity $R (X, \theta)$ whose $\mathbb{P}_{\theta}$-distribution \emph{doesn't} depend on $\theta$. This is called a \vocab{pivot}, for example in the above example our pivot was $R (X , \theta)=\sqrt{n}(\overline{X}-\theta)$.
           \item Write down a statement \[
           \mathbb{P} (c_1 \leq R (X,\theta) \leq c_2 )=\gamma
           .\] Given some $\gamma$, we find $c_1 ,c_2 $ using the distribution of $R$.
           \item Rearrange to leave $\theta$ in the middle of two inequalities.
      \end{enumerate}
\end{remark}
\begin{proposition}
      If $T$ is a monotone increasing function and $(A (X),B (X))$ is a $100 \cdot \gamma \%$ C.I for $\theta$, then $T (A (X),T (B (X)))$ is a $100 \cdot \gamma \%$ C.I for $T (\theta)$.
\end{proposition}
\begin{proof}
      Immediate from definitions. (Exercise)
\end{proof}
\begin{example*}
      Let $X_1 , \ldots, X_n \overset{\operatorname{iid}}{\sim} N (0,\sigma^{2} )$. We want to find a 95\% C.I for $\sigma^2$. Let's follow our recipe:
      \begin{enumerate}
           \item Note that $ \frac{X_1 }{\sigma}\sim N (0,1)$. This is a pivot, but ideally we would want one that depends on all the observations. So let our pivot be \[
               \sum_{i=1}^{n} \frac{X_{i}}{\sigma^2} \sim \chi_{n}^2
           .\] 
           \item Let $c_1 = {F_{\chi_{n}^2}}^{-1} (0.025) $ and $c_2 ={F_{\chi_{n}^2}}^{-1}(0.975)$.
           \item Now rearrange to get $\sigma^{2}$ in the middle: 
           \begin{align*}
                \mathbb{P}( \frac{\sum_{}^{}X_i^2}{c_2 }\leq \sigma^2 \leq \frac{\sum_{}^{}X_i^2}{c_1 })=0.95
           \end{align*}
           Hence this is our 95\% confidence interval for $\sigma^{2}$.
      \end{enumerate}
\end{example*}
\begin{example*}
      Let $X_1 , \ldots, X_n \overset{\operatorname{iid}}{\sim} \operatorname{Ber} (p)$ with $n$ ``large''. We want to find an approximate 95\% C.I for $p$.
     \begin{enumerate}
          \item The mle of $p$ is $\hat{p}=\overline{X}=\frac{1}{n}\sum_{}^{}X_{i}$. By the central limit theorem, $\hat{p}$ is approx $N (p,p (1-p)/n)$. Therefore \[
          \sqrt{n} \frac{(\hat{p}-p)}{\sqrt{p (1-p)}} \quad \text{ is approx } N (0,1)
          .\] 
          \item $\mathbb{P} (-Z_{0.025} \leq \sqrt{n}\frac{(\hat{p}-p)}{\sqrt{p (1-p)}} \leq Z_{0.025})\approx 0.95$.
          \item Note that if we wanted to rearrange for $p$ here, we would have to solve a quadratic inequality. So instead of this, we'll approximate $\sqrt{p (1-p)} \approx \sqrt{\hat{p} (1-\hat{p})}$. We argue when $n$ is large \[
               \mathbb{P} (-Z_{0.025} \leq \sqrt{n}\frac{(\hat{p}-p)}{\sqrt{\hat{p} (1-\hat{p})}} \leq Z_{0.025})\approx 0.95
          .\] This is easier to rearrange, which gives \[
          \mathbb{P} (\hat{p}-Z_{0.025} \sqrt{ \frac{\hat{p} (1-\hat{p})}{n}}\leq p \leq \hat{p}+Z_{0.025} \sqrt{ \frac{\hat{p} (1-\hat{p})}{n}})\approx 0.95
          .\] So we have found an approximate 95\% confidence interval for $p$.
     \end{enumerate}
\end{example*}
\begin{remark}
      In the above example, $p (1-p)\leq \frac{1}{4}$ on $p \in (0,1)$ hence $(\hat{p} \pm \frac{Z_{0.025}}{2 \sqrt{n}})$ is a ``conservative'' 95\% C.I for $p$.
\end{remark}
Let's go back to the issue of how to interpret a confidence interval, and the two interpretations that were mentioned. This can be seen in the following example:
\begin{example*}
      Suppose $X_1 ,X_2 $ are iid. $\operatorname{Unif}(\theta-\frac{1}{2}, \theta+\frac{1}{2})$. What is a sensible 50\% confidence interval for $\theta$? Note 
      \begin{align*}
           \mathbb{P} (\theta \text{ between }X_1 ,X_2  )&= \mathbb{P} (\min (X_1 ,X_2 ) \leq \theta \leq \max (X_1 , X_2 ))\\
           &=\mathbb{P} (X_1 \leq \theta \leq X_2 )+\mathbb{P} (X_2 \leq \theta \leq X_1 )\\
           &=\frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} =\frac{1}{2}.
      \end{align*}
      Hence the frequentist interpretation is \emph{exactly} correct.

      But suppose $\left|X_1 -X_2 \right|>0.5$. Then we \emph{know} that $\theta$ is in $(\min (X_1 ,X_2 ) , \max (X_1 , X_2 ))$ 
\end{example*}
\subsection{Bayesian analysis}
So far we've talked about \emph{frequentist} inference; we think of $\theta $ as being fixed. Inferential statements are interpreted in terms of repetitions of the experiment. 

\emph{Bayesian} analysis is a different framework. In this view, we treat $\theta $ as a random variable taking values in $\Theta $. The \vocab{prior distribution} $\pi \left(\theta \right)$ represents the investigator's beliefs or information about $\theta $ before observing the data. Conditional on $\theta $, the data $X$ has pdf (or pmf) $f_{X}\left( \cdot |\theta \right)$. (This is why we've been writing $f_{X}$ in this way when we use the frequentest interpretation.) Having observed $X$, the information in $X$ is combined with the prior to form the \vocab{posterior distribution} $\pi \left(\theta |X\right)$, which is the conditional distribution of $\theta $ given $X$. By Bayes' Rule: \[
\pi  \left(\theta |X\right)= \frac{\pi  \left(\theta \right)f_{X}\left(X|\theta \right)}{f_{X}\left(X\right)}
.\] where $f_{X}\left(x\right)$ is the marginal distribution of $X$: \[
f_{X}\left(X\right)=\begin{cases}
     \int_{\Theta  }^{}f_{X}\left(X|\theta \right)\pi \left(\theta \right)  \ \mathrm{d}\theta   &\text{ if } \theta \text{ continuous}  ;\\
     \sum_{\theta \in \Theta }^{}f_{X}\left(X|\theta \right)\pi  \left(\theta \right) &\text{ if } \theta \text{ discrete} .
\end{cases}
\] More simply, \[
\pi  \left(\theta |X\right) \propto \pi \left(\theta \right) \cdot f_{X}\left(X|\theta \right)
.\] Often, it is easy to recognize that the RHS is in some family of distributions up to a normalising constant.
\begin{remark}
     By the factorisation criterion, if $T$ is sufficient then \[
          \pi  \left(\theta |X\right) \propto \pi \left(\theta \right) \cdot g \left(T \left(X\right),\theta \right)
     .\] Therefore the posterior only depends on $X$ through $T \left(X\right)$, since we can absorb the $h \left(x\right)$ in the decomposition of $T$ into the constant.
\end{remark}
\begin{example*}
     Suppose a patient walks into a COVID-19 testing clinic (we have no prior info about the patient). Our random variable is $\theta = 1_{\text{patient infected}}$. We know the sensitivity of the test $f_{X}\left(X=1|\theta =1\right)$ and the specificity $f_{X}\left(X=0|\theta =0\right)$. 
     
     How do we choose the prior? Let $\pi  \left(\theta =1\right)$ to be the proportion of people in the UK with COVID on that day. What is the probability of an infection given a positive test? \[
     \pi \left(\theta =1 |X=1\right)= \frac{\pi  \left(\theta =1\right)f_{X}\left(X=1|\theta =1\right)}{\pi \left(\theta =1\right)f_{X}\left(X=1|\theta =1\right)+\pi \left(\theta =0\right)f_{X}\left(X=1|\theta =0\right)}
     .\] Sometimes $\pi \left(\theta =1\right)\ll \pi \left(\theta =0\right)$ which can make $\pi \left(\theta =1| X=1\right)$ small, which can be surprising. 
\end{example*}
In the second example, choosing the prior will be less clear-cut.
\begin{example*}
      Let $\theta \in [0,1]$ be the mortality rate for a new surgery in Addenbrooke's hospital. We observe the first 10 operations and see no deaths. We model $X \sim \operatorname{Bin}\left(10,\theta \right)$.

      How do we choose the prior? Suppose that in other hospitals mortality ranges between 3\% and 20\% with an average of 10\%. For example, take $\pi \left(\theta \right)\sim \beta \left(a,b\right)$. We can choose $a=3$ and $b=27$ so that $\pi \left(\theta \right)$ has mean 0.1 and $\pi \left(0.03<\theta <0.2\right) \approx 0.9$. The posterior is 
      \begin{align*}
          \pi \left(\theta |X\right)&\propto \pi \left(\theta \right) \cdot f_{X}\left(X=10|\theta \right)\\
          &\propto \theta ^{a-1} \left(1-\theta \right)^{b-1} \theta ^{x}\left(1-\theta \right)^{n-x}\\
          &\propto \theta ^{x+a-1}\left(1-\theta \right)^{b+n-x-1}\quad \text{ for } \theta \in [0,1].
      \end{align*}
      We recognise this as a $\beta \left(X+a, n+X-b\right)$ distribution. In our example this is $\beta \left(3,10+27\right)$. Plotting our prior and posterior, we can see that the posterior has a distribution shifted slightly to the left (since we observed no deaths in 10 trials), and that its variance is smaller (since we have already seen some trials).
      [picture]
      \begin{remark}
            In this example the prior and posterior are in the same family of distributions. This is called \vocab{conjugacy}.
      \end{remark}
\end{example*}
\subsubsection*{What do we do with the posterior?}
$\pi \left(\theta |X\right)$ represents information about $\theta $ \emph{after} seeing $X$. This can be used to make decisions under uncertainty.
\begin{enumerate}
     \item We must pick some decision $\delta \in D$. For example, $D=\left\{\text{ ask patient to isolate (or not) } \right\}$.
     \item Define the \vocab{loss function} $L \left(\theta ,\delta \right)$. For example, $L \left(\theta =1, \delta =1\right)$ would be the loss incurred by asking a patient to isolate if they test positive. 
     \item Pick $\delta $ that minimises \[
     \int_{\Theta }^{}L \left(\theta ,\delta \right) \pi \left(\theta |X\right)\ \mathrm{d}\theta 
     .\] This is called the ``\vocab{posterior expectation of loss}''.\footnote{See Von-Neumann/Morgenstein.}
\end{enumerate}
\subsubsection*{Point estimation}
An example of a decision is a ``best guess'' for $\theta $. The \vocab{Bayes estimator} minimises \[
h \left(\delta \right)=\int_{\Theta }^{} L \left(\theta ,\delta \right) \pi \left(\theta |X\right) \ \mathrm{d}\theta  
.\]
\begin{example*}
      If we choose a quadratic loss $L \left(\theta ,\delta \right)=\left(\theta -\delta \right)^{2}$. Then $h \left(\delta \right)=\int_{\Theta }^{} L \left(\theta -\delta \right)^{2} \pi \left(\theta |X\right) \ \mathrm{d}\theta  $. Differentiate: $h' \left(\delta \right)=0$ if 
      \begin{align*}
          \int_{\Theta }^{}\left(\theta -\delta \right)\pi \left(\theta |X\right) \ \mathrm{d}\theta =0 \\
          \iff \delta =\int_{\Theta }^{}\theta \pi \left(\theta |X\right) \ \mathrm{d}\theta . 
      \end{align*}
      This is $\hat{\theta}$ under quadratic loss.
\end{example*}
\begin{example*}
      If we use the absolute error loss $L \left(\theta ,\delta \right)=\left|\theta -\delta \right|$, then
      \begin{align*}
          h \left(\delta \right)&=\int_{\Theta }^{} \left|\theta -\delta \right| \pi \left(\theta |X\right) \ \mathrm{d}\theta\\
          &= \int_{-\infty}^{\delta }- \left(\theta -\delta \right) \pi  \left(\theta |X\right) \ \mathrm{d}\theta + \int_{\delta }^{\infty } \left(\theta -\delta \right) \pi  \left(\theta |X\right) \ \mathrm{d}\theta.
      \end{align*}
      Take the derivative wrt. $\theta $ using the FTC: \[
      h' \left(\theta \right)=\int_{-\infty}^{\delta }\pi  \left(\theta |X\right) \ \mathrm{d}\theta - \int_{\delta }^{\infty}\pi \left(\theta |X\right) \ \mathrm{d}\theta   
      .\] So $h^\prime \left(\delta \right)=0$ iff $\delta =\hat{\theta}$ is the median of the posterior $\pi \left(\theta |X\right)$.  
\end{example*}
We would also want some kind of Bayesian interpretation of a confidence interval. The next definition makes this more concrete.
\begin{definition*}[Credible interval]
      A $100 \cdot \gamma \%$ credible interval satisfies \[
      \pi (A (x)\leq \theta \leq B (x)|x)=\gamma 
      .\]
\end{definition*}
\begin{remark}
      Unlike confidence intervals, credible intervals \textbf{can} be interpreted conditionally; for example, ``given a specific observation $x$, we are 95\% certain that $\theta $ is in $(A,B)$''. The caveat here is that the credible interval depends on the choice of prior.
\end{remark}
We'll quickly look at some examples of Bayesian computations in order to get a better feel for them.
[todo]
\begin{remark}
      Asymptotically, we will often see credible intervals approach confidence intervals (as in the previous example).
\end{remark}
[next example]
This is where our discussion of Bayesian analysis ends - we now return to a frequentist viewpoint.
\newpage
\section{Hypothesis testing}
\subsection{Simple hypotheses}
\begin{definition*}[Hypothesis]
      A \vocab{hypothesis} is an assumption about the distribution of data $X$.
\end{definition*}
Scientific questions are often phrased as a decision between a \vocab{null hypothesis} $H_0 $ and an \vocab{alternative hypothesis} $H_1 $. 
\begin{example*}
      \begin{enumerate}
           \item $X = (X_1 ,\ldots,X_n)$ are iid. $\operatorname{Ber}(\theta )$. Suppose $H_0: \ \theta =1/2, \quad H_1 : \ \theta =3/4$.
           \item In 1. we could also have $X_1 :\ \theta \neq 1/2$.
           \item Let $X = (X_1 ,\ldots,X_n)$ be iid. where $X_i$ takes values in $\mathbb{N}$. Suppose \[
           H_0 : \ X_{i} \overset{\operatorname{iid}}{\sim} \operatorname{Poi}(\lambda ) \text{ for some } \lambda >0, \quad H_1 : \ X_{i} \overset{\operatorname{iid}}{\sim} f_{1} \text{ for some other dist. } f_{1}
           .\] This is known as a \vocab{goodness-of-fit test}.
      \end{enumerate}
\end{example*}
\begin{definition*}[Simple hypothesis]
      A \vocab{simple hypothesis} is one which fully specifies the pdf (resp. pmf) of $X$. Otherwise, we say the hypothesis is \vocab{composite}.
\end{definition*}
In the last example, the test in 1. was composite, and the test in 2. was simple.
\begin{definition*}[Test]
      A \vocab{test} of the null $H_0 $ is defined by a \vocab{critical region} $C \subseteq \mathcal{X}$. When $X \in C$, we \emph{reject the null}. When $X \notin C$, we \emph{fail to reject the null}.
      \begin{remark}
            When $X \notin C$, we simply don't find evidence to reject the null; it doesn't mean the null is false. We will see examples of this later.
      \end{remark}
\end{definition*}
\begin{definition*}[Error]
     There are two types of error:
     \begin{itemize}
          \item \vocab{Type I error}: rejecting $H_0 $ when $H_0 $ is true.
          \item \vocab{Type II error}: fail to reject $H_0 $ when $H_0 $ isn't true.
     \end{itemize}
\end{definition*}
Write
\begin{align*}
     &\alpha =\mathbb{P}_{H_{0}}(H_0 \text{ is rejected} )=\mathbb{P}_{H_{0}} (X \in C),\\
     &\beta  =\mathbb{P}_{H_{1}}(H_0 \text{ is not rejected} )=\mathbb{P}_{H_{1}} (X \in C).
\end{align*} The \vocab{size} of the test is $\alpha $, the \emph{power} is $1-\beta $. There is a tradeoff between $\alpha $ and $\beta $. What we typically do is choose an acceptable probability of type I errors (say 1\%); set $\alpha $ to that, and then pick the test which minimises $\beta $ (maximises power).
\begin{definition*}[Likelihood ratio statistic/test]
      Let $H_0 $ and $H_1 $ be simple, with $X$ having pdf (or pmf) $f_{i}$ under $H_{i}$ for $i \in \left\{0,1\right\}$. The \vocab{likelihood ratio statistic} is \[
      \Lambda_{x} (H_0 ,H_1 )= \frac{f_1 (x)}{f_0 (x)}
      .\] A \vocab{likelihood ratio test} (LRT) rejects when $\Lambda_{x} (H_0 ,H_1 )$ is large, i.e \[
      C= \left\{x: \Lambda_{x} (H_0 ,H_1 ) >k\right\} \text{ for some } k
      .\] 
\end{definition*}
\begin{lemma}[Neyman-Pearson lemma]\label{neymanpearson}
      Suppose that $f_0 ,f_1 $ are nonzero on some sets. Suppose there is $k >0$ such that the LRT with critical region $C= \left\{x: \Lambda_{x} (H_0 ,H_1 ) >k\right\}$ has size $\alpha $. Then out of all tests with size $\leq  \alpha $, this test has the smallest $\beta $. 
      \begin{remark}
            An LRT of size $\alpha $ does not always exist. (Exercise.) But in general, we can find a ``randomised'' test of size $\alpha $.
      \end{remark}
      \begin{proof}
          Let $\overline{C}$ be the complement of $C$. We know that the LRT has \[
          \alpha =\mathbb{P}_{H_0 } (X \in C) =\int_{C}^{}f_{0} (x) \ \mathrm{d} x, \quad \beta  =\mathbb{P}_{H_1 } (X \notin C) =\int_{\overline{C}}^{}f_{1} (x) \ \mathrm{d} x
          .\] Let $C^{*}$ be some other critical region with type I/II error probabilities $\alpha ^* ,\beta ^* $. Suppose $\alpha ^* \leq \alpha , \beta  \leq \beta ^*$. We want to show $\beta  \leq \beta ^*$. Note that
          \begin{align*}
              \beta -\beta ^* &= \int_{\overline{C}}^{}f_{1}(x) \ \mathrm{d}x - \int_{\overline{C^*}}^{}f_{1}(x) \ \mathrm{d}x \\
              &=\int_{\overline{C}\cap C^*}^{}f_{1}(x) \ \mathrm{d}x - \int_{\overline{C^*}\cap C}^{}f_{1}(x) \ \mathrm{d}x \text{ as the integrals over } \overline{C}\cap \overline{C^*} \text{ cancel }\\
              &=\int_{\overline{C}\cap C^*}^{} \underbrace{\frac{f_{1}(x)}{f_0 (x)}}_{\leq k \text{ on } \overline{C}}  f_0 (x) \ \mathrm{d}x - \int_{\overline{C^*}\cap C}^{}\underbrace{\frac{f_{1}(x)}{f_0 (x)}}_{> k \text{ on } C} f_0 (x) \ \mathrm{d}x\\
              &\leq k \left(\int_{\overline{C}\cap C^*}^{}f_{0}(x) \ \mathrm{d}x - \int_{\overline{C^*}\cap C}^{}f_{0}(x) \ \mathrm{d}x\right)\\
              &\text{Now add and subtract }k \int_{C \cap C^*}^{}f_0 (x) \ \mathrm{d}x:\\
              &=k \left(\int_{C^*}^{}f_{0}(x) \ \mathrm{d}x - \int_{C}^{}f_{0}(x) \ \mathrm{d}x\right)\\
              &= k (\alpha^*-\alpha) \leq 0.
          \end{align*}
    \end{proof}
\end{lemma}
Let's illustrate this with an example.
\begin{example*}
      Let $X_1 , \ldots, X_{n} \overset{\operatorname{iid}}{\sim}N (\mu ,\sigma_i ^2) $ where $\sigma_0 $ is known. We want the best size $\alpha $ test for $H_0 $: $\mu =\mu_0 $ vs $H_1 $: $\mu =\mu_1 $ for some fixed $\mu_1 > \mu_0 $. Skipping the algebra, we get \[
      \Lambda_{X}(H_0 ; H_1 )= \exp \left\{ \frac{\mu_1 -\mu_0 }{\sigma_0^2}n \overline{X} + \frac{n (\mu_0^2 -\mu_1^2 )}{2\sigma_0 ^2}\right\}
      .\] $\Lambda_{x}$ is monotone increasing in $\overline{X}$; it is also monotone increasing in $Z= \sqrt{n} (\overline{X}-\mu_0 )/\sigma_{0}$. Thus $\Lambda_{x}>k \iff z>k'$ for some $k'>0$. Hence the LRT has critical region of the form \[
      C=\left\{x: Z (x)>k'\right\}
      .\]  To find the most powerful test, by the Neyman-Pearson lemma, we need only find $k$ such that $C$ has size $\alpha$. Under $H_0 : \ \mu=\mu_0 $, $Z \sim N (0,1)$. Thus if we choose $k' =\Phi^{-1}(1-\alpha)$ we have \[
      \mathbb{P}_{H_0 }(z>k' )=\alpha
      ,\] i.e the test $C=\left\{x: Z (x)>k'\right\}$ has size $\alpha$. This is called a \vocab{$z$-test}.
\end{example*}
\subsubsection*{The $p$-value}
If we have a critical region $\left\{x: T (x)>k\right\}$ for some \vocab{test statistic} $T (X)$, we usually report a \vocab{$p$-value} in addition to the test's conclusion, which is defined by \[
p=\mathbb{P}_{H_0 }(T (X)>T (x^*))
,\] where $x^*$ is the observed data. 

In the example above, suppose $\mu_0 =5, \mu_1 =6, \alpha=0.05 $. Suppose we are given the data $x^* =(5.1,5.5, 4.9,5.3)$. We have $\overline{x^*}-5.2, z^*=0.4$. The LRT is \[
     \left\{x: Z (x)> \Phi^{-1} (0.95)=1.645\right\}
.\] . The conclusion of the LRT is that we do not reject $H_0 $. [drawing todo] Here $p=1-\Phi (z^*)=0.35$. 
\begin{proposition}
      Under $H_0$, the $p$-value is $\sim U[0,1]$.
\end{proposition}
\begin{proof}
     Let $F$ be the distribution of $T$. Assume that $F$ is continuous. Then 
     \begin{align*}
          \mathbb{P}_{H_0 }(p< u)&=\mathbb{P}_{H_0 } (1-F (T)<u)\\
          &=\mathbb{P}_{H_0 } (F (T)>1-u)\\
          &= \mathbb{P}_{H_0 } (T > F^{-1}(1-u))\\
          &= 1-F(F^{-1}(1-u))=u.
     \end{align*}
\end{proof}
\subsection{Composite hypotheses}
Let $X \sim f_{X}(\cdot |\theta)$, where $\theta \in \Theta$. We define a \vocab{composite hypothesis} 
\begin{align*}
     &H_0: \theta\in \Theta_{0} \subset \Theta\\
     &H_1: \theta\in \Theta_{1} \subset \Theta\\
\end{align*}
Now the probabilities of Type I or II error may depend on the value of $\theta$ within $\Theta_0 $ or $\Theta_1 $. They are not constants.
\begin{definition*}[Power function/size]
      The \vocab{power function} for a test $C$ is \[
      W (\theta)=\mathbb{P}_{\theta}(X \in C)
      .\] The \vocab{size} of a test $C$ is \[
      \alpha= \sup_{\theta \in \Theta_0 } W (\theta)
      .\]
\end{definition*}
We say that a test is \vocab{uniformly most powerful} (UMP) if for any other test $C^* $ with power function $W^*$ and size $\alpha$, \[
W (\theta) \geq W^* (\theta) \text{ for all } \theta \in \Theta_{1}
.\]\begin{remark}
      UMP tests need not exist. However, in simple models many LRTs are UMP.
\end{remark}
\begin{example*}
      One-sided test for normal location: let $X_1 ,\ldots,X_n \overset{\operatorname{iid}}{\sim} N (\mu, \sigma_0^2)$ with $\sigma_0^2$ known. We define our composite hypotheses: \[
      H_0 : \mu \leq \mu_0 , \quad H_1 : \mu >\mu_0 
      .\] The LRT for the simple hypotheses \[
      H_0 ^\prime : \mu=\mu_0 , H_1 ^\prime : \mu=\mu_1 >\mu_0 
      \] is \[
      C=\left\{x:\ z =\sqrt{n} \frac{(\overline{x}-\mu_0 )}{\sigma_0 }> \Phi^{-1}(1-\alpha)\right\}
      .\]
      \begin{claim}
           This test is UMP for $H_0 $ against $H_1 $.
      \end{claim}
      \begin{proof}
          Last time we found $W (\mu)=1- \Phi (z_0 + \frac{\sqrt{n} (\mu_0 -\mu)}{\sigma_0 })$. Indeed we have $\sup _{\mu \leq \mu_0 }W (\mu)=\alpha$. Now we need to check that for any test $C^*$ of size $\alpha$ with power function $W^* $, $W (\mu)\geq W^* (\mu)$ for all $\mu > \mu_0 $.

          First note that the critical region $C$ depends on $\mu_0 $, not on $\mu_1 $. Take any $\mu_1 > \mu_0 $, then $C $ is the LRT for $H_0 ^\prime$ vs $H_1 ^\prime $. We can alse see $C^* $ as a test of $H_0 ^\prime  $ vs $H_1 ^\prime $. And for these simple hypotheses $C^* $ has size \[
          W^* (\mu_0 ) \leq \sup_{\mu < \mu_0 }W^* (\mu) \leq  \alpha
          .\] By the Neyman-Pearson Lemma (\ref{neymanpearson}), $C$ has power no smaller than $C^* $ for $H_0 ^\prime  $ vs $H_1 ^\prime $, i.e \[
          W (\mu_1 ) \geq W^* (\mu_2 )
          .\] Hence $C $ is a UMP.
      \end{proof}
\end{example*}
\subsubsection*{Generalised likelihood ratio test}
Let $H_0 : \ \theta \in \Theta_0 $, $H_1 : \ \theta \in \Theta_1 $ with $\Theta_0 \subseteq \Theta_1 $. We say the hypotheses are nested. The \vocab{generalised likelihood ratio test} (GRT) is \[
\Lambda_{X}(H_0 ; H_1 )= \frac{\sup_{\theta \in \Theta_1 }f_{X}(X| \theta)}{\sup_{\theta \in \Theta_0 }f_{X}(X| \theta)}
.\] Large values indicate a better fit under the alternative hypothesis. A GLR test rejects $H_0 $ when $\Lambda_{X}(H_0 ; H_1 )$ is large.
\begin{example*}
       Again take our two-sided test for normal location. Let our nested hypotheses be \[
       H_0 :\ \mu=\mu_0 , \quad H_1 : \ \mu \in \mathbb{R}
       .\] In this model we have \[
            \Lambda_{X}(H_0 ; H_1 )= \frac{(2\pi \sigma_0 ^2)^{-n/2}\exp(- \frac{1}{2 \sigma_0 ^2}\sum_{i=1}^{n}(X_i-\overline{X})^2)}{(2\pi \sigma_0 ^2)^{-n/2}\exp(- \frac{1}{2 \sigma_0 ^2}\sum_{i=1}^{n}(X_i-\mu_0 )^2)}
       .\] To make it easier, we consider \[
       2 \log \Lambda_{x}=\frac{n}{\sigma_0 ^2}(\overline{X}-\mu_0 ^2)
       .\] Recall that under $H_0 $, $ \frac{\sqrt{n} (\overline{X}-\mu_0 )}{\sigma_0 }\sim N (0,1)$. So $2 \log \Lambda_{x} \sim \chi^2_1$. So the critical region of GLR test is \[
       C=\left\{x: \frac{n}{\sigma_0 ^2}(\overline{x}-\mu_0 ^2)>\underbrace{\chi_1 ^2 (\alpha)}_{\text{upper } \alpha \text{-point of } \chi^2_1 } \right\}
       .\]
\end{example*}
\begin{definition*}[Dimension of a hypothesis]
     The \vocab{dimension} of a hypothesis $H_0 : \theta \in \Theta_0 $ is the number of "free parameters" in $\Theta_0 $.
     \begin{example*}
           \begin{itemize}
                \item If $\Theta_0 =\left\{\theta \in \mathbb{R}^{k}: \ \theta_1 =\ldots=\theta_{p}=0\right\}$, then $\operatorname{dim}\Theta_0 =k-p$. 
               \item If $A \in \mathbb{R}^{p\times k}$ has linearly independent rows, and $b \in \mathbb{R}^{p}$ where $p<k$, and $\Theta_0 =\left\{\theta \in R^{k}: \ A \theta=b\right\}$, then $\operatorname{dim}\Theta_0 =k-p$.
               \item We could generalise this to $\Theta_0 $ being a differential manifold, which is overkill for this course, and we would need a notion of differential geometry which we have not yet encountered.
           \end{itemize}
     \end{example*}
\end{definition*}
\begin{theorem}[Wilks' theorem]
     Suppose $\Theta_0 \subset \Theta_1 $ and $\operatorname{dim}\Theta_1 -\operatorname{dim}\Theta_0 =p$. Then if $X=(X_1 , \ldots, X_n)$ are iid. under $f_{X}(\cdot | \theta)$ with $\theta \in \inf (\Theta_0 )$, then [under some topological conditions] the limiting distribution of $2\log \Lambda_{x}$ is $\chi_{p}^2$.
     \begin{remark}
           This is \emph{very} useful because it allows us to implement a generalised ratio test even if we can't find the exact distribution of $2 \log \Lambda_{x}$ (assuming that $n$ is large; any frequentist guarantee will be approximate).
     \end{remark}
     \begin{proof}
           Omitted; this is proved in Part II Principles of Statistics.
     \end{proof}
\end{theorem}
\begin{example*}
      In the two-sided normal location example, $\operatorname{dim}\Theta_0 =0$ and $\operatorname{dim} \Theta_1 =1$. So Wilks' theorem tells us $2 \log \Lambda_{X}$ is exactly $\chi_1^2$ (in this example, this happens to be exact). 
\end{example*}
\subsubsection*{Goodness-of-fit test}
Let $X_1 ,\ldots,X_n$ be iid. samples taking values in $\left\{1, \ldots, k\right\}$. Let $p_{i}=\mathbb{P}(X_1 =i)$, and let $N_{i}$ be the number of samples equal to $i$. Hence $\sum_{i}^{}N_{i}=n$, and $\sum_{i}^{}p_{i}=1$. 

We can view this as a model with parameters $p:= (p_1 , \ldots,p_{k})$. The parameter space has dimension $k-1$ (since we have one equality constraint). A \vocab{goodness-of-fit} (GoF) test has a null of the form \[
H_0 : p_{i}=\widetilde{p_{i}}, \quad i=1, \ldots, k
\] for some fixed distribution $\widetilde{p}$. The alternative puts no constraints on $p$.

The model is $(N_1 , \ldots, N_k) \sim \text{Multinomial}(n; p_1 , \ldots, p_{k})$. So $L (p) \propto p_1^{N_1 } \ldots p_{k}^{N_{k}}$. Hence \[
l (p)=\log L (p)= c+ \sum_{i}^{}N_{i} \log p_{i}
\] for some constant $c$. The GLR $\Lambda_{x}$ has 
\begin{align*}
     2 \log \Lambda_{x}&=2 (\sup_{p \in \Theta_1 }l (p)-\sup_{p \in \Theta_0}l (p))\\
     &= 2 (l (\hat{p}))-l (\widetilde{p})
\end{align*}
where $\hat{p}$ is the mle in this model. To find $\hat{p}$ we use Lagrange multipliers: \[
\mathcal{L}(p,\lambda)= \sum_{i}^{}N_{i}\log (p_{i})-\lambda (\sum_{}^{}p_{i}-1)=\ldots \implies \hat{p_i}=N_{i}/n
,\] i.e the fraction of samples equal to $i$. Therefore we derive the expression \[
2 \log \Lambda_{x}= 2 \sum_{i=1}^{k}N_{i} \log \left( \frac{N_{i}}{n \widetilde{p_i}}\right)
.\] We know $2 \log \Lambda_{x} \rightarrow \chi_{p}^2$ with $p=\operatorname{dim} \Theta_1 -\operatorname{dim} \Theta_0 = (k-1)-0=k-1$. A test of size approximately $\alpha$ (with $n$ large) rejects $H_0 $ when $2 \log \Lambda > \chi_{k-1}^2 (\alpha)$.

Now rewrite the test statistic: let $o_i=N_i$ be the ``observed number of samples of type $i$''. Let $e_i= n \widetilde{p_i}$ be the ``expectation under $H_0 $ of no. of samples of type $i$''. Using this notation we can write \[
2 \log \Lambda = 2 \sum_{i}^{}o_i \log \left( \frac{o_i}{e_i}\right)
.\] We can approximate this in order to form another statistic
\subsubsection*{The Pearson statistic}
In our goodness-of-fit model as before, write $\delta_i=o_i -e_i$. Then our test becomes \[
     2 \log \Lambda = 2 \sum_{i}^{}(e_i+\delta_i) \log \left( 1+\frac{\delta_i}{e_i}\right)
.\] But $\delta_i/o_i$ should be small when $n$ is large (since we would expect $\delta_i$, the difference of observed and expected values, to be small). Then we Taylor expand the logarithm. Also note that $\sum_{i}^{}\delta_i=\sum_{i}^{}(o_i-e_i)=n-n=0$. Thus
\begin{align*}
     2 \log \Lambda &= 2 \sum_{i}^{}(e_i+\delta_i) (\frac{\delta_i}{e_i}-\frac{\delta_i^2}{2e_i^2}+O (\frac{\delta_i^3}{e_i^3}))\\
     &\approx 2 \sum_{i}^{} (\delta_{i}+\frac{\delta_i^2}{e_i}-\frac{\delta_i^2}{2e_i})\\
     &=\sum_{i}^{}\frac{\delta_i^2}{e_i}=\sum_{i}^{}\frac{(o_i-e_i)^2}{e_i}.
\end{align*}
This is known as \vocab{Pearson's $\chi^2$ statistic}.
\begin{example*}
      The original example of this test is Mendel's experiment on genetics. He crossed different types of peas to obtain a sample of 556 descendants. Each descendent is one of four types, depending on colour (green or yellow) and texture (smooth or wrinkled). Let these types be denoted as SG, SY, WG, WY. 
      
      He observed $N= (315, 108, 102, 31)$. Mendel's theory gives a null hypothesis \[
      H_0 : p= \widetilde{p}= \left(\frac{9}{16}, \frac{3}{16}, \frac{3}{16},\frac{1}{16}\right)
      .\] We can compute $2 \log \Lambda=0.618$, and $\sum_{i}^{}\frac{(o_i-e_i)^2}{e_i}=0.604$. These are referred to a $\chi_3^2$ distribution (since we have 3 degrees of freedom). If we want to do a 5\% significance test, we would compute $\chi_3 ^2 (0.05)=7.815$, so a test of size $5\% $ does \emph{not} reject $H_0 $. The $p$-value in this case is $\mathbb{P} (\chi_3 ^2>0.6) \approx 0.96$. This is large that it is thought Mendel might have altered his results. 
\end{example*}
----1 or 2 lectures TODO----
\subsubsection*{Testing homogeneity}
Suppose a group of 150 patients are randomly assigned to three groups of equal size in order to test a new drug. Two sets get the drug with different doses, and the third gets a placebo.
\begin{table}[h]
     \caption{Results of the drug trial}
     \centering
     \begin{tabular}{c|ccc|c}
          Group & Improved & No difference & Worse & Total\\
          \hline
          Placebo &18&17&15&50\\
          Half dose & 20&10 &20&50\\
          Full dose& 25&13&12&50\\
          \hline
          &&&&150
     \end{tabular}
\end{table}

Let's develop a probability model for this test. Let the values in the $i$th row of the table be \[
N_{i1}, \ldots, N_{ic} \sim \operatorname{Multinomial}(n_{i+},p_{i1},\ldots, p_{ic}) \quad \text{ independently for } i=1, \ldots,r
.\] The null hypothesis is $H_0 : \ p_{i1}=p_{2j}=\ldots=p_{rj}$ for all $j =1 , \ldots,c$. 

The alternative is $H_1 : p_{i1},\ldots, p_{ic}$ is \emph{any} probability vector for each row $i=1,\ldots,r$. 

\underline{Under $H_1$:} We have \[
L (p)= \prod_{i=1}^{r} \frac{n_{i+}!}{N_{i1}! \ldots N_{ic !}}p_{i1}^{N_{i1}}\ldots p_{ic}^{N_{ic}}
.\] Taking logarithms, we get \[
l (p)= \text{ const. }+ \sum_{i,j}^{}N_{ij}\log (p_{ij})
.\] To find the MLE, we use the Lagrangian method with $\sum_{j}^{}p_{ij}=1$ for each $i=1, \ldots,r$. Therefore $\hat{p_{ij}}= N_{ij}/n_{i+}$ (the mle for the probability of column $j$ in row $i$ is just the empirical proportion of observations of column $j$ in row $i$).

\underline{Under $H_0$:} We have constrained the probabilities of each column to be equal across the rows, so let $p_{j}=p_{ij}$. As before, \[
     l (p)= \text{ const. }+  \sum_{i,j}^{}N_{ij}\log (p_{j})=\text{ const. }+\sum_{j}^{}N_{+j}\log (p_{j})
.\] Under the Lagrangian method with $\sum_{j}^{}p_{j}=1$, this gives $\hat{p_{j}}= N_{+j}/n_{++}$. Hence 
\begin{align*}
     2 \log \Lambda&=2 \sum_{i,j}^{}N_{ij} \log ( \frac{\hat{p_{ij}}}{\hat{p_{j}}})\\
     &=2 \sum_{i,j}^{}N_{ij} \log ( \frac{N_{ij}n_{++}}{n_{i+}N_{+j}}).
\end{align*}
This is the same statistic as for the $\chi^2$ test for independence! Furthermore, if $o_{ij}=N_{ij}$ and $e_{ij}=n_{i+}\hat{p_{j}}= \frac{n_{i+}N_{+j}}{n_{++}}$ we have \[
2 \log \Lambda = 2 \sum_{i,j}^{}\log ( \frac{o_{ij}}{e_{ij}})\approx \sum_{i,j}^{} \frac{(o_{ij}-e_{ij})^2}{e_{ij}}
.\] By Wilks' theorem $2 \log \Lambda \sim \chi_{d}^2$ approximately, where $d=\operatorname{dim} \Theta_1 - \operatorname{dim}\Theta_0 =r (c-1)-(c-1)= (r-1)(c-1)$. So the limiting distribution of $2 \log \Lambda$ is $\chi_{(r-1)(c-1)}^2$. This is the same as the chi-squared test for independence.
\begin{remark}
      The conclusion is that operationally the $\chi^2$ tests for independence and homogeneity are identical.
\end{remark}
\begin{example*}
      Let's go back to the case of the drug trial, with the data given in Table 1. Here $2 \log \Lambda=5.129$, and $\sum_{i,j}^{}\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=5.173$. We refer these to a $\chi_4^2$ distribution, and find that $\chi_4^2 (0.05)=9.468$. Hence we do not reject $H_0 $ with size $5\%$.
\end{example*}
\subsubsection*{Relationship between tests and confidence intervals}
\begin{definition*}[Acceptance region]
      Define the \vocab{acceptance region} $A$ of a test to be the complement of the critical region.
\end{definition*}
Let $X \sim f_{X}(\cdot | \theta)$ for some $\theta \in \Theta$.
\begin{theorem}
      \begin{enumerate}
           \item Suppose that for each $\theta_0 \in \Theta$ there is a test of size $\alpha$ with acceptance region $A (\theta_0 )$ for the null $H_0 : \ \theta =\theta_0 $. 
           
           Then $I (X)=\left\{\theta: X \in A (\Theta)\right\}$ is a $100 (1-\alpha)\%$ confidence set.
           \item Suppose $I (X)$ is a $100 (1-\alpha)\%$ confidence set for $\theta$. Then $A (\theta_0 )=\left\{X: \ \theta_0 \in I (X)\right\}$ is the acceptance region of a size $\alpha$ test for $H_0 : \ \theta =\theta_0 $.
      \end{enumerate}
      text
      \begin{proof}
            Observe that for both 1. and 2., the event $\theta_0 \in I (X) \iff X \in A (\theta_0 ) \iff $ we accept $H_0 $ that $\theta=\theta_0 $ in a test with data $X$.
      \end{proof}
\end{theorem}
\begin{example*}
      Let $X_1 , \ldots, X_{n} \overset{\operatorname{iid.}}{\sim} N (\mu, \sigma_0^2)$ where $\sigma_0 ^2$ is known. We found a $100 (1-\alpha)\%$ C.I  for $\mu$ is \[
      I (X)=\left(\overline{X} \pm \frac{Z_{\alpha/2}\sigma_0 }{\sqrt{n}}\right)
      .\] Using part 2 of the previous theorem we can find a test for $H_0: \ \mu=\mu_0 $ of size $\alpha$. 
      \begin{align*}
           A (\mu_0 )= \left\{x: \ \mu_0 \in I (x)\right\}=\left\{x: \ \mu_0 \in \left(\overline{X} \pm \frac{Z_{\alpha/2}\sigma_0 }{\sqrt{n}}\right)\right\}
      \end{align*}
      This is equivalent to rejecting $H_0 $ when \[
      \left|\sqrt{n} \frac{(\mu_0 -\overline{X})}{\sigma_0 }\right|> Z_{\alpha/2}
      .\] This is called a \vocab{2-sided test for normal location}.
\end{example*}
\subsection{Multivariate normal distribution}
Let $X = (X_1 , \ldots, X_n)$ be a vector of $n$ random variables. Recall that we defined \[
     \mathbb{E}(X)= \left(\mathbb{E}(X_1 ),\ldots, \mathbb{E}(X_{n})\right)
.\] and $\operatorname{Var}(X)$ is the matrix with \[
     \operatorname{Var}(X)_{ij}= \mathbb{E}(X_{i}-\mathbb{E}(X_{i}))\mathbb{E}(X_{j}-\mathbb{E}(X_{j}))
.\] Linearity of expectation: Let $A \in \mathbb{R}^{n \times n}, \ b \in \mathbb{R}^{n}$ be constant. Then \[
\mathbb{E}(AX+b)=A \mathbb{E}(X)+b, \quad \operatorname{Var}(AX+b)=A \operatorname{Var}(X)A^{T}
.\] 
\begin{definition*}[Multivariate normal distribution]
      We say that $X$ has a \vocab{multivariate normal (MVN)} distribution if for any fixed $t \in \mathbb{R}^{n}$, $t^T X \sim N (\mu, \sigma^2)$ for some $(\mu, \sigma^2)$.
\end{definition*}
\begin{proposition}
      If $X$ is MVN, then $AX+b $ is MVN.
      \begin{proof}
            Take any $t \in \mathbb{R}^{k}$. Then \[
            t^T (AX+b)= (A^{T}t)^{T}X+ t^T b
            .\] The first term is normal since $X$ is MVN and the second is just a constant scalar. Therefore this is $N (\mu+t^T b ,\sigma^2)$ where $(\mu, \sigma^2)$ are the mean and variance of $(A^{T}t)^{T}X$.
      \end{proof}
\end{proposition}
\begin{proposition}
      A MVN distribution is fully specified by its mean and covariance.
      \begin{proof}
            Let $X_1 , X_2 $ be MVN vectors, both with mean $\mu$ and variance matrix $\Sigma$. We'll show they have the same mgf, and hence the same distribution. Take $X_1 $:
            \begin{align*}
                 \mathbb{E} (e^{t^TX_1 })&= M_{t^TX_1 }(1) \quad \text{ but } t^TX_1 \text{ is univariate normal so} \\
                 &=\exp \left(1 \cdot \mathbb{E}(t^TX_1 )+\frac{1}{2}\operatorname{Var}(t^TX_1 )\cdot 1^2\right)\\
                 &=\exp \left( t^T \mu+ \frac{1}{2}t^T \Sigma t\right).
            \end{align*}
            Since this only depends on $\mu$ and $\Sigma$, we would obtain the same mgf for $X_2 $.
      \end{proof}
\end{proposition}
\subsubsection*{Orthogonal projections}
\begin{definition*}
      We say $P \in R^{n \times n}$ is an \vocab{orthogonal projection} onto $\operatorname{col}(P)$ if for all $v \in \operatorname{col}(P)$ we have $Pv =v $, and for all $w \in \operatorname{col}(P)^\perp$ we have $Pw =0$.
\end{definition*}
\begin{proposition}
      $P$ is an orthogonal projection iff it satisfies 
      \begin{itemize}
           \item Symmetry: $P=P^T$.
           \item Idempotency: $P^2=P$.
      \end{itemize}
      \begin{proof}
            This proof has essentially been given in IB Linear Algebra but we will repeat it here.

           \underline{$\impliedby:$} Take $v \in \operatorname{col}(P)$. By definition we can write $v=Pa $ for some $a \in \mathbb{R}^{n}$. Then $Pv = P^2 a= Pa =v$ by idempotency. Now take $w \in \operatorname{col}(P)^{\perp}$. By definition $P^{T}w=0$. So $Pw =P^Tw=0$ by symmetry.
           
           \underline{$\implies :$} We can write any $a \in \mathbb{R}^{n} $ uniquely as $a=v+w $, where $v \in \operatorname{col}(P)$ and $w \in \operatorname{col}(P)^{\perp}$. Then \[
           P^2 a = P^2 (v+w)=Pv= P (v+w)=Pa
           .\] This holds for all $a \in \mathbb{R}^{n} $, so $P$ is idempotent. For symmetry, take $u_1 , u_2 \in \mathbb{R}^{n} $. Note \[
           u_1^T (P^T (I-P))u_2 =\underbrace{(P u_1 )^{T}}_{\in \operatorname{col}(P)} \underbrace{ ((I-P)u_2 )}_{\in \operatorname{col}(P)^{\perp}} =0
           .\] Since this holds for all $u_1 ,u_2 $, we have $P^{T}(I-P)=0$, so $P^{T}=P^{T}P$. Taking the transpose gives $P=P^T$.
      \end{proof}
\end{proposition}
\begin{corollary}
      If $P$ is an orthogonal projection, so is $I-P$.
      \begin{proof}
            We know $P$ is symmetric and idempotent.
            \begin{align*}
                 (I-P)^T=I-P^T=I-P.\\
                 (I-P)^2=I -2P +P^2=I-2P +P=I-P.
            \end{align*}
            So $I-P$ is symmetric and idempotent.
      \end{proof}
\end{corollary}
\begin{proposition}
      If $P$ is an orthogonal projection, then $P=UU^T$ where the columns of $U$ are an orthonormal basis for $\operatorname{col}(P)$.
      \begin{proof}
            Check that $UU^T$ is an orthogonal projection: it is clearly symmetric and $(UU^T)=U \underbrace{U^TU}_{=I}U^T=UU^T $, so it is also idempotent. Furthermore, by definition $\operatorname{col}(P)=\operatorname{col}(UU^T)$ 
      \end{proof}
\end{proposition}
\begin{remark}
      $\operatorname{rank}(P)=\operatorname{tr}(P) $, since \[
          \operatorname{rank}(P)= \operatorname{tr}(U^TU) =\operatorname{tr}(UU^T) =\operatorname{tr}(P) 
      .\] 
\end{remark}
\begin{theorem}
      If $X$ is MVN, and $X \sim N (0, \sigma^2I)$ and $P$ is an orthogonal projection, then 
      \begin{enumerate}
           \item $PX\sim N (0, \sigma^2P)$ and $(I-P)X \sim N (0, \sigma^2 (I-P))$ and these are independent;
           \item $ \frac{\left||PX\right||^2}{\sigma^2}\sim \chi^2_{\operatorname{rank}(P)}$.
      \end{enumerate}
      \begin{proof}
            The vector $\begin{pmatrix} P\\I-P \end{pmatrix}$ is MVN as it is a linear function of $X$. Its distribution is fully specified by the mean and variance: 
            \begin{align*}
               \mathbb{E} \begin{pmatrix} PX\\(I-P)X \end{pmatrix}&= \begin{pmatrix} P\\I-P \end{pmatrix}\mathbb{E}(X)=0,\\
               \operatorname{Var}\begin{pmatrix} PX\\(I-P)X \end{pmatrix}&=\begin{pmatrix} P\\(I-P) \end{pmatrix}\sigma^2 I \begin{pmatrix} P\\(I-P) \end{pmatrix}^T\\
               &=\begin{pmatrix} P& \underbrace{P (I-P)}_{=0} \\ \underbrace{P (I-P)}_{=0}& I-P \end{pmatrix}.
            \end{align*}
            Let $Z \sim N (0, \sigma^2P), Z' \sim N (0, \sigma^2 (I-P))$ be independent. Then we can see that \[
            \begin{pmatrix} Z\\Z'\end{pmatrix} \sim N \left(0, \sigma^2 \begin{pmatrix} P&0\\0&I-P \end{pmatrix}\right) 
            .\] Hence this is equal in distribution to $\begin{pmatrix} PX\\(I-P)X \end{pmatrix}$, so $PX$ and $(I-P)X$ are independent.

            For 2, note that 
            \begin{align*}
               \frac{\left||PX\right||^2}{\sigma^2}&=\frac{X^TP^TPX}{\sigma^2}= \frac{X^T(UU^T)^T(UU^T)X}{\sigma^2}\\
               &=\frac{\left||U^TX\right||^2}{\sigma^2}
            \end{align*}
            where the columns of $U$ form an orthonormal basis. Let $r=\operatorname{rank}(P)$. Now $U^TX \sim N (0, \sigma^2 U^TU) \sim N (0, \sigma^2 I_{r})$ so $ \frac{(U^TX)_{i}}{\sigma} \sim N (0,1)$ iid. for $i= 1, \ldots, r$. We conclude that \[
               \frac{\left||PX\right||^2}{\sigma^2}= \sum_{i=1}^{r} \left(\frac{(U^TX)_{i}}{\sigma}\right)^2 \sim \chi^2_{r}
            .\] 
      \end{proof}
\end{theorem}
This can be applied immediately in another theorem:
\begin{theorem}
     Let $X_1 , \ldots, X_n \sim N (\mu, \sigma^2)$ iid. for some unknown $\mu \in \mathbb{R}, \sigma^2>0$. Recall that the mles are \[
          \hat{\mu}= \overline{X}= \frac{1}{n} \sum_{i}^{}X_{i}, \quad \hat{\sigma}^2= \frac{S_{xx}}{n}= \frac{\sum_{i}^{}(X_{i}-\overline{X})^2}{n}
     .\] We have that 
     \begin{enumerate}
          \item $\overline{X} \sim N (\mu, \frac{\sigma^2}{n})$;
          \item $ \frac{S_{xx}}{\sigma^2} \sim \chi^2_{n-1}$;
          \item $\overline{X}, S_{xx}$ are independent.
     \end{enumerate}
     \begin{proof}
           We already proved 1 previously. To show 2, define an $n \times n$ matrix $P$ with every entry being $1/n$. It's easy to check that $P$ is symmetric and idempotent, hence it is a projection matrix. Now \[
           PX =P \begin{pmatrix} X_1 \\ \ldots \\ X_n \end{pmatrix}= \begin{pmatrix} \overline{X} \\\ldots \\\overline{X} \end{pmatrix}
           .\] We'll write $X= \begin{pmatrix} \mu\\ \ldots\\ \mu \end{pmatrix}+\epsilon$ where $\epsilon \sim N (0, \sigma^2 I)$. Consider that 
           \begin{itemize}
                \item $\overline{X}$ is a function of $P \epsilon$: \[
                \overline{X}= (PX)_{1}= (P \begin{pmatrix} \mu\\ \ldots\\ \mu \end{pmatrix}+ P \epsilon)_1
                .\] 
                \item $S_{xx}$ is a function of $(I-P) \epsilon$:
                \begin{align*}
                     S_{xx}&= \sum_{i}^{}(X_i-\overline{X})^2\\
                     &= \left|| (I-P)X\right||^2\\
                     &= \left|| (I-P) \epsilon\right||^2.
                \end{align*}
           \end{itemize}
           Therefore $\overline{X}$ and $S_{xx}$ are independent, and noting that $I-P$ is a projection with $\operatorname{rank}(I-P)=\operatorname{tr}(I-P) = n-1$, we can apply the previous theorem to obtain $S_{xx}= \left||(I-P) \epsilon \right|| ^2 \sim \chi^2_{n-1}$.
     \end{proof}
\end{theorem}
\subsection{Linear models}
Suppose we have data $(x_1 , y_1 ), \ldots, (x_n , y_{n})$ where $y_{i}\in \mathbb{R}$ and $x_i \in \mathbb{R}^p$. It is convention to use $n$ for the number of samples and $p$ for their dimension. We call the $y_i$ the \vocab{dependent variables}, and the $x_{i1},\ldots, x_{ip}$ the \vocab{independent variables}.
\begin{goal}
     To predict the behaviour of the $y_{i}$ for a given $x_{i}$ using a \vocab{linear model}.
\end{goal}
So what is a linear model? It involves making the assumption \[
y_i=\alpha+ \beta_1 x_{i1} +\ldots+ \beta_p x_{ip}+ \epsilon_i
.\] Here $\alpha$ is the \emph{intercept}, the $\beta \in \mathbb{R}^{p}$ are the \emph{coefficients} and $\epsilon_{i}$ is a random variable (the \emph{noise}).
\begin{remarks}
      \begin{itemize}
           \item We will eliminate the intercept by making $x_{i1 }=1$ for all $i$, so $\beta_1 $ plays the role of the intercept.
           \item A linear model can also model nonlinear relationships: for example, $y_{i}=a+bz_{i}+cz_{i}^2+\epsilon_{i}$ can be rephrased as a linear model with $x_{i}=(1, z_{i},z_{i}^2)$.
           \item $\beta_{j}$ can be interpreted as the effect on $y_{i}$ of increasing $x_{ij}$ by 1, while keeping every other predictor fixed. This effect cannot be interpreted causally unless this is a randomised control experiment.
      \end{itemize}
\end{remarks}
We can formulate a linear model as a matrix equation: \[
Y= \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}, \ X = \begin{pmatrix} x_{11} &\ldots& x_{1p}\\ \vdots & \ddots & \vdots\\x_{n1} &\ldots& x_{np} \end{pmatrix}, \ \beta= \begin{pmatrix} \beta_1 \\ \vdots \\ \beta_{p} \end{pmatrix}, \ \epsilon= \begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}
.\] Then $Y=X \beta+\epsilon$.
\subsubsection{Moment assumptions}
\begin{enumerate}
     \item $\mathbb{E}(\epsilon)=0$, which implies $\mathbb{E} (y_{i})=x_{i}^T \beta$.
     \item $\operatorname{Var}\epsilon =\sigma^2 I$. This is true if and only if 
     \begin{itemize}
          \item $\operatorname{Var}\epsilon_{i}=\sigma^2$ (\emph{homoskedasticity})
          \item $\operatorname{Cov}(\epsilon_{i},\epsilon_{j})=0$ for $i \neq j$.
     \end{itemize}
\end{enumerate}
Initially, we won't assume anything else about the distribution of $\epsilon$. We will always assume that the design matrix $X$ has full rank $p$ (its columns are linearly independent). Since $X \in \mathbb{R}^{n \times p}$, this requires $n \geq p$. (We need at least as many samples as predictors.)
\subsubsection{Least squares estimator}
The \vocab{least squares estimator} $\hat{\beta}$ minimises the \emph{residual sum of squares} \[
S (\beta)= \left|| Y-X \beta\right||^2 = \sum_{i}^{}(y_{i}-x_{i}^T \beta)^2
.\] This is a positive definite quadratic polynomial in $\beta$, so it is minimised at the point where $\nabla S (\beta)=0$. 
\begin{align*}
     \implies -2 \sum_{i=1}^{n}x_{ik}(y_{i}-\sum_{j}^{}x_{ij}\hat{\beta}_{ij})=0 \ \forall k=1 , \ldots, p\\
     \implies X^TX \hat{\beta}=X^{T}Y\\
     \implies \hat{\beta}= (X^TX)^{-1} X^T Y.
\end{align*}
($X^TX$ is invertible since $X$ has full rank.) 
\begin{remarks}
      \begin{itemize}
           \item $\mathbb{E}(\hat{\beta})=(X^TX )^{-1} X^T \mathbb{E}(Y)=(X^TX )^{-1} X^TX  \beta=\beta$. So $\hat{\beta}$ is unbiased.
      \end{itemize}
\end{remarks}
\end{document}