\documentclass[egregdoesnotlikesansseriftitles,a4paper]{scrartcl}

\usepackage{martin}

\title{IB Statistics}
\author{Martin von Hodenberg (\texttt{mjv43@cam.ac.uk})}
\date{Last updated: \today}
\setcounter{section}{-1}

\allowdisplaybreaks

\begin{document}

\maketitle
These are my notes\footnote{Notes are posted online on \href{https://mjv43.user.srcf.net/}{my website}.} for the IB course Statistics, which was lectured in Lent 2022 at Cambridge by Dr S.Bacallado.

\newpage
\tableofcontents
\newpage

\section{Introduction}
Statistics can be defined as the science of \emph{making informed decisions}. It can include:
\begin{enumerate}
    \item Formal statistical inference
    \item Design of experiments and studies
    \item Visualisation of data
    \item Communication of uncertainty and risk
    \item Formal decision theory
\end{enumerate}
In this course we will only focus on formal statistical inference.
\begin{definition*}[Parametric inference]
     Let $X_1 , \ldots , X_n$ be iid. random variables. We will assume the distribution of $X_1 $ belongs to some family with parameter $\theta \in \Theta$.
\end{definition*}
\begin{example*}
    We will give some examples of such families:
     \begin{enumerate}
         \item $X_1 \sim \operatorname{Po}(\mu), \theta=\mu \in \Theta=(0,\infty )$ .
         \item $X_1 \sim N (\mu, \sigma^2) \quad N (\mu, \sigma^2)\in \Theta=\mathbb{R} \times (0, \infty)$.
     \end{enumerate}
\end{example*}
We will use the observed $X= (X_1 , \ldots X_n)$ to make inferences about $\theta$ such as:
\begin{enumerate}
    \item Point estimate $\theta (X)$ of $\theta$.
    \item Interval estimate of $\theta$: $(\theta_1 (x),\theta_2 (x))$ 
    \item Testing hypotheses about $\theta$: for example checking if there is evidence in $X$ against the hypothesis $H_0 : \ \theta=1$.
\end{enumerate}
\begin{remark}
     In general, we'll assume the distribution of the family $X_1 , \ldots , X_n$ is known but the parameter is unknown. Some results (on mean square error, bias, Gauss-Markov theorem) will make weaker assumptions.
\end{remark}
\newpage
\subsection{Probability}
First we will briefly recap IA Probability.

Let $\Omega$ be the \vocab{sample space} of outcomes in an experiment. A measurable subset of $\Omega$ is called an \vocab{event}. The set of events is denoted $\mathcal{F}$. 
\subsubsection*{Random variables}
\begin{definition*}[Probability measure]
     A probability measure $\mathbb{P}: \mathcal{F} \rightarrow [0,1]$ satisfies:
     \begin{enumerate}
         \item $\mathbb{P} (\emptyset)=0$ 
         \item $\mathbb{P}(\Omega)=1$
         \item $\mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i= \sum_{i}^{}\mathbb{P} (A_i)\right)$ if $(A_i)$ is a sequence of disjoint events.   
     \end{enumerate} 
\end{definition*}
\begin{definition*}[Random variable]
     A random variable is a (measurable) function $X: \Omega \rightarrow \mathbb{R}$.
\end{definition*}
\begin{example*}
     Tossing two coins has $\Omega= \left\{HH,HT,TH,TT\right\}$. Since $\Omega$ is countable, $\mathcal{F}$ is the power set of $\Omega$. We can define $X$ to be the random variable that counts the number of heads. Then \[
     X (HH)=2, X (HT)=X (TH)=1, X (TT)=0
     .\] 
\end{example*}
\begin{definition*}[Distribution function]
     The distribution function of $X$ is $F_X (x)=\mathbb{P} (X \leq x)$.
\end{definition*}
\begin{definition*}[Discrete/continuous random variable]
     A discrete random variable takes values in a countable set $S \subset \mathbb{R}$. Its probability mass function is \[
          p_X (x)=\mathbb{P}(X=x)
          .\] 
     
     A random variable $X$ has a continuous distribution if it has a probability density function $f_X (x)$ which satisfies \[
          \mathbb{P} (X \in A)=\int_A f_X (x) \mathrm{d}x
     ,\]
     for measurable sets $A$. 
\end{definition*}
\begin{definition*}[Expectation/variance]
     The expectation of $X$ is 
     \begin{equation*}
          \mathbb{E} (X)=
          \begin{cases}
              \sum_{x \in X}^{}x p_X (x) & X \text{ is discrete} \\
              \int_{-\infty }^{\infty} x f_X (x)\mathrm{d}x & X \text{ is continuous}
          \end{cases}
     \end{equation*}
     
     If $g: \mathbb{R} \to \mathbb{R} $, then for a continuous r.v \[
     \mathbb{E} (g (X))=\int_{-\infty }^{\infty} g (x) f_X (x)\mathrm{d}x
     .\] 
     The variance of $X$ is \[
          \operatorname{Var} (X)= \mathbb{E} [(X-\mathbb{E}(X))^2]
     .\] 
\end{definition*}
\begin{definition*}[Independence]
     We say $X_1 , \ldots ,X_n$ are independent if for all $x_1 , \ldots , x_n$ we have \[
     \mathbb{P} (X_1 \leq x_1 , \ldots ,X_n \leq x_n )=\mathbb{P}(X_1 \leq x_1) \ldots \mathbb{P}(X_n \leq x_n)
     .\] 
     If $X_1 , \ldots ,X_n $ have pdfs or pmfs $f_{X_1 }, \ldots,f_{X_n } $ then their joint pdf or pmf is \[
     f_X (x)=\prod_{i}f_{X_i}(x_i)
     .\] 
     If $Y=\max (X_1 , \ldots ,X_n)$ independent, then \[
     F_Y (y)=\mathbb{P} (Y \leq y)=\mathbb{P} (X_1 \leq y , \ldots ,X_n \leq y )=\prod_{i}F_{X_i}(y)
     .\] The pdf of $Y$ (if it exists) is obtained by differentiating $F_Y$.
\end{definition*}
\subsubsection*{Linear transformations}
Let $(a_1 , \ldots a_n)^T=a \in \mathbb{R}^{n}$ be a constant. \[
\mathbb{E} (a_1 X_1 +\ldots +a_n X_n)=\mathbb{E}(a^{T}X)=a^{T}\mathbb{E}(X)
.\]
This gives linearity of expectation (does not require independence). 
\[
\operatorname{Var}(a^{T}X)=\sum_{i,j}^{}a_{i}a_{j}\underbrace{\operatorname{Cov}(X_{i}, X_{j})}_{=\mathbb{E}((X_{i}-\mathbb{E}(X_{i})(X_{j}-\mathbb{E}(X_{j}))))} =a^{T}\operatorname{Var}(X)a
.\] 
where the matrix $[\operatorname{Var}(X)]_{ij}=\operatorname{Cov}(X_{i},X_{j})$. This gives the "bilinearity of variance".
\subsubsection*{Standardised statistics}
Let $X_1 , \ldots , X_n$ be iid. with $\mathbb{E}(X_1 )=\mu$ , $\operatorname{Var}(X_1)=\sigma^2$. We define $S_n=\sum_{i}^{}X_{i}$ and $\overline{X_n} \frac{S_n}{n} $ (the sample mean). By linearity \[
\mathbb{E} (\overline{X_n} )=\mu, \quad \operatorname{Var }(\overline{X_n} )= \frac{\sigma^2}{n}
.\]   
Define $Z_{n}= \frac{S_{n}-n \mu}{n}$. Then $\mathbb{E}(Z_{n})=0$ and $\operatorname{Var}(Z_{n})=1$. 
\subsubsection*{Moment generating functions}
\begin{definition*}[Moment generating function]
     The \vocab{moment generating function} (mgf) of a random variable $X$ is the function \[
          M_{x}(t)=\mathbb{E}(e^{tx})
     ,\] 
     provided that it exists for $t$ in some neighbourhood of 0.
\end{definition*} This is the Laplace transform of the pdf. It relates to moments of the pdf, for example $M_{x}^{(n)}(0)=\mathbb{E} (X^{n})$. 

Under broad conditions $M_{x}=M_{y} \iff F_{X}=F_{Y}$. (The Laplace transform is invertible.) The mgf is also useful for finding distributions of sums of independent random variables: 
\begin{example*}
     Let $X_1 , \ldots ,X_n \sim \operatorname{Po}(\mu)$. Then \[
     M_{X_{i}}(t)=\mathbb{E}(e^{tX_{i}})=\sum_{x=0}^{ \infty}e^{tx} \frac{e^{-\mu}\mu^{x}}{x!}=e^{-\mu}\sum_{x=0}^{ \infty}\frac{(e^{t}\mu^{x})}{x!}=e^{-\mu (1-e^{t})}
     .\] 
     What is $M_{S_{n}}$? We have \[
     M_{S_{n}}(t)=\mathbb{E}(e^{t (X_1 +\ldots +X_n)})=\prod_{i=1}^n e^{tX_{i}}=e^{-n \mu (1-e^{t})}
     .\] So we conclude $S_{n} \sim \operatorname{Po}(n \mu)$.
\end{example*}
\begin{example*}[mgf of the gamma distribution]
     If $X_{i} \sim \Gamma (\alpha_{i}, \lambda)$ for $i=1,\ldots ,n$ with $X_1 , \ldots X_n$ independent, then what is the distribution of $S_{n}=\sum_{i=1}^{n}X_{i}$? \[
          M_{S_{n}}(t)=\prod_{i}M_{X_{i}}(t)=
          \begin{cases}
              \left(\frac{\lambda}{\lambda t}\right)^{\sum_{i}^{}\alpha_{i}} & t<\lambda\\
              \infty & t>\lambda
          \end{cases}
          .\]  
          So $S_{n}$ is $\Gamma (\sum_{i}^{}a_{i}, \lambda)$. We call the first parameter the "shape parameter", and the second one the "rate parameter". A consequence of what we have just done is that if $X \sim \Gamma (\alpha, \lambda)$, then for all $b>0$ we have $bX \sim \Gamma (\alpha, \frac{\lambda}{b})$. 
          
          Special cases: 
          \begin{itemize}
              \item $\Gamma (1, \lambda)=\operatorname{Exp}(\lambda)$ 
              \item $\Gamma \left(\frac{k}{2},\frac{1}{2}\right)=\chi_k^2$ (the chi-squared distribution with $k$ degrees of freedom, i.e the distribution of a sum of $k$ independent squared $N (0,1)$ r.v's.)
          \end{itemize}
\end{example*}
\subsubsection*{Limits of random variables}
The weak law of large numbers states that $\forall \varepsilon >0$, as $n \rightarrow \infty$, \[
\mathbb{P} \left(|\overline{X_n} -\mu > \epsilon|\right) \rightarrow 0
.\] 
The strong law of large numbers states that as $n \rightarrow \infty$, \[
\mathbb{P}(\overline{X_{n}} \rightarrow \mu)=1
.\] The central limit theorem states that if we have the variable $Z_{n}= \frac{S_{n}-n \mu}{\sigma \sqrt{n}}$, then as $n \rightarrow \infty$ we have \[
\mathbb{P}(Z_{n} \leq z) \rightarrow \Phi (z) \quad \forall z \in \mathbb{R}
.\] where $\Phi$ is the distribution function of a $N (0,1)$ random variable.  
\subsubsection*{Conditional probability}
\begin{definition*}[Conditional probability]
     If $X,Y$ are discrete r.v's then \[
     P_{X|Y}(x|y)= \frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)}
     .\]   
     If $X,Y$ are continuous then the joint pdf of $X,Y$ satisfies: \[
     \mathbb{P}(X \leq x, P Y \leq y)=\int_{- \infty}^{x}\int_{- \infty}^{y}f_{X,Y} (x',y')\mathrm{d}y'  \mathrm{d}x' 
     .\] 
     The conditional pdf of $X$ given $Y$ is \[
     f_{x|y}= \frac{f_{X,Y}(x,y)}{\int_{- \infty}^{ \infty}f_{X,Y}(x,y) \mathrm{d}x }
     .\] 
     The conditional expectation of $X$ given $Y$ is 
     \begin{align*}
          \mathbb{E}(X|Y)=
          \begin{cases}
               \sum_{x}^{}xp_{X|Y}(x|Y) & \text{discrete}\\
               \int_{}^{}x f_{X|Y}(x|Y) \mathrm{d}x & \text{continuous}
          \end{cases}
     \end{align*}
     Note this is itself a random variable, as it is a function of $Y$. We define $\operatorname{Var}(X|Y)$ similarly. 
\end{definition*}
There are several notable properties of conditional random variables:
\begin{itemize}
     \item Tower property: $\mathbb{E}(\mathbb{E}(X|Y))=\mathbb{E}(X)$.
     \item Law of total variance: $\operatorname{Var}(X)=\mathbb{E}(\operatorname{Var}(X|Y))+\operatorname{Var}(\mathbb{E}(X|Y))$. 
     \item Change of variables (in 2D):

     Let $(x,y) \mapsto (u,v)$ be a differentiable bijection $\mathbb{R}^{2} \rightarrow \mathbb{R}^{2} $. Then \[
     f_{U,V}(u,v)=f_{X,Y}(x (u,v),y (u,v))|\det (J)|
     ,\] where $J=\frac{\partial (x,y)}{\partial (u,v)}$ is the Jacobian matrix we have seen before. 
\end{itemize}
\newpage
\section{Estimation}
Suppose $X_1 , \ldots X_n$ are iid observations with pdf or pdf (or pmf) $f_{X}(x| \theta)$ where $\theta$ is an unknown parameter in $\Theta$. Let $X=(X_1 , \ldots , X_{n})$. 
\begin{definition*}[Estimator]
     An estimator is a statistic or function of the data $T (X)=\hat{\theta}$ which does not depend on $\theta$, and is used to approximate the true parameter $\theta$. The distribution of $T (X)$ is called its "sampling distribution". 
\end{definition*}
\begin{example*}
     Let $X_1 , \ldots ,X_{n} \sim N (\mu,1)$ iid. Here $\hat{\mu}=\frac{1}{n}\sum_{i}^{}X_{i}=\overline{X_{n}}$. The sampling distribution of $\hat{\mu} $ is $T (X)=N (\mu, \frac{1}{n})$.   
\end{example*}
\begin{definition*}[Bias]
     The bias of $\hat{\theta}=T (X)$ is \[
     \operatorname{bias}(\hat{\theta})=\mathbb{E}_{\theta} (\hat{\theta})-\theta
     .\] Here $\mathbb{E}_{\theta}$ is the expectation in the model where $X_1 , X_2 , \ldots ,X_n \sim f_{X}(x|\theta)$.
\end{definition*}
\begin{remark}
     In general the bias is a function of true parameter $\theta$, even though it is not explicit in notation.  
\end{remark}
\begin{definition*}[Unbiased estimator]
     We say $\hat{\theta}$ is unbiased if $\operatorname{bias}(\hat{\theta})=0$ for all values of the true parameter $\theta$. 
\end{definition*}
In our example, $\hat{\mu}$ is unbiased because \[
\mathbb{E}_{\mu}(\hat{\mu})=\mathbb{E}_{\mu}(\overline{X_{n}})=\mu \quad \forall \mu \in \mathbb{R}
.\]  
\begin{definition*}[Mean squared error]
     The mean squared error (mse) of $\theta$ is \[
     \operatorname{mse}(\hat{\theta})=\mathbb{E}_{\theta} \left[ (\hat{\theta}-\theta)^2\right]
     .\] 
     It tells us "how far" $\hat{\theta}$ is from $\theta$ "on average".
\end{definition*}
\subsection{Bias-variance decomposition}
We expand the square in the definition of mse to get
\begin{align*}
     \operatorname{mse}(\hat{\theta})&=\mathbb{E}_{\theta} \left[ (\hat{\theta}-\theta)^2\right]\\
     &=\mathbb{E}_{\theta} \left((\hat{\theta}-\mathbb{E}_{\theta}\hat{\theta}-\theta)^{2}\right)
     &=\operatorname{Var}_{\theta}(\hat{\theta})+\operatorname{bias}^2 (\hat{\theta})\\
     & \geq 0
\end{align*}
There is a tradeoff between bias and variance. For example, let $X \sim \operatorname{Bin}(n,\theta)$. Suppose $n$ is known, and $\theta \in [0,1]$ is our unknown parameter. We define $T_{u}=\frac{X}{n}$, i.e the proportion of successes observed. Clearly $T_{u}$ is unbiased since \[
\mathbb{E}_{\theta} (T_{u})= \frac{E_{\theta}(X)}{n}=n \theta /n =\theta
.\] We can caculate \[
\operatorname{mse}(T_{u})=\operatorname{Var}_{\theta}(\frac{X}{n})= \frac{\operatorname{Var}_{\theta}}{n^2}= \frac{\theta (1-\theta)}{n}
.\] Consider another estimator $T_{B}= \frac{X+1}{n+2}=w \frac{X}{n}+ (1-w )\frac{1}{2}$ for $w=\frac{n}{n+2}$. This is called a "fixed estimator". In this case we have \[
\operatorname{bias}(T_{B})=\mathbb{E}_{\theta}(T_{B})-\theta=\mathbb{E}_{\theta}( \frac{X+1}{n+2})-\theta=\frac{n}{n+2}\theta +\frac{1}{n+2}-\theta
.\] This is $\neq 0$ for all but one value of $\theta$. Note that 
\begin{align*}
     \operatorname{Var}_{\theta}(T_{B})= \frac{\operatorname{Var}_{\theta}(X+1)}{(n+2)^2}\\
     \implies \operatorname{mse}(T_{B})=(1-w^2)\left(\frac{1}{2}-\theta\right)^2.
\end{align*}
\begin{remark}
     In this example, there are regions where either estimator is better. Prior judgement on the true value of $\theta$ determines which estimator is better. 
\end{remark}
Unbiasedness is not necessarily desirable. Let's look at a pathological example:
\begin{example*}
     Suppose $X \sim \operatorname{Po}(\lambda)$. We want to estimate $\theta= \mathbb{P} (X=0)^2=e^{-2\lambda}$. For some estimator $T (X)$ to be unbiased, we need \[
          \mathbb{E}_{\lambda}(T (x))=\sum_{x=0}^{ \infty}T (x) \frac{\lambda^{x}e^{-\lambda}}{x!}=e^{-2\lambda}=\theta \iff \sum_{x=0}^{ \infty}T (x) \frac{\lambda^{x}}{x!}=e^{-\lambda}=\sum_{x=0}^{ \infty}(-1)^{x} \frac{\lambda^{x}}{x!}  
          .\] The only function $T: N \rightarrow \mathbb{R}$ satisfying this equality is $T (x)=(-1)^{x}$. This is clearly an absurd estimator.
\end{example*}
\subsection{Sufficiency}
\begin{notation}
     From now on in the course we drop the $\theta$ subscript on expectations etc. in order to simplify notation.
\end{notation}
\begin{definition*}[Sufficiency]
     A statistic $T (X)$ is sufficient for $\theta$ if the conditional distribution of $X$ given $T (X)$ does not depend on $\theta$.
\end{definition*}
\begin{remark}
      $\theta$ can be a vector and $T (X)$ can also be vector-valued.
\end{remark}
\begin{example*}
      Let $X_1 , \ldots ,X_n$ be iid. Bernoulli$(\theta)$  variables for some $\theta$. Then \[
      f_{X}(X|\theta)=\prod_{i=1}^n \theta^{x_{i}}(1-\theta)^{1-x_{i}}=\theta^{\sum_{}^{}x_{i}} (1-\theta)^{n-\sum_{}^{}x_{i}}
      .\] This only depends on $x$ through $T (X)=\sum_{}^{}x_{i}$. To check it's sufficient: 
      \begin{align*}
           f_{X|T=t}(x|T=t)&= \frac{\mathbb{P}(X=x, T (x)=t)}{\mathbb{P} (T (x)=t)}\\
           \text{ If } \sum_{}^{}x_i =t, \quad &= \frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\mathbb{P}(T (x)=t)}\\
           &= \frac{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{{n \choose t}\theta^{t}(1-\theta)^{n-t}} \text{ since } \sum X_i \sim Bin (n,\theta)\\
           &= {n \choose t}^{-1}
      \end{align*}
      Therefore $T$ is sufficient.
\end{example*}
\begin{theorem}[Factorisation criterion]
     $T$ is sufficient for $\theta$ iff $f_{x}(x| \theta)=g (T (x), \theta)h (x)$ for suitable functions $g,h$.
\end{theorem}
\begin{proof}
      We will only prove this for the discrete case; the continuous case is similar. 
      
      \underline{Reverse implication:} Suppose $f_{x}(x| \theta)=g (T (x), \theta)h (x)$. Then if $T (x)=t$, we have 
      \begin{align*}
           f_{X|T=t} (x|T=t)&= \frac{\mathbb{P}(X=x,T (x)=t)}{\mathbb{P}(T (x)=t)}\\
           &= \frac{g t, \theta)h (x)}{\sum_{x': \ T (x')=t}^{}g (t, \theta)h (x')}\\
           &=\frac{h (x)}{\sum_{x': \ T (x')=t}^{}h (x')}.
      \end{align*}
      This doesn't depend on $\theta$ so $T$ is sufficient.

      \underline{Forward implication:} Suppose $T (X)$ is sufficient. Then we have 
      \begin{align*}
           f_X (x| \theta)&=\mathbb{P} (X=x, T (X)=T (x))\\
           &=\underbrace{\mathbb{P}(X=x| T (X)=T (x))}_{h (x)} \underbrace{\mathbb{P}(T(X)=T (x))}_{g (T (X),\theta)}.
      \end{align*}
      By noting that $\mathbb{P}(X=x| T (X)=x)$ only depends on $x$ by assumption and $\mathbb{P}(T(X)=T (x))$ only depends on $x$ through $T (x)$, we are done.
\end{proof}
\begin{remark}
      This criterion makes our previous example much easier.
\end{remark}
Let's look at another example.
\begin{example*}
      Let $X_1 , \ldots ,X_n \sim U ([0, \theta])$ be iid with $\theta >0$. Then \[
      f_{X}(x| \theta)=\prod_{i=1}^{n}\frac{1}{\theta} 1_{x_{i} \in [0, \theta]}=\frac{1}{\theta^{n}} 1_{\min_i x_{i} \geq 0} 1_{\max_i x_{i} \leq \theta} 
      .\] Define $T (x)=\max_i x_i$. Then we can write \[
      g (T (x), \theta)=\frac{1}{\theta^{n}}1_{\max_i x_{i} \leq \theta} , \quad h (x)=1_{\min_i x_{i} \geq 0}
      .\] So $T (x)$ is sufficient.
\end{example*}
\subsection{Minimal sufficiency}
Sufficient statistics are \textbf{not} unique. 
\begin{remark}
      Any bijection applied to a sufficient statistic yields another sufficient statistic.
\end{remark}
It's not hard to find sufficient statistics, for example $T (X)=X$ is a trivial sufficient statistic (that is useless!). Instead, we want statistics which give us `maximal' compression of the data in $X$. This motivates our next definition.
\begin{definition*}[Minimal sufficient statistic]
      $T (X)$ is \vocab{minimal sufficient} if for every other sufficient statistic $T'$, \[
      T' (x)=T' (y) \implies T \left(x\right)=T \left(y\right) \quad \forall x,y \in X^{n}
      .\] Note that it follows from this definition that minimal sufficient statistics are unique up to bijection.
\end{definition*}
\begin{theorem}
      Suppose that $f_{X}(x| \theta)/f_{X}(y| \theta)$ is constant in $\theta$ iff $T (x)= T (y)$. Then $T$ is minimal sufficient.
\end{theorem}
\begin{proof}
      Let $x \overset{1}{\sim}  y$ if $f_{X}(x| \theta)/f_{X}(y| \theta)$ is constant in $\theta$. It's easily checked that this defines an equivalence relation. Similarly, let $x \overset{2}{\sim} y$ if $T (x)=T (y)$; this is also an equivalence relation. The hypothesis in the theorem states that the equivalence classes of $\overset{1}{\sim} $ and $\overset{2}{\sim} $ are the same. 
      
      We will construct a statistic $T$ which is constant on the equivalence classes of $\overset{1}{\sim} $. For any value $t$ of $T$ let $z_{t}$ be a representative from $\left\{x; T (x)=t\right\}$. Then
      \begin{align*}
           f_{X}(x| \theta)&=f_{X}(z_{T (x)}|\theta)= \frac{f_{X}(x|\theta)}{f_{X}(z_{T (x)}| \theta)}\\
           &=g (T (x), \theta) h (x).
      \end{align*}
      Hence $T$ is sufficient by the factorisation criterion. To prove $T$ is minimal sufficient, let $S$ be any other sufficient statistic. By the factorisation criterion, there exist functions $g_{S}, h_{S}$ such that \[
      f_{X}(x| \theta)=g_{S}(S (x),\theta)h_{S}(x)
      .\] Now suppose $S (x)=S (y)$ for some $x,y$. Then \[
          \frac{f_{X}(x|\theta)}{f_{X}(y| \theta)}= \frac{g_{S}(S (x),\theta)h_{S}(x)}{g_{S}(S (y),\theta)h_{S}(y)}= \frac{h_{S}(x)}{h_{S}(y)}
      .\] which is constant in $\theta$, so $x \overset{1}{\sim} y$. By the hypothesis, $x \overset{2}{\sim} y$ and $T (x)=T (y)$. 
\end{proof}
\begin{example*}
      Suppose $X_1 , \ldots ,X_n \overset{\operatorname{iid}}{\sim} N (\mu, \sigma^2)$. Then 
      \begin{align*}
          \frac{f_{x}\left(\pi \mid \mu, \sigma^{2}\right)}{f_{x}\left(y \mid \mu, \sigma^{2}\right)}&=\frac{\left(2 \pi \sigma^{2}\right)^{-\pi / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i}\left(x_{i}-\mu\right)^{2}\right\}}{\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left\{-\frac{1}{2 \sigma^{2}} \sum_{i}\left(y_{i}-\mu\right)^{2}\right\}}\\
          &=\exp \left\{-\frac{1}{2 \sigma^{2}}\left(\sum_{i} x_{i}^{2}-\sum_{i} y_{i}^{2}\right)+\frac{\mu}{\sigma^{2}}\left(\sum x_{i}-\sum y_{i}\right)\right\}.
      \end{align*}
      This is constant in $\left(\mu, \sigma^{2}\right)$ iff $\sum x_{i}^{2}=\sum y_{i}^{2}$ and $\Sigma x_{i}=\sum y_{i}$. Hence $\left(\sum x_{i}^{2}, \sum x_{i}\right)$ is a minimal sufficient statistic.A more common minimal sufficient statistic is obtained by taking a bijection of $\left(\Sigma x_{i}^{2}, \Sigma x_{i}\right)$ :
     $$
     \begin{aligned}
     &S(x)=\left(\bar{x}_{n}, S_{x x}\right) \\
     &\bar{x}_{n}=\frac{1}{n} \sum x_{i} \quad S_{x x}=\sum_{i}\left(x_{i}-\bar{x}_{n}\right)^{2}
     \end{aligned}
     $$
In this example $\theta=\left(\mu, \sigma^{2}\right)$ has same dimension as $S(x)$. In general, they can be different.
\end{example*}
\subsection{Rao-Blackwell theorem}
We will now look at the Rao-Blackwell theorem. This theorem allows us to start from any estimator $\widetilde{\theta}$, and then by conditioning on a sufficient statistic we get a better one.
\begin{theorem}[Rao-Blackwell theorem]
      Let $T$ be a sufficient statistic for $\theta$ and define an estimator $\widetilde{\theta}$ with $\mathbb{E}(\widetilde{\theta}^2)< \infty$ for all $\theta$. Define a new estimator \[
      \hat{\theta}=\mathbb{E} (\widetilde{\theta} \left|T (x)\right.)
      .\] Then for all $\theta \in \Theta$, \[
      \mathbb{E}((\hat{\theta}-\theta)^2) \leq \mathbb{E} ((\widetilde{\theta}-\theta)^2)
      .\] Furthermore, the inequality is strict unless $\widetilde{\theta}$ is a function of $T (x)$.
\end{theorem}
\begin{proof}
      By tower property of $\mathbb{E}$, we have \[
      \mathbb{E}(\hat{\theta})=\mathbb{E}(\mathbb{E}(\widetilde{\theta}|T))=\mathbb{E}(\widetilde{\theta})
      .\] By the conditional variance formula, 
      \begin{align*}
          \operatorname{Var}(\widetilde{\theta})&=\mathbb{E}(\operatorname{Var}(\widetilde{\theta}|T))+\operatorname{Var}(\mathbb{E}(\widetilde{\theta}|T))\\
          &=\mathbb{E}(\operatorname{Var}(\widetilde{\theta}|T))+\operatorname{Var}(\hat{\theta})\\
          &\geq \operatorname{Var}(\hat{\theta}).
      \end{align*}
      So by the bias-variance decomposition, $\operatorname{mse} \widetilde{\theta}\geq \operatorname{mse}\hat{\theta}$. The inequality is strict unless $\operatorname{Var}(\widetilde{\theta}|T)=0$ with probability 1, which requires $\widetilde{\theta}$ is a function of $T$.
\end{proof}
\begin{remark}
      $T$ must be sufficient, since otherwise $\hat{\theta}$ would be a function of $\theta$, so it wouldn't be an estimator.
\end{remark}
We will now look at a few examples to show how powerful this theorem can be.
\begin{example*}
      $X_1 , \ldots ,X_n \overset{\operatorname{iid}}{\sim} \operatorname{Poi}(\lambda)$. Let $\theta=\mathbb{P}(X_1 =0)=e^{-\lambda}$. Then 
      \begin{align*}
           f_{X}(x|\lambda)&= \frac{e^{-n \lambda}\lambda^{\Sigma x_{i}}}{\prod_{i}^{}x_{i}!}\\
           \implies f_{X}(x|\theta)&= \frac{\theta^{n}(-\log \theta)^{\sum_{}^{}x_i}}{\prod_{i=1}^{}x_{i}!}=g (\sum_{}^{}x_i, \theta)h (x)
      \end{align*}
      So $\sum_{}^{}x_i=T (x)$ is sufficient by factorisation.

      Recall $\sum_{}^{}X_{i} \sim \operatorname{Poi}(n \lambda)$. Let $\widetilde{\theta}=1_{X_1 =0}$ (which only depends on $X_1$, so it is a bad estimator). However, it is unbiased, which is desirable as the Rao-Blackwell process will then also yield an unbiased estimator. So let's calculate 
      \begin{align*}
           \hat{\theta}&=\mathbb{E}(\widetilde{\theta}|T=t)=\mathbb{P}(X_1 =0|\sum_{i=1}^{n}X_{i}=t)\\
           &= \frac{\mathbb{P}(X_1 =0, \sum_{i=1}^{n}X_{i}=t)}{\mathbb{P}(\sum_{i=1}^{n}X_{i}=t)}
      \end{align*}
\end{example*}
\begin{example*}
      Let $X_1 , \ldots , X_n $ be iid. $U ([0, \theta])$; want to estimate $\theta$.

      We previously saw that $T=\max_{i}X_i$ is sufficient. Let $\widetilde{\theta} =2 X_1 $, an unbiased estimator of $\theta$. Then 
      \begin{align*}
          \hat{\theta}&=\mathbb{E} (\widetilde{\theta} | T=t)=2 \mathbb{E} (X_1 |\max_{i}X_i=t )\\
          &=2 \mathbb{E} (X_1 |\max_{i}X_i=t, X_1 =\max_{i}X_i) \mathbb{P}(\max_{i}X_i=X_1| \max_{i}X_i=t )\\& \quad +2\mathbb{E} (X_1 |\max_{i}X_i=t, X_1 \neq \max_{i}X_i) \mathbb{P}(\max_{i}X_i \neq X_1 | \max_{i}X_i=t )\\
          &=\frac{2t}{n}+ \frac{2 (n-1)}{n}\underbrace{\mathbb{E}(X_1 | X_1 <t, \max_{2 \leq i \leq n}X_i =t)}_{=t/2 \text{ as } X_1 | X_1 <t \sim U ([0,t])} \\
          &= \frac{n+1}{n }\max_i X_i.
      \end{align*}
      By Rao-Blackwell $\operatorname{mse}(\hat{\theta}) \leq \operatorname{mse}(\widetilde{\theta} )$. Also, $\hat{\theta} $ is unbiased.
\end{example*}
\subsection{Maximum likelihood estimation}
\begin{definition*}[Likelihood function/Maximum likelihood estimator]
     Let $X_1 , \ldots ,X_n $ be iid. with pdf (or pmf ) $f_{X}(\cdot | \theta)$. The \vocab{likelihood function} $L: \theta \rightarrow \mathbb{R}$ is given by \[
     L (\theta)=f_{X}(x| \theta)=\prod_{i=1}^n f_{X_i}(x_{i}| \theta)
     .\] (We take $X$ to be fixed observations.) We further define the \vocab{log-likelihood} \[
     l (\theta)=\log L (\theta)=\sum_{i=1}^{n}\log f_{X_i}(x_{i}| \theta)
     .\]A \vocab{maximum likelihood estimator} (mle) is an estimator that maximises $L$ over $\Theta$.
\end{definition*}
\begin{example*}
      Let $X_1 , \ldots X_n \sim^{iid} $Ber$(p)$. Then we have
      \begin{align*}
          l (p)&=\sum_{i=1}^{n }X_{i} \log p+ (1-X_i) \log (1-p)\\
          &=\log p (\sum_{}^{}X_{i})+\log (1-p)(n-\sum_{}^{}X_i)\\
          \implies \frac{\mathrm{d}l}{\mathrm{d}p}= \frac{\sum_{}^{}X_i}{p}+ \frac{n-\sum_{}^{}X_i}{1-p}
      \end{align*}
      This is $>0$ iff $p=\frac{1}{n}\sum_{}^{}X_i= \overline{X_i}$. We have $\mathbb{E} (\overline{X_i})=\frac{n}{n}\mathbb{E} (X_1 )=p$. So the mle $\hat{p} =\overline{X_i}$ is unbiased.
\end{example*}
Now let's try a more involved example.
\begin{example*}
      Let $X_1 , \ldots X_n \sim^{iid} N(\mu, \sigma^2)$. Then we have
      \begin{align*}
           l (\mu, \sigma^2)&=-\frac{n}{2}\log ( 2\pi)-\frac{n}{2 }\log (\sigma^2)-\frac{1}{2\sigma^2 }\sum_{i}^{}(X_i-\mu)^2.
      \end{align*}
      This is maximised when $\frac{\partial l}{\partial \mu}=\frac{\partial l}{\partial \sigma^2}=0$. But $\frac{\partial l}{\partial \mu}=\frac{1}{\sigma^2}\sum_{i=1}^{n}(X_i-\mu)$ so is equal to 0 iff \[
      \mu=\hat{X_n}=\frac{1}{n}\sum_{}^{}X_i
      .\] for all $\sigma^2 >0$.
      We also have that $\frac{\partial l}{\partial \sigma^2}=-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4} \sum_{i=1}^{n}(X_i-\mu)^2$.
      If we set $\mu=\overline{X_n}$, $\frac{\partial l}{\partial \sigma^2}=0$ iff \[
      \sigma^2= \frac{1}{n} \sum_{i=1}^{n}(X_i-\hat{X_n})^2= \frac{S_{xx}}{n}
      .\] Hence the mle is $(\hat{\mu},\hat{\sigma^2})=(\overline{X_n},\frac{S_{xx}}{n}) $. We can check $\hat{\mu}$ is unbiased. Later in the course we will see that \[
          \frac{S_{xx}}{\sigma^2}=\frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi_{n-1}^2
      .\] Therefore $\mathbb{E} (\sigma^2)= \frac{\sigma^2}{n}\mathbb{E}(\chi_{n-1}^2)= \frac{n-1}{n}\sigma^2 \neq \sigma^2.$ Hence $\hat{\sigma}^2$ is biased. But as $n \rightarrow \infty$ the bias converges to 0, so we say $\hat{\sigma}^2$ is \vocab{asymptotically unbiased}. 
\end{example*}
The next example will focus on an example where the mle is discontinuous, and doesn't behave as nicely.
\begin{example*}
     Let $X_1 , \ldots , X_n $ be iid. $U ([0, \theta])$. Recall the estimator we derived, $\hat{\theta}=\frac{n+1}{n }\max_i X_i$. The likelihood function is \[
     L (\theta)=\frac{1}{\theta^{n}} 1_{\max_i X_i \leq \theta}
     .\]
     \begin{figure}[H]
          \centering
          \incfig{70}{mle1}
          \caption{The plot of $L (\theta)$; note the discontinuity.}
     \end{figure}
     Hence the mle is $\hat{\theta}^{\operatorname{mle}}=\max_i X_{i}$. As $\hat{\theta}$ is unbiased, $\hat{\theta}^{\operatorname{mle}}$ is \textbf{not} unbiased.
\end{example*}
\subsubsection*{Properties of the mean likelihood estimator}
\begin{enumerate}
     \item If $T$ is sufficient for $\theta$, then the mle is a function of $T$. Recall \[
     L (\theta)=g (T, \theta)h (X)
     .\] So the maximiser of $L$ only depends on $X$ through $T$.
     \item If we parameterise $\theta$ in some way, say $\phi= H (\theta)$ where $H$ is a bijection, and $\hat{\theta}$ is the mle for $\theta$, then $H (\hat{\theta})$ is the mle for $\phi$.
     \item Asymptotic normality: Under regularity conditions, as $n \rightarrow \infty$ the statistic $\sqrt{n}(\hat{\theta}-\theta)$ is approx $N (0,\Sigma)$, i.e for some `nice' set $A$ we have \[
     \mathbb{P}(\sqrt{n}(\hat{\theta}-\theta) \in A) \xrightarrow{n \rightarrow \infty}\mathbb{P} (Z \in A), \quad \text{ where } Z \sim N (0, \Sigma)
     .\] The limiting covariance matrix $\Sigma$ is a known function of $L$. In some sense it is the `best' or `smallest' variance that any estimator can achieve asymptotically. See Part II Principles of Statistics for more on this.
     \item When the mle is not available analytically in closed form, in real-world applications it is often found numerically (see Part IB Numerical Analysis).
\end{enumerate}
\subsection{Confidence intervals}
The idea of confidence intervals is omnipresent; it is used in the real world to describe a measure of certainty, and you may well have used the term in conversation or seen it in media before. We will give a rigorous mathematical definition of confidence.
\begin{definition*}[Confidence interval]
      A $100 \cdot \gamma$\% \vocab{confidence interval} with $\gamma \in (0,1)$ and for a parameter $\theta$ is a random interval $(A (x),B (x))$ such that \[
      \mathbb{P}(A (x) \leq \theta \leq B (x)) = \gamma \quad \text{ for all } \theta \in \Theta
      .\] Note that we consider $\theta$ to be a fixed parameter, but the endpoints of the interval are randomly changing.
\end{definition*}
\begin{remark}
     When $\theta$ is a vector, we talk about confidence sets instead of confidence intervals.
\end{remark}
A \textbf{frequentist interpretation} is that if we repeat the experiment many times, on average $100 \cdot \gamma \%$ of the time the interval will contain $\theta$. 

A \textbf{misleading interpretation} is: ``having oberved $X=x$, there is now a probability $\gamma$ that $\theta \in [A (x),B (x)]$''. This is actually \textbf{incorrect}, and we will later see an example that shows this.
\begin{example*}
      Let $X_1 , \ldots, X_n \overset{\operatorname{iid}}{\sim} N (\theta,1)$. We want to find the 95\% confidence interval for $\theta$. We know $\overline{X} \sim N (\theta, \frac{1}{n})$ and $Z=\sqrt{n}(\overline{X}-\theta)\sim N (0,1)$ for all $\theta \in \mathbb{R}$. 

      Let $a,b$ be numbers s.t. $\Phi (b)-\Phi (a)=0.95$. Then \[
      \mathbb{P}(a \leq \sqrt{n}(\overline{X}-\theta)\leq b)=0.95
      .\] Rearrange: 
      \begin{align*}
           \mathbb{P}(\overline{X}-\frac{b}{\sqrt{n}}\leq \theta \leq \overline{X}-\frac{a}{\sqrt{n}})=0.95
      \end{align*}
      Hence $(\overline{X}-\frac{b}{\sqrt{n}},\overline{X}-\frac{a}{\sqrt{n}})$ is a 95\% C.I for $\theta$. 
      
      Note $a,b$ are not unique. Typically we centre the interval around some estimator $\hat{\theta}$ and aim to minimise its length. In this case, we would choose $a=-b$, which would give $b=Z_{0.025}\approx 1.96$ where $Z_{\alpha}$ is equal to ${\Phi}^{-1}(1-\alpha)$. We call this the ``upper $\alpha$-point'' of the $N (0,1)$ distribution.

      Therefore our final C.I is $(\overline{X}\pm \frac{1.96}{\sqrt{n}})$. A quick sanity check is to note that our interval decreases as $n$ gets larger (with more observations).
\end{example*}
We can generalise the method we used in this example.
\begin{remark}
      \underline{Recipe for finding a confidence interval}:
      \begin{enumerate}
           \item Find a quantity $R (X, \theta)$ whose $\mathbb{P}_{\theta}$-distribution \emph{doesn't} depend on $\theta$. This is called a \vocab{pivot}, for example in the above example our pivot was $R (X , \theta)=\sqrt{n}(\overline{X}-\theta)$.
           \item Write down a statement \[
           \mathbb{P} (c_1 \leq R (X,\theta) \leq c_2 )=\gamma
           .\] Given some $\gamma$, we find $c_1 ,c_2 $ using the distribution of $R$.
           \item Rearrange to leave $\theta$ in the middle of two inequalities.
      \end{enumerate}
\end{remark}
\begin{proposition}
      If $T$ is a monotone increasing function and $(A (X),B (X))$ is a $100 \cdot \gamma \%$ C.I for $\theta$, then $T (A (X),T (B (X)))$ is a $100 \cdot \gamma \%$ C.I for $T (\theta)$.
\end{proposition}
\begin{proof}
      Immediate from definitions. (Exercise)
\end{proof}
\begin{example*}
      Let $X_1 , \ldots, X_n \overset{\operatorname{iid}}{\sim} N (0,\sigma^{2} )$. We want to find a 95\% C.I for $\sigma^2$. Let's follow our recipe:
      \begin{enumerate}
           \item Note that $ \frac{X_1 }{\sigma}\sim N (0,1)$. This is a pivot, but ideally we would want one that depends on all the observations. So let our pivot be \[
               \sum_{i=1}^{n} \frac{X_{i}}{\sigma^2} \sim \chi_{n}^2
           .\] 
           \item Let $c_1 = {F_{\chi_{n}^2}}^{-1} (0.025) $ and $c_2 ={F_{\chi_{n}^2}}^{-1}(0.975)$.
           \item Now rearrange to get $\sigma^{2}$ in the middle: 
           \begin{align*}
                \mathbb{P}( \frac{\sum_{}^{}X_i^2}{c_2 }\leq \sigma^2 \leq \frac{\sum_{}^{}X_i^2}{c_1 })=0.95
           \end{align*}
           Hence this is our 95\% confidence interval for $\sigma^{2}$.
      \end{enumerate}
\end{example*}
\begin{example*}
      Let $X_1 , \ldots, X_n \overset{\operatorname{iid}}{\sim} \operatorname{Ber} (p)$ with $n$ ``large''. We want to find an approximate 95\% C.I for $p$.
     \begin{enumerate}
          \item The mle of $p$ is $\hat{p}=\overline{X}=\frac{1}{n}\sum_{}^{}X_{i}$. By the central limit theorem, $\hat{p}$ is approx $N (p,p (1-p)/n)$. Therefore \[
          \sqrt{n} \frac{(\hat{p}-p)}{\sqrt{p (1-p)}} \quad \text{ is approx } N (0,1)
          .\] 
          \item $\mathbb{P} (-Z_{0.025} \leq \sqrt{n}\frac{(\hat{p}-p)}{\sqrt{p (1-p)}} \leq Z_{0.025})\approx 0.95$.
          \item Note that if we wanted to rearrange for $p$ here, we would have to solve a quadratic inequality. So instead of this, we'll approximate $\sqrt{p (1-p)} \approx \sqrt{\hat{p} (1-\hat{p})}$. We argue when $n$ is large \[
               \mathbb{P} (-Z_{0.025} \leq \sqrt{n}\frac{(\hat{p}-p)}{\sqrt{\hat{p} (1-\hat{p})}} \leq Z_{0.025})\approx 0.95
          .\] This is easier to rearrange, which gives \[
          \mathbb{P} (\hat{p}-Z_{0.025} \sqrt{ \frac{\hat{p} (1-\hat{p})}{n}}\leq p \leq \hat{p}+Z_{0.025} \sqrt{ \frac{\hat{p} (1-\hat{p})}{n}})\approx 0.95
          .\] So we have found an approximate 95\% confidence interval for $p$.
     \end{enumerate}
\end{example*}
\begin{remark}
      In the above example, $p (1-p)\leq \frac{1}{4}$ on $p \in (0,1)$ hence $(\hat{p} \pm \frac{Z_{0.025}}{2 \sqrt{n}})$ is a ``conservative'' 95\% C.I for $p$.
\end{remark}
Let's go back to the issue of how to interpret a confidence interval, and the two interpretations that were mentioned. This can be seen in the following example:
\begin{example*}
      Suppose $X_1 ,X_2 $ are iid. $\operatorname{Unif}(\theta-\frac{1}{2}, \theta+\frac{1}{2})$. What is a sensible 50\% confidence interval for $\theta$? Note 
      \begin{align*}
           \mathbb{P} (\theta \text{ between }X_1 ,X_2  )&= \mathbb{P} (\min (X_1 ,X_2 ) \leq \theta \leq \max (X_1 , X_2 ))\\
           &=\mathbb{P} (X_1 \leq \theta \leq X_2 )+\mathbb{P} (X_2 \leq \theta \leq X_1 )\\
           &=\frac{1}{2} \cdot \frac{1}{2} + \frac{1}{2} \cdot \frac{1}{2} =\frac{1}{2}.
      \end{align*}
      Hence the frequentist interpretation is \emph{exactly} correct.

      But suppose $\left|X_1 -X_2 \right|>0.5$. Then we \emph{know} that $\theta$ is in $(\min (X_1 ,X_2 ) , \max (X_1 , X_2 ))$ 
\end{example*}
\subsection{Bayesian analysis}
So far we've talked about \emph{frequentist} inference; we think of $\theta $ as being fixed. Inferential statements are interpreted in terms of repetitions of the experiment. 

\emph{Bayesian} analysis is a different framework. In this view, we treat $\theta $ as a random variable taking values in $\Theta $. The \vocab{prior distribution} $\pi \left(\theta \right)$ represents the investigator's beliefs or information about $\theta $ before observing the data. Conditional on $\theta $, the data $X$ has pdf (or pmf) $f_{X}\left( \cdot |\theta \right)$. (This is why we've been writing $f_{X}$ in this way when we use the frequentest interpretation.) Having observed $X$, the information in $X$ is combined with the prior to form the \vocab{posterior distribution} $\pi \left(\theta |X\right)$, which is the conditional distribution of $\theta $ given $X$. By Bayes' Rule: \[
\pi  \left(\theta |X\right)= \frac{\pi  \left(\theta \right)f_{X}\left(X|\theta \right)}{f_{X}\left(X\right)}
.\] where $f_{X}\left(x\right)$ is the marginal distribution of $X$: \[
f_{X}\left(X\right)=\begin{cases}
     \int_{\Theta  }^{}f_{X}\left(X|\theta \right)\pi \left(\theta \right)  \ \mathrm{d}\theta   &\text{ if } \theta \text{ continuous}  ;\\
     \sum_{\theta \in \Theta }^{}f_{X}\left(X|\theta \right)\pi  \left(\theta \right) &\text{ if } \theta \text{ discrete} .
\end{cases}
\] More simply, \[
\pi  \left(\theta |X\right) \propto \pi \left(\theta \right) \cdot f_{X}\left(X|\theta \right)
.\] Often, it is easy to recognize that the RHS is in some family of distributions up to a normalising constant.
\begin{remark}
     By the factorisation criterion, if $T$ is sufficient then \[
          \pi  \left(\theta |X\right) \propto \pi \left(\theta \right) \cdot g \left(T \left(X\right),\theta \right)
     .\] Therefore the posterior only depends on $X$ through $T \left(X\right)$, since we can absorb the $h \left(x\right)$ in the decomposition of $T$ into the constant.
\end{remark}
\begin{example*}
     Suppose a patient walks into a COVID-19 testing clinic (we have no prior info about the patient). Our random variable is $\theta = 1_{\text{patient infected}}$. We know the sensitivity of the test $f_{X}\left(X=1|\theta =1\right)$ and the specificity $f_{X}\left(X=0|\theta =0\right)$. 
     
     How do we choose the prior? Let $\pi  \left(\theta =1\right)$ to be the proportion of people in the UK with COVID on that day. What is the probability of an infection given a positive test? \[
     \pi \left(\theta =1 |X=1\right)= \frac{\pi  \left(\theta =1\right)f_{X}\left(X=1|\theta =1\right)}{\pi \left(\theta =1\right)f_{X}\left(X=1|\theta =1\right)+\pi \left(\theta =0\right)f_{X}\left(X=1|\theta =0\right)}
     .\] Sometimes $\pi \left(\theta =1\right)\ll \pi \left(\theta =0\right)$ which can make $\pi \left(\theta =1| X=1\right)$ small, which can be surprising. 
\end{example*}
In the second example, choosing the prior will be less clear-cut.
\begin{example*}
      Let $\theta \in [0,1]$ be the mortality rate for a new surgery in Addenbrooke's hospital. We observe the first 10 operations and see no deaths. We model $X \sim \operatorname{Bin}\left(10,\theta \right)$.

      How do we choose the prior? Suppose that in other hospitals mortality ranges between 3\% and 20\% with an average of 10\%. For example, take $\pi \left(\theta \right)\sim \beta \left(a,b\right)$. We can choose $a=3$ and $b=27$ so that $\pi \left(\theta \right)$ has mean 0.1 and $\pi \left(0.03<\theta <0.2\right) \approx 0.9$. The posterior is 
      \begin{align*}
          \pi \left(\theta |X\right)&\propto \pi \left(\theta \right) \cdot f_{X}\left(X=10|\theta \right)\\
          &\propto \theta ^{a-1} \left(1-\theta \right)^{b-1} \theta ^{x}\left(1-\theta \right)^{n-x}\\
          &\propto \theta ^{x+a-1}\left(1-\theta \right)^{b+n-x-1}\quad \text{ for } \theta \in [0,1].
      \end{align*}
      We recognise this as a $\beta \left(X+a, n+X-b\right)$ distribution. In our example this is $\beta \left(3,10+27\right)$. Plotting our prior and posterior, we can see that the posterior has a distribution shifted slightly to the left (since we observed no deaths in 10 trials), and that its variance is smaller (since we have already seen some trials).
      [picture]
      \begin{remark}
            In this example the prior and posterior are in the same family of distributions. This is called \vocab{conjugacy}.
      \end{remark}
\end{example*}
\subsubsection*{What do we do with the posterior?}
$\pi \left(\theta |X\right)$ represents information about $\theta $ \emph{after} seeing $X$. This can be used to make decisions under uncertainty.
\begin{enumerate}
     \item We must pick some decision $\delta \in D$. For example, $D=\left\{\text{ ask patient to isolate (or not) } \right\}$.
     \item Define the \vocab{loss function} $L \left(\theta ,\delta \right)$. For example, $L \left(\theta =1, \delta =1\right)$ would be the loss incurred by asking a patient to isolate if they test positive. 
     \item Pick $\delta $ that minimises \[
     \int_{\Theta }^{}L \left(\theta ,\delta \right) \pi \left(\theta |X\right)\ \mathrm{d}\theta 
     .\] This is called the ``\vocab{posterior expectation of loss}''.\footnote{See Von-Neumann/Morgenstein.}
\end{enumerate}
\subsubsection*{Point estimation}
An example of a decision is a ``best guess'' for $\theta $. The \vocab{Bayes estimator} minimises \[
h \left(\delta \right)=\int_{\Theta }^{} L \left(\theta ,\delta \right) \pi \left(\theta |X\right) \ \mathrm{d}\theta  
.\]
\begin{example*}
      If we choose a quadratic loss $L \left(\theta ,\delta \right)=\left(\theta -\delta \right)^{2}$. Then $h \left(\delta \right)=\int_{\Theta }^{} L \left(\theta -\delta \right)^{2} \pi \left(\theta |X\right) \ \mathrm{d}\theta  $. Differentiate: $h' \left(\delta \right)=0$ if 
      \begin{align*}
          \int_{\Theta }^{}\left(\theta -\delta \right)\pi \left(\theta |X\right) \ \mathrm{d}\theta =0 \\
          \iff \delta =\int_{\Theta }^{}\theta \pi \left(\theta |X\right) \ \mathrm{d}\theta . 
      \end{align*}
      This is $\hat{\theta}$ under quadratic loss.
\end{example*}
\begin{example*}
      If we use the absolute error loss $L \left(\theta ,\delta \right)=\left|\theta -\delta \right|$, then
      \begin{align*}
          h \left(\delta \right)&=\int_{\Theta }^{} \left|\theta -\delta \right| \pi \left(\theta |X\right) \ \mathrm{d}\theta\\
          &= \int_{-\infty}^{\delta }- \left(\theta -\delta \right) \pi  \left(\theta |X\right) \ \mathrm{d}\theta + \int_{\delta }^{\infty } \left(\theta -\delta \right) \pi  \left(\theta |X\right) \ \mathrm{d}\theta.
      \end{align*}
      Take the derivative wrt. $\theta $ using the FTC: \[
      h' \left(\theta \right)=\int_{-\infty}^{\delta }\pi  \left(\theta |X\right) \ \mathrm{d}\theta - \int_{\delta }^{\infty}\pi \left(\theta |X\right) \ \mathrm{d}\theta   
      .\] So $h^\prime \left(\delta \right)=0$ iff $\delta =\hat{\theta}$ is the median of the posterior $\pi \left(\theta |X\right)$.  
\end{example*}
We would also want some kind of Bayesian interpretation of a confidence interval. The next definition makes this more concrete.
\begin{definition*}[Credible interval]
      A $100 \cdot \gamma \%$ credible interval satisfies \[
      \pi (A (x)\leq \theta \leq B (x)|x)=\gamma 
      .\]
\end{definition*}
\begin{remark}
      Unlike confidence intervals, credible intervals \textbf{can} be interpreted conditionally; for example, ``given a specific observation $x$, we are 95\% certain that $\theta $ is in $(A,B)$''. The caveat here is that the credible interval depends on the choice of prior.
\end{remark}
We'll quickly look at some examples of Bayesian computations in order to get a better feel for them.
[todo]
\begin{remark}
      Asymptotically, we will often see credible intervals approach confidence intervals (as in the previous example).
\end{remark}
[next example]
This is where our discussion of Bayesian analysis ends - we now return to a frequentist viewpoint.
\newpage
\section{Simple hypotheses}
\begin{definition*}[Hypothesis]
      A \vocab{hypothesis} is an assumption about the distribution of data $X$.
\end{definition*}
Scientific questions are often phrased as a decision between a \vocab{null hypothesis} $H_0 $ and an \vocab{alternative hypothesis} $H_1 $. 
\begin{example*}
      \begin{enumerate}
           \item $X = (X_1 ,\ldots,X_n)$ are iid. $\operatorname{Ber}(\theta )$. Suppose $H_0: \ \theta =1/2, \quad H_1 : \ \theta =3/4$.
           \item In 1. we could also have $X_1 :\ \theta \neq 1/2$.
           \item Let $X = (X_1 ,\ldots,X_n)$ be iid. where $X_i$ takes values in $\mathbb{N}$. Suppose \[
           H_0 : \ X_{i} \overset{\operatorname{iid}}{\sim} \operatorname{Poi}(\lambda ) \text{ for some } \lambda >0, \quad H_1 : \ X_{i} \overset{\operatorname{iid}}{\sim} f_{1} \text{ for some other dist. } f_{1}
           .\] This is known as a \emph{goodness-of-fit test}.
      \end{enumerate}
\end{example*}
\begin{definition*}[Simple hypothesis]
      A \vocab{simple hypothesis} is one which fully specifies the pdf (resp. pmf) of $X$. Otherwise, we say the hypothesis is \emph{composite}.
\end{definition*}
In the last example, the test in 1. was composite, and the test in 2. was simple.
\begin{definition*}[Test]
      A \vocab{test} of the null $H_0 $ is defined by a \vocab{critical region} $C \subseteq \mathcal{X}$. When $X \in C$, we \emph{reject the null}. When $X \notin C$, we \emph{fail to reject the null}.
      \begin{remark}
            When $X \notin C$, we simply don't find evidence to reject the null; it doesn't mean the null is false. We will see examples of this later.
      \end{remark}
\end{definition*}
\begin{definition*}[Error]
     There are two types of error:
     \begin{itemize}
          \item \vocab{Type I error}: rejecting $H_0 $ when $H_0 $ is true.
          \item \vocab{Type II error}: fail to reject $H_0 $ when $H_0 $ isn't true.
     \end{itemize}
\end{definition*}
Write \[
     \alpha =\mathbb{P}_{H_{0}}(H_0 \text{ is rejected} )=\mathbb{P}_{H_{0}} (X \in C), \quad \beta  =\mathbb{P}_{H_{1}}(H_0 \text{ is not rejected} )=\mathbb{P}_{H_{1}} (X \in C)
.\] The \vocab{size} of the test is $\alpha $, the \emph{power} is $1-\beta $. There is a tradeoff between $\alpha $ and $\beta $. What we typically do is choose an acceptable probability of type I errors (say 1\%); set $\alpha $ to that, and then pick the test which minimises $\beta $ (maximises power).
\begin{definition*}[Likelihood ratio statistic/test]
      Let $H_0 $ and $H_1 $ be simple, with $X$ having pdf (or pmf) $f_{i}$ under $H_{i}$ for $i \in \left\{0,1\right\}$. The \vocab{likelihood ratio statistic} is \[
      \Lambda_{x} (H_0 ,H_1 )= \frac{f_1 (x)}{f_0 (x)}
      .\] A likelihood ratio test (LRT) rejects when $\Lambda_{x} (H_0 ,H_1 )$ is large, i.e \[
      C= \left\{x: \Lambda_{x} (H_0 ,H_1 ) >k\right\} \text{ for some } k
      .\] 
\end{definition*}
\begin{theorem}[Neyman-Pearson lemma]
      Suppose that $f_0 ,f_1 $ are nonzero on some sets. Suppose there is $k >0$ such that the LRT with critical region $C= \left\{x: \Lambda_{x} (H_0 ,H_1 ) >k\right\}$ has size $\alpha $. Then out of all tests with size $\leq  \alpha $, this test has the smallest $\beta $. 
      \begin{remark}
            An LRT of size $\alpha $ does not always exist. (Exercise.) But in general, we can find a ``randomised'' test of size $\alpha $.
      \end{remark}
\end{theorem}
\begin{proof}
      Let $\overline{C}$ be the complement of $C$. We know that the LRT has \[
      \alpha =\mathbb{P}_{H_0 } (X \in C) =\int_{C}^{}f_{0} (x) \ \mathrm{d} x, \quad \beta  =\mathbb{P}_{H_1 } (X \notin C) =\int_{\overline{C}}^{}f_{1} (x) \ \mathrm{d} x
      .\] Let $C^{*}$ be some other critical region with type I/II error probabilities $\alpha ^* ,\beta ^* $. Suppose $\alpha ^* \leq \alpha , \beta  \leq \beta ^*$. We want to show $\beta  \leq \beta ^*$. Note that
      \begin{align*}
          \beta -\beta ^* &= \int_{\overline{C}}^{}f_{i}(x) \ \mathrm{d}x - \int_{\overline{C^*}}^{}f_{i}(x) \ \mathrm{d}x \\
          &=\int_{\overline{C}\cap C^*}^{}f_{i}(x) \ \mathrm{d}x - \int_{\overline{C^*}\cap C}^{}f_{i}(x) \ \mathrm{d}x \text{ as the integrals over } \overline{C}\cap \overline{C^*} \text{ cancel }
      \end{align*}
\end{proof}
Let's illustrate this with an example.
\begin{example*}
      Let $X_1 , \ldots, X_{n} \overset{\operatorname{iid}}{\sim}N (\mu ,\sigma_i ^2) $ where $\sigma_0 $ is known. We want the best size $\alpha $ test for $H_0 $: $\mu =\mu_0 $ vs $H_1 $: $\mu =\mu_1 $ for some fixed $\mu_1 > \mu_0 $. Skipping the algebra, we get \[
      \Lambda_{X}(H_0 ; H_1 )= \exp \left\{ \frac{\mu_1 -\mu_0 }{\sigma_0^2}n \overline{X} + \frac{n (\mu_0^2 -\mu_1^2 )}{2\sigma_0 ^2}\right\}
      .\] $\Lambda_{x}$ is monotone increasing in $\overline{X}$; it is also monotone increasing in $Z= \sqrt{n} (\overline{X}-\mu_0 )/\sigma_{0}$. Thus $\Lambda_{x}>k \iff z>k'$ for some $k'>0$. Hence the LRT has critical region of the form \[
      C=\left\{x: Z (x)>k'\right\}
      .\]  To find the most powerful test, by the Neyman-Pearson lemma, we need only find $k$ such that $C$ has size $\alpha$. Under $H_0 : \ \mu=\mu_0 $, $z \sim N (0,1)$. Thus if we choose $k' =\Phi^{-1}(1-\alpha)$ we have \[
      \mathbb{P}_{H_0 }(z>k' )=\alpha
      ,\] i.e the test $C=\left\{x: Z (x)>k'\right\}$ has size $\alpha$. This is called a \vocab{z-test}.
\end{example*}
\subsubsection*{The $p$-value}
If we have a critical region $\left\{x: T (x)>k\right\}$ for some \vocab{test statistic} $T (X)$, we usually report a \vocab{$p$-value} in addition to the test's conclusion, which is defined by \[
p=\mathbb{P}_{H_0 }(T (X)>T (x^*))
,\] where $x^*$ is the observed data. 

In the example above, suppose $\mu_0 =5, \mu_1 =6, \alpha=0.05 $. Suppose we are given the data $x^* =(5.1,5.5, 4.9,5.3)$. We have $\overline{x^*}-5.2, z^*=0.4$. The LRT is \[
     \left\{x: Z (x)> \Phi^{-1} (0.95)=1.645\right\}
.\] . The conclusion of the LRT is that we do not reject $H_0 $. [drawing todo] Here $p=1-\Phi (z^*)=0.35$. 
\begin{proposition}
      Under $H_0$, the $p$-value is $\sim U[0,1]$.
\end{proposition}
\begin{proof}
     Let $F$ be the distribution of $T$. Assume that $F$ is continuous. Then 
     \begin{align*}
          \mathbb{P}_{H_0 }(p< u)&=\mathbb{P}_{H_0 } (1-F (T)<u)\\
          &=\mathbb{P}_{H_0 } (F (T)>1-u)\\
          &= \mathbb{P}_{H_0 } (T > F^{-1}(1-u))\\
          &= 1-F(F^{-1}(1-u))=u.
     \end{align*}
\end{proof}
\subsection{Composite hypotheses}
Let $X \sim f_{X}(\cdot |\theta)$, where $\theta \in \Theta$. We define a \vocab{composite hypothesis} 
\begin{align*}
     &H_0: \theta\in \Theta_{0} \subset \Theta\\
     &H_1: \theta\in \Theta_{1} \subset \Theta\\
\end{align*}
Now the probabilities of Type I or II error may depend on the value of $\theta$ within $\Theta_0 $ or $\Theta_1 $. They are not constants.
\begin{definition*}[Power function/size]
      The \vocab{power function} for a test $C$ is \[
      W (\theta)=\mathbb{P}_{\theta}(X \in C)
      .\] The \vocab{size} of a test $C$ is \[
      \alpha= \sup_{\theta \in \Theta_0 } W (\theta)
      .\]
\end{definition*}
We say that a test is \vocab{uniformly most powerful} (UMP) if for any other test $C^* $ with power function $W^*$ and size $\alpha$, \[
W (\theta) \geq W^* (\theta) \text{ for all } \theta \in \Theta_{1}
.\]\begin{remark}
      UMP tests need not exist. However, in simple models many LRTs are UMP.
\end{remark}
\begin{example*}
      One-sided test for normal location
\end{example*}
\end{document}