\documentclass[a4paper]{scrartcl}

\usepackage[
    fancytheorems, 
    fancyproofs, 
    noindent, 
]{adam}
\usepackage{floatrow}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{bm}

\newcommand{\incfig}[2]{%
    \def\svgwidth{#1mm}
    \import{./figures/}{#2.pdf_tex}
}

\title{IB Statistics}
\author{Martin von Hodenberg (\texttt{mjv43@cam.ac.uk})}
\date{\today}
\setcounter{section}{-1}

\allowdisplaybreaks

\begin{document}

\maketitle

These are my notes for the IB course Statistics, which was lectured in Lent 2022 at Cambridge by Dr S.Bacallado. These notes are written in \LaTeX  \ for my own revision purposes. Any suggestions or feedback is welcome.



\tableofcontents
\newpage

\section{Introduction}
Statistics can be defined as the science of \emph{making informed decisions}. It can include:
\begin{enumerate}
    \item Formal statistical inference
    \item Design of experiments and studies
    \item Visualisation of data
    \item Communication of uncertainty and risk
    \item Formal decision theory
\end{enumerate}
In this course we will only focus on formal statistical inference.
\begin{definition*}[Parametric inference]
     Let $X_1 , \ldots , X_n$ be iid. random variables. We will assume the distribution of $X_1 $ belongs to some family with parameter $\theta \in \Theta$.
\end{definition*}
\begin{example*}
    We will give some examples of such families:
     \begin{enumerate}
         \item $X_1 \sim \operatorname{Po}(\mu), \theta=\mu \in \Theta=(0,\infty )$ .
         \item $X_1 \sim N (\mu, \sigma^2) \quad N (\mu, \sigma^2)\in \Theta=\mathbb{R} \times (0, \infty)$.
     \end{enumerate}
\end{example*}
We will use the observed $X= (X_1 , \ldots X_n)$ to make inferences about $\theta$ such as:
\begin{enumerate}
    \item Point estimate $\theta (X)$ of $\theta$.
    \item Interval estimate of $\theta$: $(\theta_1 (x),\theta_2 (x))$ 
    \item Testing hypotheses about $\theta$: for example checking if there is evidence in $X$ against the hypothesis $H_0 : \ \theta=1$.
\end{enumerate}
\begin{remark}
     In general, we'll assume the distribution of the family $X_1 , \ldots , X_n$ is known but the parameter is unknown. Some results (on mean square error, bias, Gauss-Markov theorem) will make weaker assumptions.
\end{remark}

\section{Probability}
First we will briefly recap IA Probability.

Let $\Omega$ be the \vocab{sample space} of outcomes in an experiment. A measurable subset of $\Omega$ is called an \vocab{event}. The set of events is denoted $\mathcal{F}$. 
\begin{definition*}[Probability measure]
     A probability measure $\mathbb{P}: \mathcal{F} \rightarrow [0,1]$ satisfies:
     \begin{enumerate}
         \item $\mathbb{P} (\emptyset)=0$ 
         \item $\mathbb{P}(\Omega)=1$
         \item $\mathbb{P}\left(\bigcup_{i=1}^{\infty} A_i= \sum_{i}^{}\mathbb{P} (A_i)\right)$ if $(A_i)$ is a sequence of disjoint events.   
     \end{enumerate} 
\end{definition*}
\begin{definition*}[Random variable]
     A random variable is a (measurable) function $X: \Omega \rightarrow \mathbb{R}$.
\end{definition*}
\begin{example*}
     Tossing two coins has $\Omega= \left\{HH,HT,TH,TT\right\}$. Since $\Omega$ is countable, $\mathcal{F}$ is the power set of $\Omega$. We can define $X$ to be the random variable that counts the number of heads. Then \[
     X (HH)=2, X (HT)=X (TH)=1, X (TT)=0
     .\] 
\end{example*}
\begin{definition*}[Distribution function]
     The distribution function of $X$ is $F_X (x)=\mathbb{P} (X \leq x)$.
\end{definition*}
A discrete random variable takes values in a countable set $S \subset \mathbb{R}$. Its probability mass function is \[
p_X (x)=\mathbb{P}(X=x)
.\] 
A random variable $X$ has a continuous distribution if it has a probability density function $f_X (x)$ which satisfies \[
\mathbb{P} (X \in A)=\int_A f_X (x) \mathrm{d}x
,\]
for measurable sets $A$. 

The expectation of $X$ is 
\begin{equation*}
     \mathbb{E} (X)=
     \begin{cases}
         \sum_{x \in X}^{}x p_X (x) & X \text{ is discrete} \\
         \int_{-\infty }^{\infty} x f_X (x)\mathrm{d}x & X \text{ is continuous}
     \end{cases}
\end{equation*}

If $g: \mathbb{R} \to \mathbb{R} $, then for a continuous r.v \[
\mathbb{E} (g (X))=\int_{-\infty }^{\infty} g (x) f_X (x)\mathrm{d}x
.\] 

The variance of $X$ is \[
\operatorname{Var} (X)= \mathbb{E} [(X-\mathbb{E}(X))^2]
.\] 
We say $X_1 , \ldots ,X_n$ are independent if for all $x_1 , \ldots , x_n$ we have \[
\mathbb{P} (X_1 \leq x_1 , \ldots ,X_n \leq x_n )=\mathbb{P}(X_1 \leq x_1) \ldots \mathbb{P}(X_n \leq x_n)
.\] 
If $X_1 , \ldots ,X_n $ have pdfs or pmfs $f_{X_1 }, \ldots,f_{X_n } $ then their joint pdf or pmf is \[
f_X (x)=\prod_{i}f_{X_i}(x_i)
.\] 

If $Y=\max (X_1 , \ldots ,X_n)$ independent, then \[
F_Y (y)=\mathbb{P} (Y \leq y)=\mathbb{P} (X_1 \leq y , \ldots ,X_n \leq y )=\prod_{i}F_{X_i}(y)
.\] 

The pdf of $Y$ (if it exists) is obtained by differentiating $F_Y$.
\subsection{Linear transformations}
Let $(a_1 , \ldots a_n)^T=a \in \mathbb{R}^{n}$ be a constant. \[
\mathbb{E} (a_1 X_1 +\ldots +a_n X_n)=\mathbb{E}(a^{T}X)=a^{T}\mathbb{E}(X)
.\]
This gives linearity of expectation (does not require independence). 
\[
\operatorname{Var}(a^{T}X)=\sum_{i,j}^{}a_{i}a_{j}\underbrace{\operatorname{Cov}(X_{i}, X_{j})}_{=\mathbb{E}((X_{i}-\mathbb{E}(X_{i})(X_{j}-\mathbb{E}(X_{j}))))} =a^{T}\operatorname{Var}(X)a
.\] 
where the matrix $[\operatorname{Var}(X)]_{ij}=\operatorname{Cov}(X_{i},X_{j})$. This gives the "bilinearity of variance".
\subsection{Standardised statistics}
Let $X_1 , \ldots , X_n$ be iid. with $\mathbb{E}(X_1 )=\mu$ , $\operatorname{Var}(X_1)=\sigma^2$. We define $S_n=\sum_{i}^{}X_{i}$ and $\overline{X_n} \frac{S_n}{n} $ (the sample mean). By linearity \[
\mathbb{E} (\overline{X_n} )=\mu, \quad \operatorname{Var }(\overline{X_n} )= \frac{\sigma^2}{n}
.\]   
Define $Z_{n}= \frac{S_{n}-n \mu}{n}$. Then $\mathbb{E}(Z_{n})=0$ and $\operatorname{Var}(Z_{n})=1$. 

\subsection{Moment generating functions}
The mgf of a random variable $X$ is the function \[
M_{x}(t)=\mathbb{E}(e^{tx})
.\] 
provided that it exists for $t$ in some neighbourhood of 0. This is the Laplace transform of the pdf. It relates to moments of the pdf, for example $M_{x}^{(n)}(0)=\mathbb{E} (X^{n})$. 

Under broad conditions $M_{x}=M_{y} \iff F_{X}=F_{Y}$. (The Laplace transform is invertible.) The mgf is also useful for finding distributions of sums of independent random variables: 
\begin{example*}
     Let $X_1 , \ldots ,X_n \sim \operatorname{Po}(\mu)$. Then \[
     M_{X_{i}}(t)=\mathbb{E}(e^{tX_{i}})=\sum_{x=0}^{ \infty}e^{tx} \frac{e^{-\mu}\mu^{x}}{x!}=e^{-\mu}\sum_{x=0}^{ \infty}\frac{(e^{t}\mu^{x})}{x!}=e^{-\mu (1-e^{t})}
     .\] 
     What is $M_{S_{n}}$? We have \[
     M_{S_{n}}(t)=\mathbb{E}(e^{t (X_1 +\ldots +X_n)})=\prod_{i=1}^n e^{tX_{i}}=e^{-n \mu (1-e^{t})}
     .\] So we conclude $S_{n} \sim \operatorname{Po}(n \mu)$ 
\end{example*}
\subsection{Limits of r.v's}
The weak law of large numbers states that $\forall \varepsilon >0$, as $n \rightarrow \infty$, \[
\mathbb{P} \left(|\overline{X_n} -\mu > \epsilon|\right) \rightarrow 0
.\] 
The strong law of large numbers states that as $n \rightarrow \infty$, \[
\mathbb{P}(\overline{X_{n}} \rightarrow \mu)=1
.\] The central limit theorem states that if we have the variable $Z_{n}= \frac{S_{n}-n \mu}{\sigma \sqrt{n}}$, then as $n \rightarrow \infty$ we have \[
\mathbb{P}(Z_{n} \leq z) \rightarrow \Phi (z) \quad \forall z \in \mathbb{R}
.\] where $\Phi$ is the distribution function of a $N (0,1)$ random variable.  
\subsection{Conditional probability}
If $X,Y$ are discrete r.v's then \[
P_{X|Y}(x|y)= \frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)}
.\]   
If $X,Y$ are continuous then the joint pdf of $X,Y$ satisfies: \[
\mathbb{P}(X \leq x, P Y \leq y)=\int_{- \infty}^{x}\int_{- \infty}^{y}f_{X,Y} (x',y')\mathrm{d}y'  \mathrm{d}x' 
.\] 
The conditional pdf of $X$ given $Y$ is \[
f_{x|y}= \frac{f_{X,Y}(x,y)}{\int_{- \infty}^{ \infty}f_{X,Y}(x,y) \mathrm{d}x }
.\] 
The conditional expectation of $X$ given $Y$ is 
\begin{align*}
    \mathbb{E}(X|Y)=
    \begin{cases}
        \sum_{x}^{}xp_{X|Y}(x|Y) & \text{discrete}\\
        \int_{}^{}x f_{X|Y}(x|Y) \mathrm{d}x & \text{continuous}
    \end{cases}
\end{align*}
Note this is itself a random variable, as it is a function of $Y$. We define $\operatorname{Var}(X|Y)$ similarly. 

Tower property: $\mathbb{E}(\mathbb{E}(X|Y))=\mathbb{E}(X)$

Law of total variance: $\operatorname{Var}(X)=\mathbb{E}(\operatorname{Var}(X|Y))+\operatorname{Var}(\mathbb{E}(X|Y))$. 

Change of variables (in 2D):

Let $(x,y) \mapsto (u,v)$ be a differentiable bijection $\mathbb{R}^{2} \rightarrow \mathbb{R}^{2} $. Then \[
f_{U,V}(u,v)=f_{X,Y}(x (u,v),y (u,v))|\det (J)|
.\] where $J=\frac{\partial (x,y)}{\partial (u,v)}$ is the Jacobian matrix we have seen before. 

If $X_{i} \sim \Gamma (\alpha_{i}, \lambda)$ for $i=1,\ldots ,n$ with $X_1 , \ldots X_n$ independent, then what is the distribution of $S_{n}=\sum_{i=1}^{n}X_{i}$? \[
M_{S_{n}}(t)=\prod_{i}M_{X_{i}}(t)=
\begin{cases}
    \left(\frac{\lambda}{\lambda t}\right)^{\sum_{i}^{}\alpha_{i}} & t<\lambda\\
    \infty & t>\lambda
\end{cases}
.\]  
So $S_{n}$ is $\Gamma (\sum_{i}^{}a_{i}, \lambda)$. We call the first parameter the "shape parameter", and the second one the "rate parameter". A consequence of what we have just done is that if $X \sim \Gamma (\alpha, \lambda)$, then for all $b>0$ we have $bX \sim \Gamma (\alpha, \frac{\lambda}{b})$. 

Special cases: 
\begin{itemize}
    \item $\Gamma (1, \lambda)=\operatorname{Exp}(\lambda)$ 
    \item $\Gamma \left(\frac{k}{2},\frac{1}{2}\right)=\chi_k^2$ (the chi-squared distribution with $k$ degrees of freedom, i.e the distribution of a sum of $k$ independent squared $N (0,1)$ r.v's.)
\end{itemize}

\subsection{Estimation}
Suppose $X_1 , \ldots X_n$ are iid observations with pdf or pdf (or pmf) $f_{X}(x| \theta)$ where $\theta$ is an unknown parameter in $\Theta$. Let $X=(X_1 , \ldots , X_{n})$. 
\begin{definition*}[Estimator]
     An estimator is a statistic or function of the data $T (X)=\hat{\theta}$ which does not depend on $\theta$, and is used to approximate the true parameter $\theta$. The distribution of $T (X)$ is called its "sampling distribution". 
\end{definition*}
\begin{example*}
     Let $X_1 , \ldots ,X_{n} \sim N (\mu,1)$ iid. Here $\hat{\mu}=\frac{1}{n}\sum_{i}^{}X_{i}=\overline{X_{n}}$. The sampling distribution of $\hat{\mu} $ is $T (X)=N (\mu, \frac{1}{n})$.   
\end{example*}
\begin{definition*}[Bias]
     The bias of $\hat{\theta}=T (X)$ is \[
     \operatorname{bias}(\hat{\theta})=\mathbb{E}_{\theta} (\hat{\theta})-\theta
     .\] Here $\mathbb{E}_{\theta}$ is the expectation in the model where $X_1 , X_2 , \ldots ,X_n \sim f_{X}(x|\theta)$.
\end{definition*}
\begin{remark}
     In general the bias is a function of true parameter $\theta$, even though it is not explicit in notation.  
\end{remark}
\begin{definition*}[Unbiased estimator]
     We say $\hat{\theta}$ is unbiased if $\operatorname{bias}(\hat{\theta})=0$ for all values of the true parameter $\theta$. 
\end{definition*}
In our example, $\hat{\mu}$ is unbiased because \[
\mathbb{E}_{\mu}(\hat{\mu})=\mathbb{E}_{\mu}(\overline{X_{n}})=\mu \quad \forall \mu \in \mathbb{R}
.\]  
\begin{definition*}[Mean squared error]
     The mean squared error (mse) of $\theta$ is \[
     \operatorname{mse}(\hat{\theta})=\mathbb{E}_{\theta} \left[ (\hat{\theta}-\theta)^2\right]
     .\] 
     It tells us "how far" $\hat{\theta}$ is from $\theta$ "on average".
\end{definition*}
\subsection{Bias-variance decomposition}
We expand the square in the definition of mse to get
\begin{align*}
     \operatorname{mse}(\hat{\theta})&=\mathbb{E}_{\theta} \left[ (\hat{\theta}-\theta)^2\right]\\
     &=\mathbb{E}_{\theta} \left((\hat{\theta}-\mathbb{E}_{\theta}\hat{\theta}-\theta)^{2}\right)
     &=\operatorname{Var}_{\theta}(\hat{\theta})+\operatorname{bias}^2 (\hat{\theta})\\
     & \geq 0
\end{align*}
There is a tradeoff between bias and variance. For example, let $X \sim \operatorname{Bin}(n,\theta)$. Suppose $n$ is known, and $\theta \in [0,1]$ is our unknown parameter. We define $T_{u}=\frac{X}{n}$, i.e the proportion of successes observed. Clearly $T_{u}$ is unbiased since \[
\mathbb{E}_{\theta} (T_{u})= \frac{E_{\theta}(X)}{n}=n \theta /n =\theta
.\] We can caculate \[
\operatorname{mse}(T_{u})=\operatorname{Var}_{\theta}(\frac{X}{n})= \frac{\operatorname{Var}_{\theta}}{n^2}= \frac{\theta (1-\theta)}{n}
.\] Consider another estimator $T_{B}= \frac{X+1}{n+2}=w \frac{X}{n}+ (1-w )\frac{1}{2}$ for $w=\frac{n}{n+2}$. This is called a "fixed estimator". In this case we have \[
\operatorname{bias}(T_{B})=\mathbb{E}_{\theta}(T_{B})-\theta=\mathbb{E}_{\theta}( \frac{X+1}{n+2})-\theta=\frac{n}{n+2}\theta +\frac{1}{n+2}-\theta
.\] This is $\neq 0$ for all but one value of $\theta$. Note that 
\begin{align*}
     \operatorname{Var}_{\theta}(T_{B})= \frac{\operatorname{Var}_{\theta}(X+1)}{(n+2)^2}\\
     \implies \operatorname{mse}(T_{B})=(1-w^2)\left(\frac{1}{2}-\theta\right)^2.
\end{align*}
\begin{remark}
     In this example, there are regions where either estimator is better. Prior judgement on the true value of $\theta$ determines which estimator is better. 
\end{remark}
Unbiasedness is not necessarily desirable. Let's look at a pathological example:
\begin{example*}
     Suppose $X \sim \operatorname{Po}(\lambda)$. We want to estimate $\theta= \mathbb{P} (X=0)^2=e^{-2\lambda}$. For some estimator $T (X)$ to be unbiased, we need \[
          \mathbb{E}_{\lambda}(T (x))=\sum_{x=0}^{ \infty}T (x) \frac{\lambda^{x}e^{-\lambda}}{x!}=e^{-2\lambda}=\theta \iff \sum_{x=0}^{ \infty}T (x) \frac{\lambda^{x}}{x!}=e^{-\lambda}=\sum_{x=0}^{ \infty}(-1)^{x} \frac{\lambda^{x}}{x!}  
          .\] The only function $T: N \rightarrow \mathbb{R}$ satisfying this equality is $T (x)=(-1)^{x}$. This is clearly an absurd estimator.
\end{example*}
\subsection{Sufficiency}
\begin{definition*}[Sufficiency]
     A statistic $T (X)$ is sufficient for $\theta$ if the conditional distribution of $X$ given $T (X)$ does not depend on $\theta$.
\end{definition*}
\begin{remark}
      $\theta$ can be a vector and $T (X)$ can also be vector-valued.
\end{remark}
\begin{example*}
      Let $X_1 , \ldots ,X_n$ be iid. Bernoulli$(\theta)$  variables for some $\theta$. Then \[
      f_{X}(X|\theta)=\prod_{i=1}^n \theta^{x_{i}}(1-\theta)^{1-x_{i}}=\theta^{\sum_{}^{}x_{i}} (1-\theta)^{n-\sum_{}^{}x_{i}}
      .\] This only depends on $x$ through $T (X)=\sum_{}^{}x_{i}$. 
\end{example*}
\end{document}